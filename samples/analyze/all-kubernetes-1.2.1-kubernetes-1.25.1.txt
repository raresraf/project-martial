>>comment_fuzzy_lines_files
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
If conn has no activity for serviceInfotimeout since last ReadWrite it shoule be closed because of timeout 
If conn has no activity for serviceInfotimeout since last ReadWrite it should be closed because of timeout 

When connecting to a UDP service endpoint there shoule be a Conn for proxy 
When connecting to a UDP service endpoint there should be a Conn for proxy 

Resource takes an unqualified resource and returns back a Group qualified GroupResource 
Resource takes an unqualified resource and returns a Group qualified GroupResource 

udpProxySocket implements proxySocket  Close is implemented by netUDPConn  When Close is called 
udpProxySocket implements ProxySocket  Close is implemented by netUDPConn  When Close is called 

proxyTCP proxies data bidirectionally between in and out 
ProxyTCP proxies data bidirectionally between in and out 

tcpProxySocket implements proxySocket  Close is implemented by netListener  When Close is called 
tcpProxySocket implements ProxySocket  Close is implemented by netListener  When Close is called 

ListenPort returns the host port that the proxySocket is listening on 
ListenPort returns the host port that the ProxySocket is listening on 

Close stops the proxySocket from accepting incoming connections 
Close stops the ProxySocket from accepting incoming connections 

Addr gets the netAddr for a proxySocket 
Addr gets the netAddr for a ProxySocket 

the originial logic of isPodRunning happens to return true when podstatus is empty so the test can always pass 
the original logic of isPodRunning happens to return true when podstatus is empty so the test can always pass 

construct lock id using host name and a magic prefix 
Construct lock id using host name and a magic prefix 

found a match check if device exists 
Found a match check if device exists 

search sysbus for rbd device that matches given pool and image 
Search sysbus for rbd device that matches given pool and image 

TODO check health of the socket  What if ProxyLoop exited 
TODO check health of the socket What if ProxyLoop exited 

assert Proxier is a ProxyProvider 
assert Proxier is a proxyProvider 

RealOS is used to dispatch the real system level operaitons 
RealOS is used to dispatch the real system level operations 

ProbeVolumePlugin is the entry point for plugin detection in a package 
ProbeVolumePlugins is the entry point for plugin detection in a package 

Setup the capability set  It wraps Initialize for improving usibility 
Setup the capability set  It wraps Initialize for improving usability 

Assuemes that the container has Custom Metrics enabled if it has etccustommetrics directory 
Assumes that the container has Custom Metrics enabled if it has etccustommetrics directory 

resolvePort attempts to turn a IntOrString port reference into a concrete port number 
resolvePort attempts to turn an IntOrString port reference into a concrete port number 

pairs and returns a a populated stringstring httpHeader map 
pairs and returns a populated stringstring httpHeader map 

Deleter removes the resource from the underlying storage provider  Calls to this method should block until the deletion is complete Any error returned indicates the volume has failed to be reclaimed A nil return indicates success 
Deleter removes the resource from the underlying storage provider Calls to this method should block until the deletion is complete Any error returned indicates the volume has failed to be reclaimed A nil return indicates success 

GetMetrics returns the Metrics for the Volume  Maybe expensive for some implementations 
GetMetrics returns the Metrics for the Volume Maybe expensive for some implementations 

MetricsProvider embeds methods for exposing metrics eg usedavailable space 
MetricsProvider embeds methods for exposing metrics eg used available space 

MetricsProvider embeds methods for exposing metrics eg usedavailable space 
MetricsProvider embeds methods for exposing metrics eg used available space 

diskManager interface and diskSetupTearDown functions abtract commonly used procedures to setup a block volume 
diskManager interface and diskSetupTearDown functions abstract commonly used procedures to setup a block volume 

We must as per iptables write a chainline for it which has the nice effect of flushing the chain  Then we can remove the 
We must as per iptables write a chainline for it which has the nice effect of flushing the chain Then we can remove the chain 

CanUseIptablesProxier returns true if we should use the iptables Proxier 
CanUseIPTablesProxier returns true if we should use the iptables Proxier 

Set default MaxSurge as 1 by default 
Set default MaxSurge as 0 by default 

downwardAPIVolumeCleander handles cleaning up downwardAPI volumes 
downwardAPIVolumeCleaner handles cleaning up downwardAPI volumes 

collectData collects requested downwardAPI in data map 
CollectData collects requested downwardAPI in data map 

EnvVarsToMap constructs a map of environment name to value from a slice 
envVarsToMap constructs a map of environment name to value from a slice 

IsIpv6 returns true if this is managing ipv6 tables 
IsIPv6 returns true if this is managing ipv6 tables 

the deleted keyvalue Note that this value might be stale If the pod 
the deleted keyvalue Note that this value might be stale If the Pod 

ReplicaSetsByCreationTimestamp sorts a list of ReplicationSets by creation timestamp using their names as a tie breaker 
ReplicaSetsByCreationTimestamp sorts a list of ReplicaSet by creation timestamp using their names as a tie breaker 

expcept theres not possibility for error since we know the exact type 
except theres not possibility for error since we know the exact type 

6 Empty creation time pods  newer pods  older pods 
8 Empty creation time pods  newer pods  older pods 

5 Pods with containers with higher restart counts  lower restart counts 
7 Pods with containers with higher restart counts  lower restart counts 

4 Been ready for empty time  less time  more time 
6 Been ready for empty time  less time  more time 

DeletionTimestamp on an object as a delete To do so consistenly one needs 
DeletionTimestamp on an object as a delete To do so consistently one needs 

CreationObserved atomically decrements the add expecation count of the given controller 
CreationObserved atomically decrements the add expectation count of the given controller 

Resource takes an unqualified resource and returns back a Group qualified GroupResource 
Resource takes an unqualified resource and returns a Group qualified GroupResource 

ProbeVolumePlugin is the entry point for plugin detection in a package 
ProbeVolumePlugins is the entry point for plugin detection in a package 

if device is no longer used see if need to logout the target 
If device is no longer used see if need to logout the target 

if device is no longer used see if need to logout the target 
If device is no longer used see if need to logout the target 

This function is run to sync the desired stated of pod 
This function is run to sync the desired state of pod 

Namspace of the pod 
Namespace of the pod 

information of all containers in the pod that are visble in Runtime 
information of all containers in the pod that are visible in Runtime 

Maintains NodeSpecUnschedulable value from previous run of tryUpdateNodeStatus 
maintains NodeSpecUnschedulable value from previous run of tryUpdateNodeStatus 

admission process and may be rejcted This can be resolved 
admission process and may be rejected This can be resolved 

The resyncTicker wakes up kubelet to checks if there are any pod workers 
The syncTicker wakes up kubelet to checks if there are any pod workers 

backOffPeriod is the period to back off when pod syncing resulting in an 
backOffPeriod is the period to back off when pod syncing results in an 

validatePayload returns an error if any path in the payload  returns a copy of the payload with the paths cleaned 
validatePayload returns an error if any path in the payload returns a copy of the payload with the paths cleaned 

9  The new data directory symlink is renamed to the data directory rename is atomic 
8  The new data directory symlink is renamed to the data directory rename is atomic 

8  A symlink to the new timestamped directory data_tmp is created that will 
7  A symlink to the new timestamped directory data_tmp is created that will 

7  The current timestamped directory is detected by reading the data directory 
2  The current timestamped directory is detected by reading the data directory 

6  Symlinks and directory for new uservisible files are created if needed 
9  Symlinks and directory for new uservisible files are created if needed 

5  The payload is written to the new timestamped directory 
6 The payload is written to the new timestamped directory 

4  A new timestamped dir is created 
5  A new timestamped dir is created 

1  The payload is validated if the payload is invalid the function returns 
1 The payload is validated if the payload is invalid the function returns 

GenerateContainerRef returns an apiObjectReference which references the given container 
GenerateContainerRef returns an v1ObjectReference which references the given container 

lock the volume and thus wait for any concurrrent SetUpAt to finish 
lock the volume and thus wait for any concurrent SetUpAt to finish 

getFullContainerName gets the container name given the root process id of the container 
GetFullContainerName gets the container name given the root process id of the container 

package qos contains helper functions for quality of service 
Package qos contains helper functions for quality of service 

Resource takes an unqualified resource and returns back a Group qualified GroupResource 
Resource takes an unqualified resource and returns a Group qualified GroupResource 

Allowed is required  True if the action would be allowed false otherwise 
Allowed is required True if the action would be allowed false otherwise 

NonResourceAttributes includes the authorization attributes available for nonresource requests to the Authorizer interface 
ResourceAttributes includes the authorization attributes available for resource requests to the Authorizer interface 

ResourceAttributes includes the authorization attributes available for resource requests to the Authorizer interface 
NonResourceAttributes includes the authorization attributes available for nonresource requests to the Authorizer interface 

Authorizer implements authorizerAuthorize 
Authorize implements authorizerAuthorize 

File format is one map per line  This allows easy concatentation of files 
File format is one map per line  This allows easy concatenation of files 

formatMap formats mapstringstring to a string 
FormatMap formats mapstringstring to a string 

Controls full resync of objects monitored for replenihsment 
Controls full resync of objects monitored for replenishment 

TearDownAt simply deletes everything in the directory 
TearDown simply deletes everything in the directory 

TearDown simply deletes everything in the directory 
TearDownAt simply deletes everything in the directory 

SetUpAt creates new directory and clones a git repo 
SetUp creates new directory and clones a git repo 

SetUp creates new directory and clones a git repo 
SetUpAt creates new directory and clones a git repo 

Resource takes an unqualified resource and returns back a Group qualified GroupResource 
Resource takes an unqualified resource and returns a Group qualified GroupResource 

TearDownAt unmounts the bind mount 
TearDown unmounts the bind mount 

TearDown unmounts the bind mount 
TearDownAt unmounts the bind mount 

SetUpAt attaches the disk and bind mounts to the volume path 
SetUp attaches the disk and bind mounts to the volume path 

SetUp attaches the disk and bind mounts to the volume path 
SetUpAt attaches the disk and bind mounts to the volume path 

This CA will be added in the secretes of service accounts 
This CA will be added in the secrets of service accounts 

InstallDeguggingHandlers registers the HTTP request patterns that serve logs or run commandscontainers 
InstallDebuggingHandlers registers the HTTP request patterns that serve logs or run commandscontainers 

identfier for a loadbalanced service 
identifier for a loadbalanced service 

with the internal podscontainers and generats events accordingly 
with the internal podscontainers and generates events accordingly 

garbage collector is implemented to work with such situtations However to 
garbage collector is implemented to work with such situations However to 

periodic listing to discover container changes It should be be used 
periodic listing to discover container changes It should be used 

package capbabilities manages system level capabilities 
Package capabilities manages system level capabilities 

Simulate a matchin with 1 core and 375GB of memory 
Simulate a machine with 1 core and 375GB of memory 

NewManager creates ane returns an empty results manager 
NewManager creates and returns an empty results manager 

Eg If the devices cgroup for the container is stored in sysfscgroupdevicesdockernginx 
Eg if the devices cgroup for the container is stored in sysfscgroupdevicesdockernginx 

Runs e will not return until stopCh is closed workers determines how many 
Run will not return until stopCh is closed workers determines how many 

1 PUT for the ReplicaSet status during dormancy window 
2 PUT for the ReplicaSet status during dormancy window 

Time when the data was last modfied 
Time when the data was last modified 

Cache provides two methods to retrive the PodStatus the nonblocking Get 
Cache provides two methods to retrieve the PodStatus the nonblocking Get 

3  podSpecActiveDeadlineSeconds gives the recycler pod a maximum timeout before failing  Recommended  Default is 60 seconds 
3 podSpecActiveDeadlineSeconds gives the recycler pod a maximum timeout before failing  Recommended  Default is 60 seconds 

2  podGenerateName helps distinguish recycler pods by name  Recommended Default is pvrecycler 
2 podGenerateName helps distinguish recycler pods by name  Recommended Default is pvrecycler 

1  podSpecVolumes0VolumeSource must be overridden  Recycler implementations without a valid VolumeSource will fail 
1 podSpecVolumes0VolumeSource must be overridden  Recycler implementations without a valid VolumeSource will fail 

GetPodPluginDir returns the absolute path to a directory under which 
GetPluginDir returns the absolute path to a directory under which 

GetPluginDir returns the absolute path to a directory under which 
GetPodPluginDir returns the absolute path to a directory under which 

unionDockerKeyring delegates to a set of keyrings 
UnionDockerKeyring delegates to a set of keyrings 

>>comment_spacy_core_web_lines_files
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
If conn has no activity for serviceInfotimeout since last ReadWrite it shoule be closed because of timeout 
If conn has no activity for serviceInfotimeout since last ReadWrite it should be closed because of timeout 

When connecting to a UDP service endpoint there shoule be a Conn for proxy 
When connecting to a UDP service endpoint there should be a Conn for proxy 

Adds the list of known types to apiScheme 
Adds the list of known types to the given scheme 

Resource takes an unqualified resource and returns back a Group qualified GroupResource 
Resource takes an unqualified resource and returns a Group qualified GroupResource 

Kind takes an unqualified kind and returns back a Group qualified GroupKind 
Kind takes an unqualified kind and returns a Group qualified GroupKind 

no new connections are allowed and existing connections are broken 
no new connections are allowed but existing connections are left untouched 

udpProxySocket implements proxySocket  Close is implemented by netUDPConn  When Close is called 
udpProxySocket implements ProxySocket  Close is implemented by netUDPConn  When Close is called 

udpProxySocket implements proxySocket  Close is implemented by netUDPConn  When Close is called 
tcpProxySocket implements ProxySocket  Close is implemented by netListener  When Close is called 

proxyTCP proxies data bidirectionally between in and out 
ProxyTCP proxies data bidirectionally between in and out 

no new connections are allowed but existing connections are left untouched 
no new connections are allowed and existing connections are broken 

tcpProxySocket implements proxySocket  Close is implemented by netListener  When Close is called 
udpProxySocket implements ProxySocket  Close is implemented by netUDPConn  When Close is called 

tcpProxySocket implements proxySocket  Close is implemented by netListener  When Close is called 
tcpProxySocket implements ProxySocket  Close is implemented by netListener  When Close is called 

ListenPort returns the host port that the proxySocket is listening on 
ListenPort returns the host port that the ProxySocket is listening on 

Close stops the proxySocket from accepting incoming connections 
Close stops the ProxySocket from accepting incoming connections 

Addr gets the netAddr for a proxySocket 
Addr gets the netAddr for a ProxySocket 

Returns a NodeConfig that is being used by the container manager 
GetNodeConfig returns a NodeConfig that is being used by the container manager 

Returns resources allocated to system cgroups in the machine 
SystemCgroupsLimit returns resources allocated to system cgroups in the machine 

readOnly bool is supplied by persistentclaim volume source when its builder creates other volumes 
readOnly bool is supplied by persistentclaim volume source when its mounter creates other volumes 

filter parent 1 protocol ip pref 1 u32 fh 800800 order 2048 key ht 800 bkt 0 flowid 11 
filter parent 1 protocol ip pref 1 u32 chain 0 fh 800800 order 2048 key ht 800 bkt 0 flowid 11 not_in_hw new version 

tcShaper provides an implementation of the BandwidthShaper interface on Linux using the tc tool 
tcShaper provides an implementation of the Shaper interface on Linux using the tc tool 

the originial logic of isPodRunning happens to return true when podstatus is empty so the test can always pass 
the original logic of isPodRunning happens to return true when podstatus is empty so the test can always pass 

rbd map 
Commandline rbdCmd map imgPath  

rbd map 
rbdnbd map poolimage  

construct lock id using host name and a magic prefix 
Construct lock id using host name and a magic prefix 

found a match check if device exists 
Found a match check if device exists 

first match pool then match name 
First match pool then match name 

search sysbus for rbd device that matches given pool and image 
Search sysbus for rbd device that matches given pool and image 

TODO Can we DNAT with IPv6 
TODO Can we REDIRECT with IPv6 

Build a slice of iptables args for a fromhost publicport rule 
Build a slice of iptables args for a fromhost portal rule 

Build a slice of iptables args for a fromhost publicport rule 
Build a slice of iptables args for a fromcontainer portal rule 

TODO Can we DNAT with IPv6 
TODO Can we REDIRECT with IPv6 

TODO Can we REDIRECT with IPv6 
TODO Can we DNAT with IPv6 

TODO Can we REDIRECT with IPv6 
TODO Can we DNAT with IPv6 

TODO Can we REDIRECT with IPv6 
TODO Can we DNAT with IPv6 

TODO Should we just reuse iptablesContainerPortalArgs 
TODO Should we just reuse iptablesHostPortalArgs 

See iptablesContainerPortalArgs 
See iptablesHostPortalArgs 

Build a slice of iptables args for a fromcontainer publicport rule 
Build a slice of iptables args for a fromhost publicport rule 

Build a slice of iptables args for a fromcontainer publicport rule 
Build a slice of iptables args for a fromhost portal rule 

Build a slice of iptables args for a fromcontainer publicport rule 
Build a slice of iptables args for a fromcontainer portal rule 

TODO Can we DNAT with IPv6 
TODO Can we REDIRECT with IPv6 

ONLY listening on that IP not the bridge If the proxy is bound to localhost only this should work but we dont allow it for now 
REDIRECT  Except that it doesnt work empirically  REDIRECT on local packets sends the traffic to localhost special case but it is documented but the response comes from the eth0 IP not sure why truthfully which makes DNS unhappy So we have to use DNAT  DNAT to 127001 cant work for the same 

that IP  Unlike the previous case this works because the proxy is ONLY listening on that IP not the bridge If the proxy is bound to localhost only this should work but we dont allow it for now TODO Can we DNAT with IPv6 Build a slice of iptables args for a fromcontainer publicport rule 
m addrtype dsttype LOCAL is what we want except that it is broken by intent without foresight to our usecase on at least GCE Specifically GCE machines have a daemon which learns what external IPs are forwarded to that machine and configure a local route for that IP making a match for dsttype LOCAL when we dont want it to Removing the route gives correct behavior until the daemon recreates it Killing the daemon is an option but means that any nonkubernetes use of the machine with external IP will be broken 

that IP  Unlike the previous case this works because the proxy is ONLY listening on that IP not the bridge If the proxy is bound to localhost only this should work but we dont allow it for now 
REDIRECT  Except that it doesnt work empirically  REDIRECT on local packets sends the traffic to localhost special case but it is documented but the response comes from the eth0 IP not sure why 

If the proxy is bound to a specific IP then we have to use DNAT to that IP  Unlike the previous case this works because the proxy is ONLY listening on that IP not the bridge If the proxy is bound to localhost only this should work but we dont allow it for now 
local packets sends the traffic to localhost special case but it is documented but the response comes from the eth0 IP not sure why truthfully which makes DNS unhappy So we have to use DNAT  DNAT to 127001 cant work for the same 

If the proxy is bound to a specific IP then we have to use DNAT to that IP  Unlike the previous case this works because the proxy is ONLY listening on that IP not the bridge If the proxy is bound to localhost only this should work but we dont allow it for now 
If the proxy is bound see ProxierlistenIP to 0000 any interface we want to do the same as fromcontainer traffic and use REDIRECT  Except that it doesnt work empirically  REDIRECT on local packets sends the traffic to localhost special case but it is 

DNAT to that  This works again empirically If the proxy is bound to a specific IP then we have to use DNAT to that IP  Unlike the previous case this works because the proxy is ONLY listening on that IP not the bridge If the proxy is bound to localhost only this should work but we 
If the proxy is bound see ProxierlistenIP to 0000 any interface we want to do the same as fromcontainer traffic and use REDIRECT  Except that it doesnt work empirically  REDIRECT on local packets sends the traffic to localhost special case but it is documented but the response comes from the eth0 IP not sure why truthfully which makes DNS unhappy 

DNAT to that  This works again empirically If the proxy is bound to a specific IP then we have to use DNAT to that IP  Unlike the previous case this works because the proxy is ONLY listening on that IP not the bridge If the proxy is bound to localhost only this should work but we 
This is tricky If the proxy is bound see ProxierlistenIP to 0000 any interface we want to do the same as fromcontainer traffic and use REDIRECT  Except that it doesnt work empirically  REDIRECT on local packets sends the traffic to localhost special case but it is documented but the response comes from the eth0 IP not sure why 

So we do our best to find an interface that is not a loopback and DNAT to that  This works again empirically If the proxy is bound to a specific IP then we have to use DNAT to that IP  Unlike the previous case this works because the proxy is ONLY listening on that IP not the bridge If the proxy is bound to localhost only this should work but we 
If the proxy is bound see ProxierlistenIP to 0000 any interface we want to do the same as fromcontainer traffic and use REDIRECT  Except that it doesnt work empirically  REDIRECT on local packets sends the traffic to localhost special case but it is documented but the response comes from the eth0 IP not sure why 

So we do our best to find an interface that is not a loopback and DNAT to that  This works again empirically If the proxy is bound to a specific IP then we have to use DNAT to that IP  Unlike the previous case this works because the proxy is ONLY listening on that IP not the bridge If the proxy is bound to localhost only this should work but we 
This is tricky If the proxy is bound see ProxierlistenIP to 0000 any interface we want to do the same as fromcontainer traffic and use REDIRECT  Except that it doesnt work empirically  REDIRECT on local packets sends the traffic to localhost special case but it is 

So we do our best to find an interface that is not a loopback and DNAT to that  This works again empirically If the proxy is bound to a specific IP then we have to use DNAT to that IP  Unlike the previous case this works because the proxy is ONLY listening on that IP not the bridge 
If the proxy is bound see ProxierlistenIP to 0000 any interface we want to do the same as fromcontainer traffic and use REDIRECT  Except that it doesnt work empirically  REDIRECT on local packets sends the traffic to localhost special case but it is documented but the response comes from the eth0 IP not sure why truthfully which makes DNS unhappy 

So we do our best to find an interface that is not a loopback and DNAT to that  This works again empirically If the proxy is bound to a specific IP then we have to use DNAT to that IP  Unlike the previous case this works because the proxy is ONLY listening on that IP not the bridge 
This is tricky If the proxy is bound see ProxierlistenIP to 0000 any interface we want to do the same as fromcontainer traffic and use REDIRECT  Except that it doesnt work empirically  REDIRECT on local packets sends the traffic to localhost special case but it is documented but the response comes from the eth0 IP not sure why 

reason So we do our best to find an interface that is not a loopback and DNAT to that  This works again empirically If the proxy is bound to a specific IP then we have to use DNAT to that IP  Unlike the previous case this works because the proxy is ONLY listening on that IP not the bridge 
If the proxy is bound see ProxierlistenIP to 0000 any interface we want to do the same as fromcontainer traffic and use REDIRECT  Except that it doesnt work empirically  REDIRECT on local packets sends the traffic to localhost special case but it is documented but the response comes from the eth0 IP not sure why 

So we have to use DNAT  DNAT to 127001 cant work for the same reason So we do our best to find an interface that is not a loopback and DNAT to that  This works again empirically If the proxy is bound to a specific IP then we have to use DNAT to that IP  Unlike the previous case this works because the proxy is 
If the proxy is bound see ProxierlistenIP to 0000 any interface we want to do the same as fromcontainer traffic and use REDIRECT  Except that it doesnt work empirically  REDIRECT on local packets sends the traffic to localhost special case but it is 

documented but the response comes from the eth0 IP not sure why truthfully which makes DNS unhappy So we have to use DNAT  DNAT to 127001 cant work for the same reason So we do our best to find an interface that is not a loopback and DNAT to that  This works again empirically 
If the proxy is bound see ProxierlistenIP to 0000 any interface we want to do the same as fromcontainer traffic and use REDIRECT  Except that it doesnt work empirically  REDIRECT on local packets sends the traffic to localhost special case but it is 

local packets sends the traffic to localhost special case but it is documented but the response comes from the eth0 IP not sure why truthfully which makes DNS unhappy So we have to use DNAT  DNAT to 127001 cant work for the same 
If the proxy is bound to a specific IP then we have to use DNAT to that IP  Unlike the previous case this works because the proxy is ONLY listening on that IP not the bridge If the proxy is bound to localhost only this should work but we dont allow it for now 

REDIRECT  Except that it doesnt work empirically  REDIRECT on local packets sends the traffic to localhost special case but it is documented but the response comes from the eth0 IP not sure why truthfully which makes DNS unhappy So we have to use DNAT  DNAT to 127001 cant work for the same 
ONLY listening on that IP not the bridge If the proxy is bound to localhost only this should work but we dont allow it for now 

REDIRECT  Except that it doesnt work empirically  REDIRECT on local packets sends the traffic to localhost special case but it is documented but the response comes from the eth0 IP not sure why 
that IP  Unlike the previous case this works because the proxy is ONLY listening on that IP not the bridge If the proxy is bound to localhost only this should work but we dont allow it for now 

interface we want to do the same as fromcontainer traffic and use REDIRECT  Except that it doesnt work empirically  REDIRECT on local packets sends the traffic to localhost special case but it is documented but the response comes from the eth0 IP not sure why truthfully which makes DNS unhappy So we have to use DNAT  DNAT to 127001 cant work for the same 
ONLY listening on that IP not the bridge Why would anyone bind to an address that is not inclusive of localhost  Apparently some cloud environments have their public IP exposed as a real network interface AND do not have firewalling  We dont want to expose everything out to the world Unfortunately I dont know of any way to listen on some N  1 

interface we want to do the same as fromcontainer traffic and use REDIRECT  Except that it doesnt work empirically  REDIRECT on local packets sends the traffic to localhost special case but it is documented but the response comes from the eth0 IP not sure why 
that IP  Unlike the previous case this works because the proxy is ONLY listening on that IP not the bridge Why would anyone bind to an address that is not inclusive of localhost  Apparently some cloud environments have their public IP exposed as a real network interface AND do not have firewalling  We dont want to expose everything out to the world 

If the proxy is bound see ProxierlistenIP to 0000 any interface we want to do the same as fromcontainer traffic and use REDIRECT  Except that it doesnt work empirically  REDIRECT on local packets sends the traffic to localhost special case but it is documented but the response comes from the eth0 IP not sure why truthfully which makes DNS unhappy 
DNAT to that  This works again empirically If the proxy is bound to a specific IP then we have to use DNAT to that IP  Unlike the previous case this works because the proxy is ONLY listening on that IP not the bridge If the proxy is bound to localhost only this should work but we 

If the proxy is bound see ProxierlistenIP to 0000 any interface we want to do the same as fromcontainer traffic and use REDIRECT  Except that it doesnt work empirically  REDIRECT on local packets sends the traffic to localhost special case but it is documented but the response comes from the eth0 IP not sure why truthfully which makes DNS unhappy 
So we do our best to find an interface that is not a loopback and DNAT to that  This works again empirically If the proxy is bound to a specific IP then we have to use DNAT to that IP  Unlike the previous case this works because the proxy is ONLY listening on that IP not the bridge 

If the proxy is bound see ProxierlistenIP to 0000 any interface we want to do the same as fromcontainer traffic and use REDIRECT  Except that it doesnt work empirically  REDIRECT on local packets sends the traffic to localhost special case but it is documented but the response comes from the eth0 IP not sure why truthfully which makes DNS unhappy 
breaks the NAT and makes things like DNS not accept them  If this could be resolved it would simplify all of this code If the proxy is bound to a specific IP then we have to use DNAT to 

If the proxy is bound see ProxierlistenIP to 0000 any interface we want to do the same as fromcontainer traffic and use REDIRECT  Except that it doesnt work empirically  REDIRECT on local packets sends the traffic to localhost special case but it is documented but the response comes from the eth0 IP not sure why 
So we do our best to find an interface that is not a loopback and DNAT to that  This works again empirically If the proxy is bound to a specific IP then we have to use DNAT to that IP  Unlike the previous case this works because the proxy is ONLY listening on that IP not the bridge If the proxy is bound to localhost only this should work but we 

If the proxy is bound see ProxierlistenIP to 0000 any interface we want to do the same as fromcontainer traffic and use REDIRECT  Except that it doesnt work empirically  REDIRECT on local packets sends the traffic to localhost special case but it is documented but the response comes from the eth0 IP not sure why 
reason So we do our best to find an interface that is not a loopback and DNAT to that  This works again empirically If the proxy is bound to a specific IP then we have to use DNAT to that IP  Unlike the previous case this works because the proxy is ONLY listening on that IP not the bridge 

If the proxy is bound see ProxierlistenIP to 0000 any interface we want to do the same as fromcontainer traffic and use REDIRECT  Except that it doesnt work empirically  REDIRECT on local packets sends the traffic to localhost special case but it is documented but the response comes from the eth0 IP not sure why 
breaks the NAT and makes things like DNS not accept them  If this could be resolved it would simplify all of this code If the proxy is bound to a specific IP then we have to use DNAT to that IP  Unlike the previous case this works because the proxy is 

If the proxy is bound see ProxierlistenIP to 0000 any interface we want to do the same as fromcontainer traffic and use REDIRECT  Except that it doesnt work empirically  REDIRECT on local packets sends the traffic to localhost special case but it is 
If the proxy is bound to a specific IP then we have to use DNAT to that IP  Unlike the previous case this works because the proxy is ONLY listening on that IP not the bridge If the proxy is bound to localhost only this should work but we dont allow it for now 

If the proxy is bound see ProxierlistenIP to 0000 any interface we want to do the same as fromcontainer traffic and use REDIRECT  Except that it doesnt work empirically  REDIRECT on local packets sends the traffic to localhost special case but it is 
So we have to use DNAT  DNAT to 127001 cant work for the same reason So we do our best to find an interface that is not a loopback and DNAT to that  This works again empirically If the proxy is bound to a specific IP then we have to use DNAT to that IP  Unlike the previous case this works because the proxy is 

If the proxy is bound see ProxierlistenIP to 0000 any interface we want to do the same as fromcontainer traffic and use REDIRECT  Except that it doesnt work empirically  REDIRECT on local packets sends the traffic to localhost special case but it is 
documented but the response comes from the eth0 IP not sure why truthfully which makes DNS unhappy So we have to use DNAT  DNAT to 127001 cant work for the same reason So we do our best to find an interface that is not a loopback and DNAT to that  This works again empirically 

This is tricky If the proxy is bound see ProxierlistenIP to 0000 any interface we want to do the same as fromcontainer traffic and use REDIRECT  Except that it doesnt work empirically  REDIRECT on local packets sends the traffic to localhost special case but it is documented but the response comes from the eth0 IP not sure why 
DNAT to that  This works again empirically If the proxy is bound to a specific IP then we have to use DNAT to that IP  Unlike the previous case this works because the proxy is ONLY listening on that IP not the bridge If the proxy is bound to localhost only this should work but we 

This is tricky If the proxy is bound see ProxierlistenIP to 0000 any interface we want to do the same as fromcontainer traffic and use REDIRECT  Except that it doesnt work empirically  REDIRECT on local packets sends the traffic to localhost special case but it is documented but the response comes from the eth0 IP not sure why 
So we do our best to find an interface that is not a loopback and DNAT to that  This works again empirically If the proxy is bound to a specific IP then we have to use DNAT to that IP  Unlike the previous case this works because the proxy is ONLY listening on that IP not the bridge 

This is tricky If the proxy is bound see ProxierlistenIP to 0000 any interface we want to do the same as fromcontainer traffic and use REDIRECT  Except that it doesnt work empirically  REDIRECT on local packets sends the traffic to localhost special case but it is documented but the response comes from the eth0 IP not sure why 
this could be resolved it would simplify all of this code If the proxy is bound to a specific IP then we have to use DNAT to that IP  Unlike the previous case this works because the proxy is ONLY listening on that IP not the bridge Why would anyone bind to an address that is not inclusive of 

This is tricky If the proxy is bound see ProxierlistenIP to 0000 any interface we want to do the same as fromcontainer traffic and use REDIRECT  Except that it doesnt work empirically  REDIRECT on local packets sends the traffic to localhost special case but it is 
So we do our best to find an interface that is not a loopback and DNAT to that  This works again empirically If the proxy is bound to a specific IP then we have to use DNAT to that IP  Unlike the previous case this works because the proxy is ONLY listening on that IP not the bridge If the proxy is bound to localhost only this should work but we 

Build a slice of iptables args for a fromhost portal rule 
Build a slice of iptables args for a fromhost publicport rule 

Build a slice of iptables args for a fromhost portal rule 
Build a slice of iptables args for a fromcontainer portal rule 

TODO Can we DNAT with IPv6 
TODO Can we REDIRECT with IPv6 

TODO Can we REDIRECT with IPv6 
TODO Can we DNAT with IPv6 

TODO Can we REDIRECT with IPv6 
TODO Can we DNAT with IPv6 

TODO Can we REDIRECT with IPv6 
TODO Can we DNAT with IPv6 

ONLY listening on that IP not the bridge Why would anyone bind to an address that is not inclusive of localhost  Apparently some cloud environments have their public IP exposed as a real network interface AND do not have firewalling  We dont want to expose everything out to the world Unfortunately I dont know of any way to listen on some N  1 
interface we want to do the same as fromcontainer traffic and use REDIRECT  Except that it doesnt work empirically  REDIRECT on local packets sends the traffic to localhost special case but it is documented but the response comes from the eth0 IP not sure why truthfully which makes DNS unhappy So we have to use DNAT  DNAT to 127001 cant work for the same 

that IP  Unlike the previous case this works because the proxy is ONLY listening on that IP not the bridge Why would anyone bind to an address that is not inclusive of localhost  Apparently some cloud environments have their public IP exposed as a real network interface AND do not have firewalling  We dont want to expose everything out to the world 
interface we want to do the same as fromcontainer traffic and use REDIRECT  Except that it doesnt work empirically  REDIRECT on local packets sends the traffic to localhost special case but it is documented but the response comes from the eth0 IP not sure why 

this could be resolved it would simplify all of this code If the proxy is bound to a specific IP then we have to use DNAT to that IP  Unlike the previous case this works because the proxy is ONLY listening on that IP not the bridge Why would anyone bind to an address that is not inclusive of localhost  Apparently some cloud environments have their public IP 
on at least GCE Specifically GCE machines have a daemon which learns what external IPs are forwarded to that machine and configure a local route for that IP making a match for dsttype LOCAL when we dont want it to Removing the route gives correct behavior until the daemon recreates it Killing the daemon is an option but means that any nonkubernetes use of the machine with external IP will be broken 

this could be resolved it would simplify all of this code If the proxy is bound to a specific IP then we have to use DNAT to that IP  Unlike the previous case this works because the proxy is ONLY listening on that IP not the bridge Why would anyone bind to an address that is not inclusive of 
This is tricky If the proxy is bound see ProxierlistenIP to 0000 any interface we want to do the same as fromcontainer traffic and use REDIRECT  Except that it doesnt work empirically  REDIRECT on local packets sends the traffic to localhost special case but it is documented but the response comes from the eth0 IP not sure why 

this could be resolved it would simplify all of this code If the proxy is bound to a specific IP then we have to use DNAT to that IP  Unlike the previous case this works because the proxy is ONLY listening on that IP not the bridge Why would anyone bind to an address that is not inclusive of 
m addrtype dsttype LOCAL is what we want except that it is broken by intent without foresight to our usecase on at least GCE Specifically GCE machines have a daemon which learns what external IPs are forwarded to that machine and configure a local route for that IP making a match for dsttype LOCAL when we dont want it to Removing the route gives correct behavior until the daemon recreates it Killing the daemon is an option but means that any nonkubernetes use of the machine with external IP will be broken 

this could be resolved it would simplify all of this code If the proxy is bound to a specific IP then we have to use DNAT to 
The alternative would be to use DNAT except that it doesnt work empirically DNAT to 127001  Packets just disappear  this seems to be a wellknown limitation of iptables DNAT to eth0s IP  Response packets come from the bridge which breaks the NAT and makes things like DNS not accept them  If 

breaks the NAT and makes things like DNS not accept them  If this could be resolved it would simplify all of this code If the proxy is bound to a specific IP then we have to use DNAT to that IP  Unlike the previous case this works because the proxy is ONLY listening on that IP not the bridge 
m addrtype dsttype LOCAL is what we want except that it is broken by intent without foresight to our usecase on at least GCE Specifically GCE machines have a daemon which learns what external IPs are forwarded to that machine and configure a local route for that IP making a match for dsttype LOCAL when we dont want it to Removing the route gives correct behavior until the daemon recreates it Killing the daemon is an option but means that any nonkubernetes use of the machine with external IP will be broken 

breaks the NAT and makes things like DNS not accept them  If this could be resolved it would simplify all of this code If the proxy is bound to a specific IP then we have to use DNAT to that IP  Unlike the previous case this works because the proxy is 
If the proxy is bound see ProxierlistenIP to 0000 any interface we want to do the same as fromcontainer traffic and use REDIRECT  Except that it doesnt work empirically  REDIRECT on local packets sends the traffic to localhost special case but it is documented but the response comes from the eth0 IP not sure why 

breaks the NAT and makes things like DNS not accept them  If this could be resolved it would simplify all of this code If the proxy is bound to a specific IP then we have to use DNAT to 
If the proxy is bound see ProxierlistenIP to 0000 any interface we want to do the same as fromcontainer traffic and use REDIRECT  Except that it doesnt work empirically  REDIRECT on local packets sends the traffic to localhost special case but it is documented but the response comes from the eth0 IP not sure why truthfully which makes DNS unhappy 

wellknown limitation of iptables DNAT to eth0s IP  Response packets come from the bridge which breaks the NAT and makes things like DNS not accept them  If this could be resolved it would simplify all of this code If the proxy is bound to a specific IP then we have to use DNAT to that IP  Unlike the previous case this works because the proxy is 
on at least GCE Specifically GCE machines have a daemon which learns what external IPs are forwarded to that machine and configure a local route for that IP making a match for dsttype LOCAL when we dont want it to Removing the route gives correct behavior until the daemon recreates it Killing the daemon is an option but means that any nonkubernetes use of the machine with external IP will be broken 

wellknown limitation of iptables DNAT to eth0s IP  Response packets come from the bridge which breaks the NAT and makes things like DNS not accept them  If this could be resolved it would simplify all of this code If the proxy is bound to a specific IP then we have to use DNAT to 
If the proxy is bound see ProxierlistenIP to 0000 any interface we want to use REDIRECT which sends traffic to the primary address of the incoming interface which means the container bridge if there is one  When the response comes it comes from that 

The alternative would be to use DNAT except that it doesnt work empirically DNAT to 127001  Packets just disappear  this seems to be a wellknown limitation of iptables DNAT to eth0s IP  Response packets come from the bridge which breaks the NAT and makes things like DNS not accept them  If 
this could be resolved it would simplify all of this code If the proxy is bound to a specific IP then we have to use DNAT to 

same interface so the NAT matches and the response packet is correct  This matters for UDP since there is no perconnection port number The alternative would be to use DNAT except that it doesnt work empirically DNAT to 127001  Packets just disappear  this seems to be a 
m addrtype dsttype LOCAL is what we want except that it is broken by intent without foresight to our usecase on at least GCE Specifically GCE machines have a daemon which learns what external IPs are forwarded to that machine and configure a local route for that IP making a match for dsttype LOCAL when we dont want it to Removing the route gives correct behavior until the daemon recreates it Killing the daemon is an option but means that any nonkubernetes use of the machine with external IP will be broken This applies to IPs on GCE that are actually from a loadbalancer they will be categorized as LOCAL 

interface we want to use REDIRECT which sends traffic to the primary address of the incoming interface which means the container bridge if there is one  When the response comes it comes from that same interface so the NAT matches and the response packet is correct  This matters for UDP since there is no perconnection port 
There is one complication per thockin m addrtype dsttype LOCAL is what we want except that it is broken by intent without foresight to our usecase on at least GCE Specifically GCE machines have a daemon which learns what external IPs are forwarded to that machine and configure a local route for that IP making a match for dsttype LOCAL when we dont want it to Removing the route gives correct behavior until the daemon recreates it Killing the daemon is an option but means that any nonkubernetes use of the machine with external IP will be broken 

If the proxy is bound see ProxierlistenIP to 0000 any interface we want to use REDIRECT which sends traffic to the primary address of the incoming interface which means the container bridge if there is one  When the response comes it comes from that same interface so the NAT matches and the response packet is correct  This matters for UDP since there is no perconnection port 
m addrtype dsttype LOCAL is what we want except that it is broken by intent without foresight to our usecase on at least GCE Specifically GCE machines have a daemon which learns what external IPs are forwarded to that machine and configure a local route for that IP making a match for dsttype LOCAL when we dont want it to Removing the route gives correct behavior until the daemon recreates it Killing the daemon is an option but means that any nonkubernetes use of the machine with external IP will be broken 

If the proxy is bound see ProxierlistenIP to 0000 any interface we want to use REDIRECT which sends traffic to the primary address of the incoming interface which means the container bridge if there is one  When the response comes it comes from that 
wellknown limitation of iptables DNAT to eth0s IP  Response packets come from the bridge which breaks the NAT and makes things like DNS not accept them  If this could be resolved it would simplify all of this code If the proxy is bound to a specific IP then we have to use DNAT to 

This is tricky If the proxy is bound see ProxierlistenIP to 0000 any interface we want to use REDIRECT which sends traffic to the primary address of the incoming interface which means the container bridge if there is one  When the response comes it comes from that same interface so the NAT matches and the response packet is 
There is one complication per thockin m addrtype dsttype LOCAL is what we want except that it is broken by intent without foresight to our usecase on at least GCE Specifically GCE machines have a daemon which learns what external IPs are forwarded to that machine and configure a local route for that IP making a match for dsttype LOCAL when we dont want it to Removing the route gives correct behavior until the daemon recreates it Killing the daemon is an option but means that any nonkubernetes use of the machine with external IP will be broken 

Build a slice of iptables args for a fromcontainer portal rule 
Build a slice of iptables args for a fromhost publicport rule 

Build a slice of iptables args for a fromcontainer portal rule 
Build a slice of iptables args for a fromhost portal rule 

iptables versions Build a slice of iptables args for a fromcontainer portal rule This is tricky If the proxy is bound see ProxierlistenIP to 0000 any interface we want to use REDIRECT which sends traffic to the primary address of the incoming interface which means the container 
if iptables fails to update or acquire the initial lock Once a proxier is created it will keep iptables up to date in the background and will not terminate if a particular iptables call fails NewCustomProxier functions similarly to NewProxier returning a new Proxier for the given LoadBalancer and address  The new proxier is constructed using the ProxySocket constructor provided however instead of constructing the 

Build a slice of iptables args that are common to fromcontainer and fromhost portal rules This list needs to include all fields as they are eventually spit out by iptablessave  This is because some systems do not support the iptables C arg and so fall back on parsing iptablessave output If this does not match it will not pass the check  For example adding the 32 on the destination IP arg is not strictly required 
machine and configure a local route for that IP making a match for dsttype LOCAL when we dont want it to Removing the route gives correct behavior until the daemon recreates it Killing the daemon is an option but means that any nonkubernetes use of the machine with external IP will be broken This applies to IPs on GCE that are actually from a loadbalancer they will be categorized as LOCAL _If_ the chains were in the wrong order and the LB traffic had dstport  a NodePort on some other service 

Build a slice of iptables args that are common to fromcontainer and fromhost portal rules This list needs to include all fields as they are eventually spit out by iptablessave  This is because some systems do not support the iptables C arg and so fall back on parsing iptablessave output If this does not match it will not pass the check  For example adding the 32 on the destination IP arg is not strictly required 
on at least GCE Specifically GCE machines have a daemon which learns what external IPs are forwarded to that machine and configure a local route for that IP making a match for dsttype LOCAL when we dont want it to Removing the route gives correct behavior until the daemon recreates it Killing the daemon is an option but means that any nonkubernetes use of the machine with external IP will be broken This applies to IPs on GCE that are actually from a loadbalancer they will be categorized as LOCAL 

Killing the daemon is an option but means that any nonkubernetes use of the machine with external IP will be broken This applies to IPs on GCE that are actually from a loadbalancer they will be categorized as LOCAL _If_ the chains were in the wrong order and the LB traffic had dstport  a NodePort on some other service the NodePort would take priority incorrectly This is unlikely and would only affect outgoing traffic from the cluster to the load balancer which seems doublyunlikely but we need to be careful to keep the rules in the right order 
When OnUpdate is first called the rules will be recreated CleanupLeftovers removes all iptables rules and chains created by the Proxier It returns true if an error was encountered Errors are logged NOTE Warning this needs to be kept in sync with the userspace Proxier we want to ensure we remove all of the iptables rules it creates 

machine and configure a local route for that IP making a match for dsttype LOCAL when we dont want it to Removing the route gives correct behavior until the daemon recreates it Killing the daemon is an option but means that any nonkubernetes use of the machine with external IP will be broken This applies to IPs on GCE that are actually from a loadbalancer they will be categorized as LOCAL _If_ the chains were in the wrong order and the LB traffic had dstport  a NodePort on some other service 
Build a slice of iptables args that are common to fromcontainer and fromhost portal rules This list needs to include all fields as they are eventually spit out by iptablessave  This is because some systems do not support the iptables C arg and so fall back on parsing iptablessave output If this does not match it will not pass the check  For example adding the 32 on the destination IP arg is not strictly required 

machine and configure a local route for that IP making a match for dsttype LOCAL when we dont want it to Removing the route gives correct behavior until the daemon recreates it Killing the daemon is an option but means that any nonkubernetes use of the machine with external IP will be broken This applies to IPs on GCE that are actually from a loadbalancer they will be categorized as LOCAL _If_ the chains were in the wrong order and the LB traffic had dstport  a NodePort on some other service 
Marks a port as being owned by a particular service or returns error if already claimed Idempotent reclaiming with the same owner is not an error TODO We could prepopulate some reserved ports into portMap andor blacklist some wellknown ports Hold the actual port open even though we use iptables to redirect it  This ensures that a its safe to take and b that stays true 

machine and configure a local route for that IP making a match for dsttype LOCAL when we dont want it to Removing the route gives correct behavior until the daemon recreates it Killing the daemon is an option but means that any nonkubernetes use of the machine with external IP will be broken This applies to IPs on GCE that are actually from a loadbalancer they will be categorized as LOCAL _If_ the chains were in the wrong order and the LB traffic had dstport  a NodePort on some other service 
SyncLoop runs periodic work  This is expected to run as a goroutine or as the main loop of the app  It does not return Ensure that portals exist for all services NB This does not remove rules that should not be present clean up any stale sticky session records in the hash map addServiceOnPortInternal starts listening for a new service returning the ServiceInfo Pass proxyPort0 to allocate a random port The timeout only applies to UDP 

machine and configure a local route for that IP making a match for dsttype LOCAL when we dont want it to Removing the route gives correct behavior until the daemon recreates it Killing the daemon is an option but means that any nonkubernetes use of the machine with external IP will be broken This applies to IPs on GCE that are actually from a loadbalancer they will be categorized as LOCAL 
dont sync rules till weve received services and endpoints SyncLoop runs periodic work  This is expected to run as a goroutine or as the main loop of the app  It does not return Ensure that portals exist for all services NB This does not remove rules that should not be present clean up any stale sticky session records in the hash map addServiceOnPortInternal starts listening for a new service returning the ServiceInfo 

on at least GCE Specifically GCE machines have a daemon which learns what external IPs are forwarded to that machine and configure a local route for that IP making a match for dsttype LOCAL when we dont want it to Removing the route gives correct behavior until the daemon recreates it Killing the daemon is an option but means that any nonkubernetes use of the machine with external IP will be broken This applies to IPs on GCE that are actually from a loadbalancer they will be categorized as LOCAL 
Build a slice of iptables args that are common to fromcontainer and fromhost portal rules This list needs to include all fields as they are eventually spit out by iptablessave  This is because some systems do not support the iptables C arg and so fall back on parsing iptablessave output If this does not match it will not pass the check  For example adding the 32 on the destination IP arg is not strictly required 

on at least GCE Specifically GCE machines have a daemon which learns what external IPs are forwarded to that machine and configure a local route for that IP making a match for dsttype LOCAL when we dont want it to Removing the route gives correct behavior until the daemon recreates it Killing the daemon is an option but means that any nonkubernetes use of the machine with external IP will be broken 
this could be resolved it would simplify all of this code If the proxy is bound to a specific IP then we have to use DNAT to that IP  Unlike the previous case this works because the proxy is ONLY listening on that IP not the bridge Why would anyone bind to an address that is not inclusive of localhost  Apparently some cloud environments have their public IP 

on at least GCE Specifically GCE machines have a daemon which learns what external IPs are forwarded to that machine and configure a local route for that IP making a match for dsttype LOCAL when we dont want it to Removing the route gives correct behavior until the daemon recreates it Killing the daemon is an option but means that any nonkubernetes use of the machine with external IP will be broken 
wellknown limitation of iptables DNAT to eth0s IP  Response packets come from the bridge which breaks the NAT and makes things like DNS not accept them  If this could be resolved it would simplify all of this code If the proxy is bound to a specific IP then we have to use DNAT to that IP  Unlike the previous case this works because the proxy is 

m addrtype dsttype LOCAL is what we want except that it is broken by intent without foresight to our usecase on at least GCE Specifically GCE machines have a daemon which learns what external IPs are forwarded to that machine and configure a local route for that IP making a match for dsttype LOCAL when we dont want it to Removing the route gives correct behavior until the daemon recreates it Killing the daemon is an option but means that any nonkubernetes use of the machine with external IP will be broken This applies to IPs on GCE that are actually from a loadbalancer they will be categorized as LOCAL 
same interface so the NAT matches and the response packet is correct  This matters for UDP since there is no perconnection port number The alternative would be to use DNAT except that it doesnt work empirically DNAT to 127001  Packets just disappear  this seems to be a 

m addrtype dsttype LOCAL is what we want except that it is broken by intent without foresight to our usecase on at least GCE Specifically GCE machines have a daemon which learns what external IPs are forwarded to that machine and configure a local route for that IP making a match for dsttype LOCAL when we dont want it to Removing the route gives correct behavior until the daemon recreates it Killing the daemon is an option but means that any nonkubernetes use of the machine with external IP will be broken This applies to IPs on GCE that are actually from a loadbalancer they will be categorized as LOCAL 
Marks a port as being owned by a particular service or returns error if already claimed Idempotent reclaiming with the same owner is not an error TODO We could prepopulate some reserved ports into portMap andor blacklist some wellknown ports Hold the actual port open even though we use iptables to redirect it  This ensures that a its safe to take and b that stays true NOTE We should not need to have a real listening socket  bind 

m addrtype dsttype LOCAL is what we want except that it is broken by intent without foresight to our usecase on at least GCE Specifically GCE machines have a daemon which learns what external IPs are forwarded to that machine and configure a local route for that IP making a match for dsttype LOCAL when we dont want it to Removing the route gives correct behavior until the daemon recreates it Killing the daemon is an option but means that any nonkubernetes use of the machine with external IP will be broken 
that IP  Unlike the previous case this works because the proxy is ONLY listening on that IP not the bridge If the proxy is bound to localhost only this should work but we dont allow it for now TODO Can we DNAT with IPv6 Build a slice of iptables args for a fromhost publicport rule 

m addrtype dsttype LOCAL is what we want except that it is broken by intent without foresight to our usecase on at least GCE Specifically GCE machines have a daemon which learns what external IPs are forwarded to that machine and configure a local route for that IP making a match for dsttype LOCAL when we dont want it to Removing the route gives correct behavior until the daemon recreates it Killing the daemon is an option but means that any nonkubernetes use of the machine with external IP will be broken 
this could be resolved it would simplify all of this code If the proxy is bound to a specific IP then we have to use DNAT to that IP  Unlike the previous case this works because the proxy is ONLY listening on that IP not the bridge Why would anyone bind to an address that is not inclusive of 

m addrtype dsttype LOCAL is what we want except that it is broken by intent without foresight to our usecase on at least GCE Specifically GCE machines have a daemon which learns what external IPs are forwarded to that machine and configure a local route for that IP making a match for dsttype LOCAL when we dont want it to Removing the route gives correct behavior until the daemon recreates it Killing the daemon is an option but means that any nonkubernetes use of the machine with external IP will be broken 
breaks the NAT and makes things like DNS not accept them  If this could be resolved it would simplify all of this code If the proxy is bound to a specific IP then we have to use DNAT to that IP  Unlike the previous case this works because the proxy is ONLY listening on that IP not the bridge 

m addrtype dsttype LOCAL is what we want except that it is broken by intent without foresight to our usecase on at least GCE Specifically GCE machines have a daemon which learns what external IPs are forwarded to that machine and configure a local route for that IP making a match for dsttype LOCAL when we dont want it to Removing the route gives correct behavior until the daemon recreates it Killing the daemon is an option but means that any nonkubernetes use of the machine with external IP will be broken 
If the proxy is bound see ProxierlistenIP to 0000 any interface we want to use REDIRECT which sends traffic to the primary address of the incoming interface which means the container bridge if there is one  When the response comes it comes from that same interface so the NAT matches and the response packet is correct  This matters for UDP since there is no perconnection port 

There is one complication per thockin m addrtype dsttype LOCAL is what we want except that it is broken by intent without foresight to our usecase on at least GCE Specifically GCE machines have a daemon which learns what external IPs are forwarded to that machine and configure a local route for that IP making a match for dsttype LOCAL when we dont want it to Removing the route gives correct behavior until the daemon recreates it Killing the daemon is an option but means that any nonkubernetes use of the machine with external IP will be broken 
interface we want to use REDIRECT which sends traffic to the primary address of the incoming interface which means the container bridge if there is one  When the response comes it comes from that same interface so the NAT matches and the response packet is correct  This matters for UDP since there is no perconnection port 

There is one complication per thockin m addrtype dsttype LOCAL is what we want except that it is broken by intent without foresight to our usecase on at least GCE Specifically GCE machines have a daemon which learns what external IPs are forwarded to that machine and configure a local route for that IP making a match for dsttype LOCAL when we dont want it to Removing the route gives correct behavior until the daemon recreates it Killing the daemon is an option but means that any nonkubernetes use of the machine with external IP will be broken 
This is tricky If the proxy is bound see ProxierlistenIP to 0000 any interface we want to use REDIRECT which sends traffic to the primary address of the incoming interface which means the container bridge if there is one  When the response comes it comes from that same interface so the NAT matches and the response packet is 

Release a claim on a port  Returns an error if the owner does not match the claim Tolerates release on an unclaimed port to simplify  We tolerate this it happens if we are cleaning up a failed allocation TODO Do we want to allow containers to access public services  Probably yes TODO We could refactor this to be the same code as portal but with IP  nil Handle traffic from containers 
Idempotent reclaiming with the same owner is not an error TODO We could prepopulate some reserved ports into portMap andor blacklist some wellknown ports Hold the actual port open even though we use iptables to redirect it  This ensures that a its safe to take and b that stays true NOTE We should not need to have a real listening socket  bind 

Idempotent reclaiming with the same owner is not an error TODO We could prepopulate some reserved ports into portMap andor blacklist some wellknown ports Hold the actual port open even though we use iptables to redirect it  This ensures that a its safe to take and b that stays true NOTE We should not need to have a real listening socket  bind 
Release a claim on a port  Returns an error if the owner does not match the claim Tolerates release on an unclaimed port to simplify  We tolerate this it happens if we are cleaning up a failed allocation TODO Do we want to allow containers to access public services  Probably yes TODO We could refactor this to be the same code as portal but with IP  nil Handle traffic from containers 

Marks a port as being owned by a particular service or returns error if already claimed Idempotent reclaiming with the same owner is not an error TODO We could prepopulate some reserved ports into portMap andor blacklist some wellknown ports Hold the actual port open even though we use iptables to redirect it  This ensures that a its safe to take and b that stays true NOTE We should not need to have a real listening socket  bind 
m addrtype dsttype LOCAL is what we want except that it is broken by intent without foresight to our usecase on at least GCE Specifically GCE machines have a daemon which learns what external IPs are forwarded to that machine and configure a local route for that IP making a match for dsttype LOCAL when we dont want it to Removing the route gives correct behavior until the daemon recreates it Killing the daemon is an option but means that any nonkubernetes use of the machine with external IP will be broken This applies to IPs on GCE that are actually from a loadbalancer they will be categorized as LOCAL 

Marks a port as being owned by a particular service or returns error if already claimed Idempotent reclaiming with the same owner is not an error TODO We could prepopulate some reserved ports into portMap andor blacklist some wellknown ports Hold the actual port open even though we use iptables to redirect it  This ensures that a its safe to take and b that stays true 
machine and configure a local route for that IP making a match for dsttype LOCAL when we dont want it to Removing the route gives correct behavior until the daemon recreates it Killing the daemon is an option but means that any nonkubernetes use of the machine with external IP will be broken This applies to IPs on GCE that are actually from a loadbalancer they will be categorized as LOCAL _If_ the chains were in the wrong order and the LB traffic had dstport  a NodePort on some other service 

TODO check health of the socket  What if ProxyLoop exited 
TODO check health of the socket What if ProxyLoop exited 

addServiceOnPort starts listening for a new service returning the serviceInfo 
addServiceOnPortInternal starts listening for a new service returning the ServiceInfo 

When OnUpdate is first called the rules will be recreated CleanupLeftovers removes all iptables rules and chains created by the Proxier It returns true if an error was encountered Errors are logged NOTE Warning this needs to be kept in sync with the userspace Proxier we want to ensure we remove all of the iptables rules it creates 
Killing the daemon is an option but means that any nonkubernetes use of the machine with external IP will be broken This applies to IPs on GCE that are actually from a loadbalancer they will be categorized as LOCAL _If_ the chains were in the wrong order and the LB traffic had dstport  a NodePort on some other service the NodePort would take priority incorrectly This is unlikely and would only affect outgoing traffic from the cluster to the load balancer which seems doublyunlikely but we need to be careful to keep the rules in the right order 

the loopback address May be checked for by callers of NewProxier to know whether the caller provided invalid input IsProxyLocked returns true if the proxy could not acquire the lock on iptables NewProxier returns a new Proxier given a LoadBalancer and an address on which to listen  Because of the iptables logic It is assumed that there 
Killing the daemon is an option but means that any nonkubernetes use of the machine with external IP will be broken This applies to IPs on GCE that are actually from a loadbalancer they will be categorized as LOCAL _If_ the chains were in the wrong order and the LB traffic had dstport  a NodePort on some other service the NodePort would take priority incorrectly This is unlikely and would only affect outgoing traffic from the cluster to the load balancer which seems 

the loopback address May be checked for by callers of NewProxier to know whether the caller provided invalid input IsProxyLocked returns true if the proxy could not acquire the lock on iptables NewProxier returns a new Proxier given a LoadBalancer and an address on which to listen  Because of the iptables logic It is assumed that there 
Removing the route gives correct behavior until the daemon recreates it Killing the daemon is an option but means that any nonkubernetes use of the machine with external IP will be broken This applies to IPs on GCE that are actually from a loadbalancer they will be categorized as LOCAL _If_ the chains were in the wrong order and the LB traffic had dstport  a NodePort on some other service 

assert Proxier is a ProxyProvider 
assert Proxier is a proxyProvider 

protects serviceMap 
protects serviceChanges 

Symlink will call osSymlink to create a symbolic link 
MkdirAll will call osMkdirAll to create a directory 

MkDir will will call osMkdir to create a directory 
MkdirAll will call osMkdirAll to create a directory 

RealOS is used to dispatch the real system level operaitons 
RealOS is used to dispatch the real system level operations 

Wrap EmptyDir let it do the teardown 
Wrap EmptyDir let it do the setup 

secretVolumeCleaner handles cleaning up secret volumes 
secretVolumeUnmounter handles cleaning up secret volumes 

secretVolumeBuilder handles retrieving secrets from the API server 
secretVolumeMounter handles retrieving secrets from the API server 

ProbeVolumePlugin is the entry point for plugin detection in a package 
ProbeVolumePlugins is the entry point for plugin detection in a package 

specifying a namespaceresource removes the  match on nonresource path 
specifying a resource removes the  match on nonresource path 

specifying a namespaceresource removes the  match on nonresource path 
specifying a namespace removes the  match on nonresource path 

specifying a resource removes the  match on nonresource path 
specifying a namespaceresource removes the  match on nonresource path 

specifying a resource removes the  match on nonresource path 
specifying a namespace removes the  match on nonresource path 

specifying a namespace removes the  match on nonresource path 
specifying a namespaceresource removes the  match on nonresource path 

specifying a namespace removes the  match on nonresource path 
specifying a resource removes the  match on nonresource path 

TODO This can clobber an update if we allow multiple agents to write to the same status Update the ReplicaSet with the latest resource version for the next poll If the GET fails we cant trust statusReplicas anymore This error is bound to be more interesting than the update failure overlappingReplicaSets sorts a list of ReplicaSets by creation timestamp using their names as a tie breaker 
If you make changes to this file you should also make the corresponding change in ReplicationController updateReplicaSetStatus attempts to update the StatusReplicas of the given ReplicaSet with a single GETPUT retry This is the steady state It happens when the ReplicaSet doesnt have any expectations since we do a periodic relist every 30s If the generations differ but the replicas are the same a caller mightve resized to the same replica count Save the generation number we acted on otherwise we might wrongfully indicate 

TODO This can clobber an update if we allow multiple agents to write to the same status Update the ReplicaSet with the latest resource version for the next poll If the GET fails we cant trust statusReplicas anymore This error is bound to be more interesting than the update failure overlappingReplicaSets sorts a list of ReplicaSets by creation timestamp using their names as a tie breaker 
limitations under the License If you make changes to this file you should also make the corresponding change in ReplicationController updateReplicaSetStatus attempts to update the StatusReplicas of the given ReplicaSet with a single GETPUT retry This is the steady state It happens when the ReplicaSet doesnt have any expectations since we do a periodic relist every 30s If the generations differ but the replicas are the same a caller mightve resized to the same replica count 

updateReplicaCount attempts to update the StatusReplicas of the given ReplicaSet with a single GETPUT retry This is the steady state It happens when the ReplicaSet doesnt have any expectations since we do a periodic relist every 30s If the generations differ but the replicas are the same a caller mightve resized to the same replica count Save the generation number we acted on otherwise we might wrongfully indicate 
NewReplicaSetCondition creates a new replicaset condition GetCondition returns a replicaset condition with the provided type if it exists SetCondition addsreplaces the given condition in the replicaset status If the condition that we are about to add already exists and has the same status and reason then we are not going to update 

updateReplicaCount attempts to update the StatusReplicas of the given ReplicaSet with a single GETPUT retry This is the steady state It happens when the ReplicaSet doesnt have any expectations since we do a periodic relist every 30s If the generations differ but the replicas are the same a caller mightve resized to the same replica count Save the generation number we acted on otherwise we might wrongfully indicate 
same status Stop retrying if we exceed statusUpdateRetries  the replicaSet will be requeued with a rate limit Update the ReplicaSet with the latest resource version for the next poll If the GET fails we cant trust statusReplicas anymore This error is bound to be more interesting than the update failure Count the number of pods that have labels matching the labels of the pod 

updateReplicaCount attempts to update the StatusReplicas of the given ReplicaSet with a single GETPUT retry 
updateReplicaSetStatus attempts to update the StatusReplicas of the given ReplicaSet with a single GETPUT retry 

If you make changes to this file you should also make the corresponding change in ReplicationController updateReplicaCount attempts to update the StatusReplicas of the given ReplicaSet with a single GETPUT retry This is the steady state It happens when the ReplicaSet doesnt have any expectations since we do a periodic relist every 30s If the generations differ but the replicas are the same a caller mightve resized to the same replica count Save the generation number we acted on otherwise we might wrongfully indicate 
Stop retrying if we exceed statusUpdateRetries  the replicaSet will be requeued with a rate limit Update the ReplicaSet with the latest resource version for the next poll If the GET fails we cant trust statusReplicas anymore This error is bound to be more interesting than the update failure Count the number of pods that have labels matching the labels of the pod 

If you make changes to this file you should also make the corresponding change in ReplicationController updateReplicaCount attempts to update the StatusReplicas of the given ReplicaSet with a single GETPUT retry This is the steady state It happens when the ReplicaSet doesnt have any expectations since we do a periodic relist every 30s If the generations differ but the replicas are 
GetCondition returns a replicaset condition with the provided type if it exists SetCondition addsreplaces the given condition in the replicaset status If the condition that we are about to add already exists and has the same status and reason then we are not going to update 

limitations under the License If you make changes to this file you should also make the corresponding change in ReplicationController updateReplicaCount attempts to update the StatusReplicas of the given ReplicaSet with a single GETPUT retry This is the steady state It happens when the ReplicaSet doesnt have any expectations since we do a periodic relist every 30s If the generations differ but the replicas are 
Stop retrying if we exceed statusUpdateRetries  the replicaSet will be requeued with a rate limit Update the ReplicaSet with the latest resource version for the next poll If the GET fails we cant trust statusReplicas anymore This error is bound to be more interesting than the update failure Count the number of pods that have labels matching the labels of the pod 

See the License for the specific language governing permissions and limitations under the License If you make changes to this file you should also make the corresponding change in ReplicationController updateReplicaCount attempts to update the StatusReplicas of the given ReplicaSet with a single GETPUT retry This is the steady state It happens when the ReplicaSet doesnt have any expectations since 
GetCondition returns a replicaset condition with the provided type if it exists SetCondition addsreplaces the given condition in the replicaset status If the condition that we are about to add already exists and has the same status and reason then we are not going to update RemoveCondition removes the condition with the provided type from the replicaset status 

See the License for the specific language governing permissions and limitations under the License If you make changes to this file you should also make the corresponding change in ReplicationController updateReplicaCount attempts to update the StatusReplicas of the given ReplicaSet with a single GETPUT retry This is the steady state It happens when the ReplicaSet doesnt have any expectations since 
a superset of the selector of the replica set so the possible matching pods must be part of the filteredPods NewReplicaSetCondition creates a new replicaset condition GetCondition returns a replicaset condition with the provided type if it exists SetCondition addsreplaces the given condition in the replicaset status If the condition that we are about to add already exists and has the same status and reason then we are not going to update 

Package gcp_credentials contains  implementations of DockerConfigProvider 
Package gcp contains implementations of DockerConfigProvider 

Returns a readonly copy of the system capabilities 
Get returns a readonly copy of the system capabilities 

SetCapabilitiesForTests  Convenience method for testing  This should only be called from tests 
SetForTests sets capabilities for tests  Convenience method for testing  This should only be called from tests 

Setup the capability set  It wraps Initialize for improving usibility 
Setup the capability set  It wraps Initialize for improving usability 

List of pod sources for which using host ipc is allowed 
List of pod sources for which using host pid namespace is allowed 

List of pod sources for which using host ipc is allowed 
List of pod sources for which using host network is allowed 

List of pod sources for which using host pid namespace is allowed 
List of pod sources for which using host ipc is allowed 

List of pod sources for which using host pid namespace is allowed 
List of pod sources for which using host network is allowed 

List of pod sources for which using host network is allowed 
List of pod sources for which using host ipc is allowed 

List of pod sources for which using host network is allowed 
List of pod sources for which using host pid namespace is allowed 

PrivilegedSources defines the pod sources allowed to make privileged requests for certain types of capabilities like host networking sharing the host IPC namespace and sharing the host PID namespace 
Pod sources from which to allow privileged capabilities like host networking sharing the host IPC namespace and sharing the host PID namespace 

Pod sources from which to allow privileged capabilities like host networking sharing the host IPC namespace and sharing the host PID namespace 
PrivilegedSources defines the pod sources allowed to make privileged requests for certain types of capabilities like host networking sharing the host IPC namespace and sharing the host PID namespace 

Assuemes that the container has Custom Metrics enabled if it has etccustommetrics directory 
Assumes that the container has Custom Metrics enabled if it has etccustommetrics directory 

Returns a path to a cAdvisorspecific custom metrics configuration 
GetCAdvisorCustomMetricsDefinitionPath returns a path to a cAdvisorspecific custom metrics configuration 

readOnly bool is supplied by persistentclaim volume source when its builder creates other volumes 
readOnly bool is supplied by persistentclaim volume source when its mounter creates other volumes 

readOnly bool is supplied by persistentclaim volume source when its builder creates other volumes 
readOnly bool is supplied by persistentclaim volume source when its mounter creates other volumes 

Symlink is a fake call that just returns nil 
Rename is a fake call that return nil 

Symlink is a fake call that just returns nil 
OpenFile is a fake call that return nil 

Symlink is a fake call that just returns nil 
Chtimes is a fake call that returns nil 

Symlink is a fake call that just returns nil 
Hostname is a fake call that returns nil 

Symlink is a fake call that just returns nil 
Chmod is a fake call that returns nil 

Symlink is a fake call that just returns nil 
RemoveAll is a fake call that just returns nil 

Symlink is a fake call that just returns nil 
Remove is a fake call that returns nil 

Symlink is a fake call that just returns nil 
Mkdir is a fake call that just returns nil 

Mkdir is a fake call that just returns nil 
Rename is a fake call that return nil 

Mkdir is a fake call that just returns nil 
OpenFile is a fake call that return nil 

Mkdir is a fake call that just returns nil 
Chtimes is a fake call that returns nil 

Mkdir is a fake call that just returns nil 
Hostname is a fake call that returns nil 

Mkdir is a fake call that just returns nil 
Chmod is a fake call that returns nil 

Mkdir is a fake call that just returns nil 
RemoveAll is a fake call that just returns nil 

Mkdir is a fake call that just returns nil 
Remove is a fake call that returns nil 

Mkdir is a fake call that just returns nil 
Symlink is a fake call that just returns nil 

Prober helps to check the livenessreadiness of a container 
Prober helps to check the livenessreadinessstartup of a container 

Deleter removes the resource from the underlying storage provider  Calls to this method should block until the deletion is complete Any error returned indicates the volume has failed to be reclaimed A nil return indicates success This method should block until completion Attacher can attach a volume to a node Detacher can detach a volume from a node 
the given timeout period an error will be returned DeviceMounterArgs provides auxiliary optional arguments to DeviceMounter DeviceMounter can mount a block volume to a global path GetDeviceMountPath returns a path where the device should 

Deleter removes the resource from the underlying storage provider  Calls to this method should block until 
Deleter removes the resource from the underlying storage provider Calls to this method should block until the deletion is complete Any error 

This func should NOT persist the PV in the API  That is left to the caller Deleter removes the resource from the underlying storage provider  Calls to this method should block until the deletion is complete Any error returned indicates the volume has failed to be reclaimed 
VolumesAreAttached checks whether the list of volumes still attached to the specified node It returns a map which maps from the volume spec to the checking result If an error is occurred during checking the error will be returned WaitForAttach blocks until the device is attached to this 

NewPersistentVolumeTemplate creates a new PersistentVolume to be used as a template before saving The provisioner will want to tweak its properties assign correct annotations etc This func should NOT persist the PV in the API  That is left to the caller Deleter removes the resource from the underlying storage provider  Calls to this method should block until the deletion is complete Any error returned indicates the volume has failed to be reclaimed A nil return indicates success 
sync This can be used to postpone deletion of a volume that is being detached from a node Deletion of such volume would fail anyway and such error would confuse users Attacher can attach a volume to a node Attaches the volume specified by the given spec to the node with the given Name On success returns the device path where the device was attached on the 

NewPersistentVolumeTemplate creates a new PersistentVolume to be used as a template before saving The provisioner will want to tweak its properties assign correct annotations etc This func should NOT persist the PV in the API  That is left to the caller Deleter removes the resource from the underlying storage provider  Calls to this method should block until the deletion is complete Any error returned indicates the volume has failed to be reclaimed A nil return indicates success 
volume controller will retry deleting the volume in the next periodic sync This can be used to postpone deletion of a volume that is being detached from a node Deletion of such volume would fail anyway and such error would confuse users Attacher can attach a volume to a node Attaches the volume specified by the given spec to the node with the given Name 

NewPersistentVolumeTemplate creates a new PersistentVolume to be used as a template before saving The provisioner will want to tweak its properties assign correct annotations etc This func should NOT persist the PV in the API  That is left to the caller Deleter removes the resource from the underlying storage provider  Calls to this method should block until the deletion is complete Any error returned indicates the volume has failed to be reclaimed 
as error and it will be sent as Info event to the PV being deleted The volume controller will retry deleting the volume in the next periodic sync This can be used to postpone deletion of a volume that is being detached from a node Deletion of such volume would fail anyway and such error would confuse users Attacher can attach a volume to a node 

NewPersistentVolumeTemplate creates a new PersistentVolume to be used as a template before saving The provisioner will want to tweak its properties assign correct annotations etc This func should NOT persist the PV in the API  That is left to the caller Deleter removes the resource from the underlying storage provider  Calls to this method should block until the deletion is complete Any error returned indicates the volume has failed to be reclaimed 
This method should block until completion deletedVolumeInUseError returned from this function will not be reported as error and it will be sent as Info event to the PV being deleted The volume controller will retry deleting the volume in the next periodic sync This can be used to postpone deletion of a volume that is being detached from a node Deletion of such volume would fail anyway and such 

This method should block until completion NewPersistentVolumeTemplate creates a new PersistentVolume to be used as a template before saving The provisioner will want to tweak its properties assign correct annotations etc This func should NOT persist the PV in the API  That is left to the caller 
as error and it will be sent as Info event to the PV being deleted The volume controller will retry deleting the volume in the next periodic sync This can be used to postpone deletion of a volume that is being detached from a node Deletion of such volume would fail anyway and such error would confuse users Attacher can attach a volume to a node 

Provision creates the resource by allocating the underlying volume in a storage system This method should block until completion NewPersistentVolumeTemplate creates a new PersistentVolume to be used as a template before saving The provisioner will want to tweak its properties assign correct annotations etc This func should NOT persist the PV in the API  That is left to the caller 
sync This can be used to postpone deletion of a volume that is being detached from a node Deletion of such volume would fail anyway and such error would confuse users Attacher can attach a volume to a node Attaches the volume specified by the given spec to the node with the given Name On success returns the device path where the device was attached on the 

Provision creates the resource by allocating the underlying volume in a storage system This method should block until completion NewPersistentVolumeTemplate creates a new PersistentVolume to be used as a template before saving The provisioner will want to tweak its properties assign correct annotations etc This func should NOT persist the PV in the API  That is left to the caller 
volume controller will retry deleting the volume in the next periodic sync This can be used to postpone deletion of a volume that is being detached from a node Deletion of such volume would fail anyway and such error would confuse users Attacher can attach a volume to a node Attaches the volume specified by the given spec to the node with the given Name 

Any error returned indicates the volume has failed to be reclaimed  A nil return indicates success 
returned indicates the volume has failed to be reclaimed A nil return 

Recycle reclaims the resource  Calls to this method should block until the recycling task is complete Any error returned indicates the volume has failed to be reclaimed  A nil return indicates success Provisioner is an interface that creates templates for PersistentVolumes and can create the volume as a new resource in the infrastructure provider 
and can create the volume as a new resource in the infrastructure provider Provision creates the resource by allocating the underlying volume in a storage system This method should block until completion and returns PersistentVolume representing the created storage resource Deleter removes the resource from the underlying storage provider Calls to this method should block until the deletion is complete Any error 

Recycler provides methods to reclaim the volume resource Recycle reclaims the resource  Calls to this method should block until the recycling task is complete Any error returned indicates the volume has failed to be reclaimed  A nil return indicates success Provisioner is an interface that creates templates for PersistentVolumes and can create the volume 
When FsUser is set the ownership of the volume will be modified to be owned and writable by FsUser Otherwise there is no side effects Currently only supported with projected service account tokens Mounter interface provides methods to set upmount the volume Uses Interface to provide the path for Docker binds SetUp prepares and mountsunpacks the volume to a 

Recycler provides methods to reclaim the volume resource 
Unmounter interface provides methods to cleanupunmount the volumes 

removes traces of the SetUp procedure Recycler provides methods to reclaim the volume resource Recycle reclaims the resource  Calls to this method should block until the recycling task is complete 
Deleter removes the resource from the underlying storage provider Calls to this method should block until the deletion is complete Any error 

removes traces of the SetUp procedure 
UnmapPodDevice removes traces of the MapPodDevice procedure 

removes traces of the SetUp procedure 
TearDownDevice removes traces of the SetUpDevice procedure 

removes traces of the SetUp procedure 
UnmapPodDevice removes traces of the MapPodDevice procedure 

removes traces of the SetUp procedure 
TearDownDevice removes traces of the SetUpDevice procedure 

Cleaner interface provides methods to cleanupunmount the volumes 
CustomBlockVolumeUnmapper interface provides custom methods to cleanupunmap the volumes 

Cleaner interface provides methods to cleanupunmount the volumes 
Unmounter interface provides methods to cleanupunmount the volumes 

GetAttributes returns the attributes of the builder Cleaner interface provides methods to cleanupunmount the volumes TearDown unmounts the volume from a selfdetermined directory and 
TearDown unmounts the volume from the specified directory and removes traces of the SetUp procedure BlockVolumeMapper interface is a mapper interface for block volume CustomBlockVolumeMapper interface provides custom methods to set upmap the volume SetUpDevice prepares the volume to the node by the plugin specific way 

GetAttributes returns the attributes of the builder Cleaner interface provides methods to cleanupunmount the volumes TearDown unmounts the volume from a selfdetermined directory and 
removes traces of the SetUp procedure TearDown unmounts the volume from the specified directory and removes traces of the SetUp procedure BlockVolumeMapper interface is a mapper interface for block volume CustomBlockVolumeMapper interface provides custom methods to set upmap the volume 

GetAttributes returns the attributes of the builder 
GetAttributes returns the attributes of the mounter 

idempotent GetAttributes returns the attributes of the builder Cleaner interface provides methods to cleanupunmount the volumes TearDown unmounts the volume from a selfdetermined directory and 
TearDown unmounts the volume from the specified directory and removes traces of the SetUp procedure BlockVolumeMapper interface is a mapper interface for block volume CustomBlockVolumeMapper interface provides custom methods to set upmap the volume 

idempotent GetAttributes returns the attributes of the builder Cleaner interface provides methods to cleanupunmount the volumes 
removes traces of the SetUp procedure TearDown unmounts the volume from the specified directory and removes traces of the SetUp procedure BlockVolumeMapper interface is a mapper interface for block volume CustomBlockVolumeMapper interface provides custom methods to set upmap the volume SetUpDevice prepares the volume to the node by the plugin specific way 

be called more than once so implementations must be idempotent GetAttributes returns the attributes of the builder Cleaner interface provides methods to cleanupunmount the volumes TearDown unmounts the volume from a selfdetermined directory and removes traces of the SetUp procedure 
Attributes represents the attributes of this mounter MounterArgs provides more easily extensible arguments to Mounter When FsUser is set the ownership of the volume will be modified to be 

fsGroup so that it can be accessed by the pod This may be called more than once so implementations must be idempotent GetAttributes returns the attributes of the builder Cleaner interface provides methods to cleanupunmount the volumes TearDown unmounts the volume from a selfdetermined directory and 
SetUp prepares and mountsunpacks the volume to a selfdetermined directory path The mount point and its content should be owned by fsUser or fsGroup so that it can be 

fsGroup so that it can be accessed by the pod This may be called more than once so implementations must be 
accessed by the pod This may be called more than once so implementations must be idempotent 

fsGroup so that it can be accessed by the pod This may be called more than once so implementations must be 
content should be owned by fsUser or fsGroup so that it can be accessed by the pod This may be called more than once so 

The mount point and its content should be owned by fsGroup so that it can be accessed by the pod This may 
selfdetermined directory path The mount point and its content should be owned by fsUser or fsGroup so that it can be 

accessed by the pod This may be called more than once so implementations must be idempotent SetUpAt prepares and mountsunpacks the volume to the specified directory path which may or may not exist yet The mount point and its content should be owned by 
SetUp prepares and mountsunpacks the volume to a selfdetermined directory path The mount point and its content should be owned by fsUser or fsGroup so that it can be 

accessed by the pod This may be called more than once so implementations must be idempotent SetUpAt prepares and mountsunpacks the volume to the 
SetUpDevice prepares the volume to the node by the plugin specific way For most intree plugins attacherAttach and attacherWaitForAttach will do necessary works This may be called more than once so implementations must be idempotent 

accessed by the pod This may be called more than once so implementations must be idempotent 
fsGroup so that it can be accessed by the pod This may be called more than once so implementations must be 

content should be owned by fsGroup so that it can be accessed by the pod This may be called more than once so implementations must be idempotent SetUpAt prepares and mountsunpacks the volume to the 
The mount point and its content should be owned by fsUser fsGroup so that it can be accessed by the pod This may 

content should be owned by fsGroup so that it can be 
fsGroup so that it can be accessed by the pod This may be called more than once so implementations must be 

content should be owned by fsGroup so that it can be 
content should be owned by fsUser or fsGroup so that it can be 

selfdetermined directory path The mount point and its content should be owned by fsGroup so that it can be 
The mount point and its content should be owned by fsUser fsGroup so that it can be accessed by the pod This may 

Uses Interface to provide the path for Docker binds SetUp prepares and mountsunpacks the volume to a selfdetermined directory path The mount point and its content should be owned by fsGroup so that it can be accessed by the pod This may be called more than once so 
SetUpDevice prepares the volume to the node by the plugin specific way For most intree plugins attacherAttach and attacherWaitForAttach will do necessary works This may be called more than once so implementations must be idempotent SetUpDevice returns stagingPath if device setup was successful MapPodDevice maps the block device to a path and return the path 

Builder interface provides methods to set upmount the volume Uses Interface to provide the path for Docker binds SetUp prepares and mountsunpacks the volume to a 
Unmounter interface provides methods to cleanupunmount the volumes 

Builder interface provides methods to set upmount the volume 
CustomBlockVolumeMapper interface provides custom methods to set upmap the volume 

Builder interface provides methods to set upmount the volume 
Mounter interface provides methods to set upmount the volume 

Attributes represents the attributes of this builder 
Attributes represents the attributes of this mounter 

space on the underlying storage and is shared with host processes and other Volumes 
on the underlying storage and is shared with host processes and other volumes 

For Volumes that share a filesystem with the host eg emptydir hostpath this is the available space on the underlying storage and is shared with host processes and other Volumes Attributes represents the attributes of this builder 
eg emptydir hostpath this is the size of the underlying storage and will not equal Used  Available as the fs is shared Available represents the storage space available bytes for the Volume For Volumes that share a filesystem with the host eg 

For Volumes that share a filesystem with the host eg emptydir hostpath this is the available space on the underlying storage and is shared with host processes and other Volumes Attributes represents the attributes of this builder 
underlying storage For Volumes that share a filesystem with the host eg emptydir hostpath this is the size of the underlying storage 

For Volumes that share a filesystem with the host eg emptydir hostpath this is the available space on the underlying storage and is shared with host processes and other Volumes 
underlying storage For Volumes that share a filesystem with the host eg emptydir hostpath this is the size of the underlying storage and will not equal Used  Available as the fs is shared Available represents the storage space available bytes for the Volume For Volumes that share a filesystem with the host eg 

For Volumes that share a filesystem with the host eg emptydir hostpath this is the available 
InodesFree represent the inodes available for the volume  For Volumes that share a filesystem with the host eg emptydir hostpath this is the free inodes 

For Volumes that share a filesystem with the host eg emptydir hostpath this is the available 
For volumes that share a filesystem with the host eg emptydir hostpath this is the inodes available in the underlying storage 

of the underlying storage and will not equal Used  Available as the fs is shared Available represents the storage space available bytes for the Volume For Volumes that share a filesystem with the host eg emptydir hostpath this is the available 
this is the inodes available in the underlying storage and will not equal InodesUsed  InodesFree as the fs is shared InodesFree represent the inodes available for the volume  For Volumes that share 

of the underlying storage and will not equal Used  Available as the fs is shared Available represents the storage space available bytes for the Volume For Volumes that share a filesystem with the host eg emptydir hostpath this is the available 
underlying storage For Volumes that share a filesystem with the host eg emptydir hostpath this is the size of the underlying storage 

For Volumes that share a filesystem with the host eg emptydir hostpath this is the size of the underlying storage and will not equal Used  Available as the fs is shared Available represents the storage space available bytes for the Volume 
InodesFree represent the inodes available for the volume  For Volumes that share a filesystem with the host eg emptydir hostpath this is the free inodes 

For Volumes that share a filesystem with the host eg emptydir hostpath this is the size of the underlying storage and will not equal Used  Available as the fs is shared 
and will not equal InodesUsed  InodesFree as the fs is shared InodesFree represent the inodes available for the volume  For Volumes that share a filesystem with the host eg emptydir hostpath this is the free inodes 

For Volumes that share a filesystem with the host eg emptydir hostpath this is the size of the underlying storage and will not equal Used  Available as the fs is shared 
this is the inodes available in the underlying storage and will not equal InodesUsed  InodesFree as the fs is shared InodesFree represent the inodes available for the volume  For Volumes that share 

For Volumes that share a filesystem with the host eg emptydir hostpath this is the size of the underlying storage and will not equal Used  Available as the fs is shared 
For volumes that share a filesystem with the host eg emptydir hostpath this is the inodes available in the underlying storage and will not equal InodesUsed  InodesFree as the fs is shared 

For Volumes that share a filesystem with the host eg emptydir hostpath this is the size of the underlying storage and will not equal Used  Available as the fs is shared 
and will not equal Used  Available as the fs is shared Available represents the storage space available bytes for the Volume For Volumes that share a filesystem with the host eg 

Capacity represents the total capacity bytes of the volumes underlying storage For Volumes that share a filesystem with the host eg emptydir hostpath this is the size of the underlying storage and will not equal Used  Available as the fs is shared Available represents the storage space available bytes for the Volume 
emptydir hostpath this is the available space on the underlying storage and is shared with host processes and other Volumes InodesUsed represents the total inodes used by the Volume Inodes represents the total number of inodes available in the volume For volumes that share a filesystem with the host eg emptydir hostpath 

Capacity represents the total capacity bytes of the volumes underlying storage For Volumes that share a filesystem with the host eg emptydir hostpath this is the size of the underlying storage and will not equal Used  Available as the fs is shared Available represents the storage space available bytes for the Volume 
Volume For Volumes that share a filesystem with the host eg emptydir hostpath this is the available space on the underlying storage and is shared with host processes and other Volumes InodesUsed represents the total inodes used by the Volume Inodes represents the total number of inodes available in the volume 

Capacity represents the total capacity bytes of the volumes underlying storage For Volumes that share a filesystem with the host eg emptydir hostpath this is the size of the underlying storage and will not equal Used  Available as the fs is shared 
emptydir hostpath this is the available space on the underlying storage and is shared with host processes and other Volumes InodesUsed represents the total inodes used by the Volume Inodes represents the total number of inodes available in the volume For volumes that share a filesystem with the host eg emptydir hostpath this is the inodes available in the underlying storage 

Capacity represents the total capacity bytes of the volumes underlying storage For Volumes that share a filesystem with the host eg emptydir hostpath this is the size 
Inodes represents the total number of inodes available in the volume For volumes that share a filesystem with the host eg emptydir hostpath this is the inodes available in the underlying storage and will not equal InodesUsed  InodesFree as the fs is shared InodesFree represent the inodes available for the volume  For Volumes that share a filesystem with the host eg emptydir hostpath this is the free inodes 

Capacity represents the total capacity bytes of the volumes underlying storage For Volumes that share a filesystem with the host eg emptydir hostpath this is the size 
Volume For Volumes that share a filesystem with the host eg emptydir hostpath this is the available space on the underlying storage and is shared with host processes and other Volumes InodesUsed represents the total inodes used by the Volume Inodes represents the total number of inodes available in the volume For volumes that share a filesystem with the host eg emptydir hostpath 

Capacity represents the total capacity bytes of the volumes underlying storage For Volumes that share a filesystem with the host eg emptydir hostpath this is the size 
eg emptydir hostpath this is the size of the underlying storage and will not equal Used  Available as the fs is shared Available represents the storage space available bytes for the Volume For Volumes that share a filesystem with the host eg 

Used represents the total bytes used by the Volume 
InodesUsed represents the total inodes used by the Volume 

GetMetrics returns the Metrics for the Volume  Maybe expensive for some implementations 
GetMetrics returns the Metrics for the Volume Maybe expensive for 

GetPath returns the directory path the volume is mounted to MetricsProvider embeds methods for exposing metrics eg usedavailable space MetricsProvider exposes metrics eg usedavailable space related to a Volume GetMetrics returns the Metrics for the Volume  Maybe expensive for some implementations Metrics represents the used and available bytes of the Volume Used represents the total bytes used by the Volume 
removes traces of the SetUp procedure TearDown unmounts the volume from the specified directory and removes traces of the SetUp procedure BlockVolumeMapper interface is a mapper interface for block volume CustomBlockVolumeMapper interface provides custom methods to set upmap the volume SetUpDevice prepares the volume to the node by the plugin specific way 

GetPath returns the directory path the volume is mounted to MetricsProvider embeds methods for exposing metrics eg usedavailable space MetricsProvider exposes metrics eg usedavailable space related to a Volume GetMetrics returns the Metrics for the Volume  Maybe expensive for some implementations Metrics represents the used and available bytes of the Volume 
TearDown unmounts the volume from the specified directory and removes traces of the SetUp procedure BlockVolumeMapper interface is a mapper interface for block volume CustomBlockVolumeMapper interface provides custom methods to set upmap the volume SetUpDevice prepares the volume to the node by the plugin specific way For most intree plugins attacherAttach and attacherWaitForAttach 

See the License for the specific language governing permissions and limitations under the License Volume represents a directory used by pods or hosts on a node All method implementations of methods in the volume interface must be idempotent GetPath returns the directory path the volume is mounted to 
and can create the volume as a new resource in the infrastructure provider Provision creates the resource by allocating the underlying volume in a storage system This method should block until completion and returns PersistentVolume representing the created storage resource Deleter removes the resource from the underlying storage provider Calls to this method should block until the deletion is complete Any error 

See the License for the specific language governing permissions and limitations under the License Volume represents a directory used by pods or hosts on a node All method implementations of methods in the volume interface must be idempotent GetPath returns the directory path the volume is mounted to 
TearDown unmounts the volume from the specified directory and removes traces of the SetUp procedure BlockVolumeMapper interface is a mapper interface for block volume CustomBlockVolumeMapper interface provides custom methods to set upmap the volume SetUpDevice prepares the volume to the node by the plugin specific way 

Detaches the disk from the kubelets host machine 
Detaches the block disk from the kubelets host machine 

rbd volume implements diskManager calls diskSetup when creating a volume and calls diskTearDown inside volume cleaner 
rbd volume implements diskManager calls diskSetup when creating a volume and calls diskTearDown inside volume unmounter 

diskManager interface and diskSetupTearDown functions abtract commonly used procedures to setup a block volume 
diskManager interface and diskSetupTearDown functions abstract commonly used procedures to setup a block volume 

uses the specified storage to retrieve service accounts and secrets 
uses the specified client to retrieve service accounts and secrets 

NewGetterFromStorageInterface returns a ServiceAccountTokenGetter that 
NewGetterFromClient returns a ServiceAccountTokenGetter that 

uses the specified registries to retrieve service accounts and secrets 
uses the specified client to retrieve service accounts and secrets 

NewGetterFromRegistries returns a ServiceAccountTokenGetter that 
NewGetterFromClient returns a ServiceAccountTokenGetter that 

This is the primary entrypoint for volume plugins 
ProbeVolumePlugins is the primary entrypoint for volume plugins 

GetString returns the time in the string format using the RFC3339Nano 
GetString returns the time in the string format using the RFC3339NanoFixed 

and converts it to a Timestamp object Get returns the time as timeTime GetString returns the time in the string format using the RFC3339Nano layout A type to help sort container statuses based on container names Reservation represents reserved resources for nonpod components 
Timestamp wraps around timeTime and offers utilities to format and parse the time using RFC3339Nano NewTimestamp returns a Timestamp object using the current time 

ConvertToTimestamp takes a string parses it using the RFC3339Nano layout 
ConvertToTimestamp takes a string parses it using the RFC3339NanoLenient layout 

This ensures a that its safe to use that port and b that a stays true  The risk is that some process on the node eg sshd or kubelet is using a port and we give that same port out to a Service  That would be bad because iptables would silently claim the traffic but the process would never know NOTE We should not need to have a real listening socket  bind 
servicePortEndpointChainName returns the name of the KUBESEPXXXX chain for a particular service endpoint After a UDP or SCTP endpoint has been removed we must flush any pending conntrack entries to it or else we risk sending more traffic to it all of which will be lost This assumes the proxier mutex is held TODO move it to util 

This ensures a that its safe to use that port and b that a stays true  The risk is that some process on the node eg sshd or kubelet is using a port and we give that same port out to a Service  That would be bad because iptables would silently claim the traffic but the process would never know 
Because of the iptables logic it is assumed that there is only a single Proxier active on a machine An error will be returned if iptables fails to update or acquire the initial lock Once a proxier is created it will keep iptables up to date in the background and will not terminate if a particular iptables call fails 

This ensures a that its safe to use that port and b that a stays true  The risk is that some process on the node eg sshd or kubelet is using a port and we give that same port out to a Service  That would be bad because iptables would silently claim the traffic but the process 
Because of the iptables logic it is assumed that there is only a single Proxier active on a machine An error will be returned if iptables fails to update or acquire the initial lock Once a proxier is created it will keep iptables up to date in the background and will not terminate if a particular iptables call fails Set the route_localnet sysctl we need for exposing NodePorts on loopback addresses 

use iptables to redirect traffic This ensures a that its safe to use that port and b that a stays true  The risk is that some process on the node eg sshd or kubelet is using a port and we give that same port out to a Service  That would be bad because iptables would silently claim the traffic but the process would never know 
servicePortEndpointChainName returns the name of the KUBESEPXXXX chain for a particular service endpoint After a UDP or SCTP endpoint has been removed we must flush any pending conntrack entries to it or else we risk sending more traffic to it all of which will be lost This assumes the proxier mutex is held TODO move it to util 

Finally tailcall to the nodeports chain  This needs to be after all other service portal rules Write the endoftable markers Sync rules NOTE NoFlushTables is used so we dont flush nonkubernetes chains in the table Revert new local ports 
Below this point we will not return until we try to write the iptables rules Reset all buffers used later This is to avoid memory reallocations and thus improve performance Write chain lines for all the toplevel chains well be filling in Install the kubernetesspecific postrouting rules We use a whole chain for 

Finally tailcall to the nodeports chain  This needs to be after all other service portal rules Write the endoftable markers Sync rules NOTE NoFlushTables is used so we dont flush nonkubernetes chains in the table Revert new local ports 
Create and link the kube chains Below this point we will not return until we try to write the iptables rules Reset all buffers used later This is to avoid memory reallocations and thus improve performance Write chain lines for all the toplevel chains well be filling in 

Finally tailcall to the nodeports chain  This needs to be after all other service portal rules Write the endoftable markers Sync rules NOTE NoFlushTables is used so we dont flush nonkubernetes chains in the table 
Below this point we will not return until we try to write the iptables rules Reset all buffers used later This is to avoid memory reallocations and thus improve performance Write chain lines for all the toplevel chains well be filling in Install the kubernetesspecific postrouting rules We use a whole chain for this so that it is easier to flush and change for example if the mark 

Finally tailcall to the nodeports chain  This needs to be after all other service portal rules Write the endoftable markers 
policycluster chain This allows traffic originating from the host to be redirected to the service correctly Anything else falls thru to the appropriate policy chain 

Finally tailcall to the nodeports chain  This needs to be after all other service portal rules Write the endoftable markers 
Redirect all srctypeLOCAL  external destination to the policycluster chain This allows traffic originating from the host to be redirected to the service correctly 

chain Finally tailcall to the nodeports chain  This needs to be after all other service portal rules Write the endoftable markers Sync rules NOTE NoFlushTables is used so we dont flush nonkubernetes chains in the table 
Below this point we will not return until we try to write the iptables rules Reset all buffers used later This is to avoid memory reallocations and thus improve performance Write chain lines for all the toplevel chains well be filling in Install the kubernetesspecific postrouting rules We use a whole chain for 

the nice effect of flushing the chain  Then we can remove the chain 
for it which has the nice effect of flushing the chain Then we can remove the chain 

We must as per iptables write a chainline for it which has the nice effect of flushing the chain  Then we can remove the 
We must as per iptables write a chainline for it which has the nice effect of flushing the chain Then we can remove the chain 

Now write loadbalancing  DNAT rules 
Now write loadbalancing rules 

This is imperfect in the face of network plugins that might not use a bridge but we can revisit that later Allow traffic bound for external IPs that happen to be recognized as local IPs to stay local This covers cases like GCE loadbalancers which get added to the local routing table Capture loadbalancer ingress We have to SNAT packets from external IPs Capture nodeports  If we had more than 2 rules it might be 
Below this point we will not return until we try to write the iptables rules Reset all buffers used later This is to avoid memory reallocations and thus improve performance Write chain lines for all the toplevel chains well be filling in Install the kubernetesspecific postrouting rules We use a whole chain for this so that it is easier to flush and change for example if the mark 

This rule roughly translates to all traffic from offmachine This is imperfect in the face of network plugins that might not use a bridge but we can revisit that later Allow traffic bound for external IPs that happen to be recognized as local IPs to stay local This covers cases like GCE loadbalancers which get added to the local routing table Capture loadbalancer ingress We have to SNAT packets from external IPs 
Local external traffic policy  It forwards traffic from local sources to the KUBESVCXXXX chain and traffic from external sources to the KUBESVLXXXX chain servicePortEndpointChainName returns the name of the KUBESEPXXXX chain for a particular service endpoint After a UDP or SCTP endpoint has been removed we must flush any pending conntrack entries to it or else we risk sending more traffic to it all of which will be lost 

nor from a local process to be forwarded to the service This rule roughly translates to all traffic from offmachine This is imperfect in the face of network plugins that might not use a bridge but we can revisit that later Allow traffic bound for external IPs that happen to be recognized as local IPs to stay local This covers cases like GCE loadbalancers which get added to the local routing table 
then encoding to base32 and truncating to 16 chars We do this because IPTables Chain Names must be  28 chars long and the longer they are the harder they are to read For cleanup  This can be removed after 126 is released servicePortPolicyClusterChain returns the name of the KUBESVCXXXX chain for a service which is the main iptables chain for that service used for dispatching to endpoints when using Cluster traffic policy 

nor from a local process to be forwarded to the service This rule roughly translates to all traffic from offmachine This is imperfect in the face of network plugins that might not use a bridge but we can revisit that later Allow traffic bound for external IPs that happen to be recognized as local IPs to stay local 
still needs masquerade because the LBIP itself is a local address so that will be the chosen source IP Redirect all srctypeLOCAL  external destination to the policycluster chain This allows traffic originating from the host to be redirected to the service correctly 

nor from a local process to be forwarded to the service This rule roughly translates to all traffic from offmachine This is imperfect in the face of network plugins that might not use a bridge but we can revisit that later 
servicePortEndpointChainName returns the name of the KUBESEPXXXX chain for a particular service endpoint After a UDP or SCTP endpoint has been removed we must flush any pending conntrack entries to it or else we risk sending more traffic to it all of which will be lost This assumes the proxier mutex is held TODO move it to util 

nor from a local process to be forwarded to the service This rule roughly translates to all traffic from offmachine This is imperfect in the face of network plugins that might not use a bridge but we can revisit that later 
chain and traffic from external sources to the KUBESVLXXXX chain servicePortEndpointChainName returns the name of the KUBESEPXXXX chain for a particular service endpoint After a UDP or SCTP endpoint has been removed we must flush any pending conntrack entries to it or else we risk sending more traffic to it all of which will be lost 

Allow traffic for external IPs that does not come from a bridge ie not from a container nor from a local process to be forwarded to the service This rule roughly translates to all traffic from offmachine This is imperfect in the face of network plugins that might not use a bridge but we can revisit that later Allow traffic bound for external IPs that happen to be recognized as local IPs to stay local This covers cases like GCE loadbalancers which get added to the local routing table 
form of traffic policy which simulates going upandout to an external loadbalancer and coming back in Locally originated traffic not a pod but the host node still needs masquerade because the LBIP itself is a local address so that will be the chosen source IP Redirect all srctypeLOCAL  external destination to the 

Were holding the port so its OK to install iptables rules We have to SNAT packets to external IPs Allow traffic for external IPs that does not come from a bridge ie not from a container nor from a local process to be forwarded to the service This rule roughly translates to all traffic from offmachine This is imperfect in the face of network plugins that might not use a bridge but we can revisit that later 
still needs masquerade because the LBIP itself is a local address so that will be the chosen source IP Redirect all srctypeLOCAL  external destination to the policycluster chain This allows traffic originating from the host to be redirected to the service correctly Anything else falls thru to the appropriate policy chain 

If the external IP happens to be an IP that is local to this machine hold the local port open so no other process can open it because the socket might open but it would never work Were holding the port so its OK to install iptables rules We have to SNAT packets to external IPs Allow traffic for external IPs that does not come from a bridge ie not from a container 
are connected to a Linux bridge but not SDN bridges  Until most plugins handle this log when config is missing Generate the masquerade mark to use for SNAT rules Log the IPs not matching the ipFamily We pass syncPeriod to iptMonitor which will call us only if it needs to 

Capture externalIPs 
Capture healthCheckNodePorts 

Capture externalIPs 
Capture nodeports 

Install the kubernetesspecific masquerade mark rule We use a whole chain for 
Install the kubernetesspecific postrouting rules We use a whole chain for 

Install the kubernetesspecific postrouting rules We use a whole chain for 
Install the kubernetesspecific masquerade mark rule We use a whole chain for 

Get iptablessave output so we can check for existing chains and rules This will be a map of chain name to chain with rules as stored in iptablessaveiptablesrestore if we failed to get any rules otherwise parse the output 
Below this point we will not return until we try to write the iptables rules Reset all buffers used later This is to avoid memory reallocations and thus improve performance Write chain lines for all the toplevel chains well be filling in Install the kubernetesspecific postrouting rules We use a whole chain for this so that it is easier to flush and change for example if the mark 

Create and link the kube postrouting chain 
Create and link the kube chains 

Create and link the kube services chain 
Create and link the kube chains 

assumes proxiermu is held 
Assumes proxiermu is held 

returns the associated iptables chain  This is computed by hashing sha256 then encoding to base32 and truncating with the prefix KUBESVC  We do this because Iptables Chain Names must be  28 chars long and the longer 
then encoding to base32 and truncating to 16 chars We do this because IPTables Chain Names must be  28 chars long and the longer they are the harder they are to read For cleanup  This can be removed after 126 is released servicePortPolicyClusterChain returns the name of the KUBESVCXXXX chain for a service which is the 

returns the associated iptables chain  This is computed by hashing sha256 then encoding to base32 and truncating with the prefix KUBESVC  We do this because Iptables Chain Names must be  28 chars long and the longer 
portProtoHash takes the ServicePortName and protocol for a service returns the associated 16 character hash This is computed by hashing sha256 then encoding to base32 and truncating to 16 chars We do this because IPTables Chain Names must be  28 chars long and the longer they are the harder they are to read 

servicePortChainName takes the ServicePortName for a service and returns the associated iptables chain  This is computed by hashing sha256 then encoding to base32 and truncating with the prefix KUBESVC  We do this because Iptables Chain Names must be  28 chars long and the longer 
Chain Names must be  28 chars long and the longer they are the harder they are to read For cleanup  This can be removed after 126 is released servicePortPolicyClusterChain returns the name of the KUBESVCXXXX chain for a service which is the main iptables chain for that service used for dispatching to endpoints when using Cluster 

servicePortChainName takes the ServicePortName for a service and 
portProtoHash takes the ServicePortName and protocol for a service 

Unlink the postrouting chain 
the kubernetes postrouting chain 

Unlink the postrouting chain 
the nodeports chain 

Unlink the services chain 
the external services chain 

Proxier implements ProxyProvider 
Proxier implements proxyProvider 

Check for the required sysctls  We dont care about the value just 
IsCompatible checks for the required sysctls  We dont care about the value just 

CanUseIptablesProxier returns true if we should use the iptables Proxier 
CanUseIPTablesProxier returns true if we should use the iptables Proxier 

the markformasquerade chain 
the kubernetes postrouting chain 

the markformasquerade chain 
the nodeports chain 

the kubernetes postrouting chain 
the nodeports chain 

the nodeports chain 
the kubernetes postrouting chain 

the services chain 
the external services chain 

iptablesMinVersion is the minimum version of iptables for which we will use the Proxier from this package instead of the userspace Proxier  While most of the features we need were available earlier the C flag was added more recently  We use that indirectly in Ensure functions and if we dont 
KernelCompatTester tests whether the required kernel capabilities are present to run the iptables proxier CanUseIPTablesProxier returns true if we should use the iptables Proxier instead of the classic userspace Proxier LinuxKernelCompatTester is the Linux implementation of KernelCompatTester IsCompatible checks for the required sysctls  We dont care about the value just 

newSourceApiserverFromLW holds creates a config source that watches and pulls from the apiserver 
NewSourceApiserver creates a config source that watches and pulls from the apiserver 

NewSourceApiserver creates a config source that watches and pulls from the apiserver 
newSourceApiserverFromLW holds creates a config source that watches and pulls from the apiserver 

Set default MaxSurge as 1 by default 
Set default MaxUnavailable as 1 by default 

Set default MaxSurge as 1 by default 
Set default MaxUnavailable as 1 by default 

Set default MaxUnavailable as 1 by default 
Set default MaxSurge as 1 by default 

Set default DeploymentStrategyType as RollingUpdate 
Set default extensionsv1beta1DeploymentStrategyType as RollingUpdate 

Set DeploymentSpecReplicas to 1 if it is not set 
Set extensionsv1beta1DeploymentSpecReplicas to 1 if it is not set 

Pods returns a string representating a list of pods in a human readable format 
PodDesc returns a string representing a pod in a consistent human readable format 

Pods returns a string representating a list of pods in a human readable format 
Pod returns a string representing a pod in a consistent human readable format 

Pod returns a string reprenetating a pod in a human readable format 
PodDesc returns a string representing a pod in a consistent human readable format 

Pod returns a string reprenetating a pod in a human readable format 
Pod returns a string representing a pod in a consistent human readable format 

This is the primary entrypoint for volume plugins 
ProbeVolumePlugins is the primary entrypoint for volume plugins 

downwardAPIVolumeCleaner implements volumeCleaner interface 
downwardAPIVolumeUnmounter implements volumeUnmounter interface 

downwardAPIVolumeCleaner implements volumeCleaner interface 
downwardAPIVolumeMounter implements volumeMounter interface 

downwardAPIVolumeCleander handles cleaning up downwardAPI volumes 
downwardAPIVolumeCleaner handles cleaning up downwardAPI volumes 

collectData collects requested downwardAPI in data map 
CollectData collects requested downwardAPI in data map 

downwardAPIVolumeBuilder implements volumeBuilder interface 
downwardAPIVolumeUnmounter implements volumeUnmounter interface 

downwardAPIVolumeBuilder implements volumeBuilder interface 
downwardAPIVolumeMounter implements volumeMounter interface 

and dumps it in files downwardAPIVolumeBuilder implements volumeBuilder interface downward API volumes are always ReadOnlyManaged SetUp puts in place the volume plugin This function is not idempotent by design We want the data to be refreshed periodically The internal sync interval of kubelet will drive the refresh of data 
CollectData collects requested downwardAPI in data map Maps key is the requested name of file to dump Maps value is the sorted content of the field to be dumped in the file Note this function is exported so that it can be called from the projection volume driver TODO unify with KubeletpodFieldSelectorRuntimeValue downwardAPIVolumeCleaner handles cleaning up downwardAPI volumes 

downwardAPIVolumeBuilder fetches info from downward API from the pod 
downwardAPIVolumeMounter fetches info from downward API from the pod 

TODO remove this redundancy as soon NewCleaner func will have apiPOD and not only typesUID 
TODO remove this redundancy as soon NewUnmounter func will have v1POD and not only typesUID 

NewServiceConfig creates a new ServiceConfig 
NewNodeConfig creates a new NodeConfig 

NewServiceConfig creates a new ServiceConfig 
NewEndpointSliceConfig creates a new EndpointSliceConfig 

NewServiceConfig creates a new ServiceConfig 
NewEndpointsConfig creates a new EndpointsConfig 

It accepts set add and remove operations of services via channels and invokes registered handlers on change 
It accepts set add and remove operations of node via channels and invokes registered handlers on change 

ServiceConfig tracks a set of service configurations 
EndpointSliceConfig tracks a set of endpoints configurations 

ServiceConfig tracks a set of service configurations 
EndpointsConfig tracks a set of endpoints configurations 

NewEndpointsConfig creates a new EndpointsConfig 
NewNodeConfig creates a new NodeConfig 

NewEndpointsConfig creates a new EndpointsConfig 
NewServiceConfig creates a new ServiceConfig 

NewEndpointsConfig creates a new EndpointsConfig 
NewEndpointSliceConfig creates a new EndpointSliceConfig 

It accepts set add and remove operations of endpoints via channels and invokes registered handlers on change 
It accepts set add and remove operations of node via channels and invokes registered handlers on change 

EndpointsConfig tracks a set of endpoints configurations 
NodeConfig tracks a set of node configurations 

EndpointsConfig tracks a set of endpoints configurations 
ServiceConfig tracks a set of service configurations 

EndpointsConfig tracks a set of endpoints configurations 
EndpointSliceConfig tracks a set of endpoints configurations 

OnEndpointsUpdate gets called when endpoints configuration is changed for a given service on any of the configuration sources An example is when a new 
NewServiceConfig creates a new ServiceConfig RegisterEventHandler registers a handler which is called on every service change Run waits for cache synced and invokes handlers after syncing NodeHandler is an abstract interface of objects which receive 

EndpointsConfigHandler is an abstract interface of objects which receive update notifications for the set of endpoints OnEndpointsUpdate gets called when endpoints configuration is changed for a given service on any of the configuration sources An example is when a new 
RegisterEventHandler registers a handler which is called on every service change Run waits for cache synced and invokes handlers after syncing NodeHandler is an abstract interface of objects which receive 

EndpointsConfigHandler is an abstract interface of objects which receive update notifications for the set of endpoints OnEndpointsUpdate gets called when endpoints configuration is changed for a given 
RegisterEventHandler registers a handler which is called on every service change Run waits for cache synced and invokes handlers after syncing NodeHandler is an abstract interface of objects which receive notifications about node object changes OnNodeAdd is called whenever creation of new node object is observed 

To remove all endpoints set Endpoints to empty array and Op to SET ServiceConfigHandler is an abstract interface of objects which receive update notifications for the set of services OnServiceUpdate gets called when a configuration has been changed by one of the sources This is the union of all the configuration sources EndpointsConfigHandler is an abstract interface of objects which receive update notifications for the set of endpoints OnEndpointsUpdate gets called when endpoints configuration is changed for a given 
called and the state is fully propagated to local cache EndpointsHandler is an abstract interface of objects which receive notifications about endpoints object changes This is not a required subinterface of proxyProvider and proxy implementations should 

Create an event recorder to record objects event except implicitly required containers like infra container 
FilterEventRecorder creates an event recorder to record objects event except implicitly required containers like infra container 

EnvVarsToMap constructs a map of environment name to value from a slice 
envVarsToMap constructs a map of environment name to value from a slice 

Gets all validated sources from the specified sources 
GetValidatedSources gets all validated sources from the specified sources 

Updates from Kubernetes API Server 
ApiserverSource identifies updates from Kubernetes API Server 

Updates from querying a web page 
HTTPSource identifies updates from querying a web page 

Updates from a file 
Filesource idenitified updates from a file 

This is the current pod configuration Pods with the given ids are new to this source 
SET is the current pod configuration ADD signifies pods that are new to this source 

PodOperation defines what changes will be made on a pod configuration This is the current pod configuration Pods with the given ids are new to this source Pods with the given ids have been removed from this source Pods with the given ids have been updated in this source Pods with the given ids have unexpected status in this source 
REMOVE signifies pods that have been removed from this source UPDATE signifies pods have been updated in this source RECONCILE signifies pods that have unexpected status in this source kubelet should reconcile status with this source These constants identify the sources of pods Filesource idenitified updates from a file 

PodOperation defines what changes will be made on a pod configuration This is the current pod configuration 
These constants identify the PodOperations that can be made on a pod configuration SET is the current pod configuration 

Detaches the disk from the kubelets host machine 
Detaches the block disk from the kubelets host machine 

iptables version check 
iptablesrestore version check 

iptables version check 
iptablesrestore version check 

iptables version check 
iptablesrestore version check 

iptables version check 
iptablesrestore version check 

iptables version check 
iptablesrestore version check 

iptables version check 
iptablesrestore version check 

iptables version check 
iptablesrestore version check 

iptables version check 
iptablesrestore version check 

iptables version check 
iptablesrestore version check 

iptables version check 
iptablesrestore version check 

iptables version check 
iptablesrestore version check 

iptables version check 
iptablesrestore version check 

iptables version check 
iptablesrestore version check 

iptables version check 
iptablesrestore version check 

iptables version check 
iptablesrestore version check 

iptables version check 
iptablesrestore version check 

iptables version check 
iptablesrestore version check 

iptables version check 
iptablesrestore version check 

iptables version check 
iptablesrestore version check 

iptables version check 
iptablesrestore version check 

iptables version check 
iptablesrestore version check 

iptables version check 
iptablesrestore version check 

iptables version check 
iptablesrestore version check 

iptables version check 
iptablesrestore version check 

iptables version check 
iptablesrestore version check 

iptables version check 
iptablesrestore version check 

iptables version check 
iptablesrestore version check 

iptables version check 
iptablesrestore version check 

iptables version check 
iptablesrestore version check 

iptables version check 
iptablesrestore version check 

iptables version check 
iptablesrestore version check 

iptables version check 
iptablesrestore version check 

iptables version check 
iptablesrestore version check 

iptables version check 
iptablesrestore version check 

iptables version check 
iptablesrestore version check 

iptables version check 
iptablesrestore version check 

The first Command call is checking the rule  Success of that exec means done 
The second Command call is checking the rule  Success of that means delete it 

The first Command call is checking the rule  Success of that exec means done 
The second Command call is checking the rule  Failure of that means create it 

The first Command call is checking the rule  Success of that exec means done 
The second Command call is checking the rule  Success of that means delete it 

The first Command call is checking the rule  Success of that exec means done 
The second Command call is checking the rule  Failure of that means create it 

The first Command call is checking the rule  Success of that exec means done 
The second Command call is checking the rule  Failure of that means create it 

The first Command call is checking the rule  Success of that exec means done 
The second Command call is checking the rule  Failure of that means create it 

The first Command call is checking the rule  Success of that exec means done 
The second Command call is checking the rule  Success of that exec means done 

The first Command call is checking the rule  Success of that exec means done 
The second Command call is checking the rule  Success of that means delete it 

The first Command call is checking the rule  Success of that exec means done 
The second Command call is checking the rule  Failure of that means create it 

The first Command call is checking the rule  Success of that exec means done 
The second Command call is checking the rule  Success of that means delete it 

The first Command call is checking the rule  Success of that exec means done 
The second Command call is checking the rule  Failure of that means create it 

The first Command call is checking the rule  Success of that exec means done 
The second Command call is checking the rule  Failure of that means create it 

The first Command call is checking the rule  Success of that exec means done 
The second Command call is checking the rule  Failure of that means create it 

The first Command call is checking the rule  Success of that exec means done 
The second Command call is checking the rule  Success of that exec means done 

The second Command call is checking the rule  Success of that means delete it 
The first Command call is checking the rule  Success of that exec means done 

The second Command call is checking the rule  Success of that means delete it 
The first Command call is checking the rule  Success of that exec means done 

The second Command call is checking the rule  Success of that means delete it 
The second Command call is checking the rule  Failure of that means create it 

The second Command call is checking the rule  Success of that means delete it 
The second Command call is checking the rule  Failure of that means create it 

The second Command call is checking the rule  Success of that means delete it 
The second Command call is checking the rule  Failure of that means create it 

The second Command call is checking the rule  Success of that means delete it 
The second Command call is checking the rule  Failure of that means create it 

The second Command call is checking the rule  Success of that means delete it 
The second Command call is checking the rule  Success of that exec means done 

iptables version check 
iptablesrestore version check 

iptables version check 
iptablesrestore version check 

iptables version check 
iptablesrestore version check 

iptables version check 
iptablesrestore version check 

iptables version check 
iptablesrestore version check 

iptables version check 
iptablesrestore version check 

iptables version check 
iptablesrestore version check 

iptables version check 
iptablesrestore version check 

iptables version check 
iptablesrestore version check 

Status 1 on the second call 
Status 1 on the first call 

Status 1 on the second call 
Status 1 on the first call 

Status 1 on the second call 
Status 1 on the first call 

Success on the first call 
Success on the second call 

Success on the first call 
Success on the second call 

iptables version check 
iptablesrestore version check 

iptables version check 
iptablesrestore version check 

iptables version check 
iptablesrestore version check 

iptables version check 
iptablesrestore version check 

iptables version check 
iptablesrestore version check 

iptables version check 
iptablesrestore version check 

iptables version check 
iptablesrestore version check 

iptables version check 
iptablesrestore version check 

iptables version check 
iptablesrestore version check 

The second Command call is checking the rule  Failure of that means create it 
The first Command call is checking the rule  Success of that exec means done 

The second Command call is checking the rule  Failure of that means create it 
The first Command call is checking the rule  Success of that exec means done 

The second Command call is checking the rule  Failure of that means create it 
The second Command call is checking the rule  Success of that means delete it 

The second Command call is checking the rule  Failure of that means create it 
The second Command call is checking the rule  Success of that means delete it 

The second Command call is checking the rule  Failure of that means create it 
The second Command call is checking the rule  Success of that exec means done 

iptables version check 
iptablesrestore version check 

iptables version check 
iptablesrestore version check 

iptables version check 
iptablesrestore version check 

iptables version check 
iptablesrestore version check 

iptables version check 
iptablesrestore version check 

iptables version check 
iptablesrestore version check 

iptables version check 
iptablesrestore version check 

iptables version check 
iptablesrestore version check 

iptables version check 
iptablesrestore version check 

Status 2 on the first call 
Status 1 on the first call 

Status 2 on the first call 
Status 1 on the first call 

Status 2 on the first call 
Status 1 on the first call 

iptables version check 
iptablesrestore version check 

iptables version check 
iptablesrestore version check 

iptables version check 
iptablesrestore version check 

iptables version check 
iptablesrestore version check 

iptables version check 
iptablesrestore version check 

iptables version check 
iptablesrestore version check 

iptables version check 
iptablesrestore version check 

iptables version check 
iptablesrestore version check 

iptables version check 
iptablesrestore version check 

The second Command call is checking the rule  Success of that means delete it 
The first Command call is checking the rule  Success of that exec means done 

The second Command call is checking the rule  Success of that means delete it 
The first Command call is checking the rule  Success of that exec means done 

The second Command call is checking the rule  Success of that means delete it 
The second Command call is checking the rule  Failure of that means create it 

The second Command call is checking the rule  Success of that means delete it 
The second Command call is checking the rule  Failure of that means create it 

The second Command call is checking the rule  Success of that means delete it 
The second Command call is checking the rule  Failure of that means create it 

The second Command call is checking the rule  Success of that means delete it 
The second Command call is checking the rule  Failure of that means create it 

The second Command call is checking the rule  Success of that means delete it 
The second Command call is checking the rule  Success of that exec means done 

iptables version check 
iptablesrestore version check 

iptables version check 
iptablesrestore version check 

iptables version check 
iptablesrestore version check 

iptables version check 
iptablesrestore version check 

iptables version check 
iptablesrestore version check 

iptables version check 
iptablesrestore version check 

iptables version check 
iptablesrestore version check 

iptables version check 
iptablesrestore version check 

iptables version check 
iptablesrestore version check 

Success on the second call 
Success on the first call 

Success on the second call 
Success on the first call 

Success on the first call 
Success on the second call 

Success on the first call 
Success on the second call 

iptables version check 
iptablesrestore version check 

iptables version check 
iptablesrestore version check 

iptables version check 
iptablesrestore version check 

iptables version check 
iptablesrestore version check 

iptables version check 
iptablesrestore version check 

iptables version check 
iptablesrestore version check 

iptables version check 
iptablesrestore version check 

iptables version check 
iptablesrestore version check 

iptables version check 
iptablesrestore version check 

iptables version check 
iptablesrestore version check 

iptables version check 
iptablesrestore version check 

iptables version check 
iptablesrestore version check 

iptables version check 
iptablesrestore version check 

iptables version check 
iptablesrestore version check 

iptables version check 
iptablesrestore version check 

iptables version check 
iptablesrestore version check 

iptables version check 
iptablesrestore version check 

iptables version check 
iptablesrestore version check 

Status 1 on the first call 
Status 1 on the second call 

Status 1 on the first call 
Status 2 on the first call 

Status 1 on the first call 
Status 1 on the second call 

Status 1 on the first call 
Status 2 on the first call 

iptables version check 
iptablesrestore version check 

iptables version check 
iptablesrestore version check 

iptables version check 
iptablesrestore version check 

iptables version check 
iptablesrestore version check 

iptables version check 
iptablesrestore version check 

iptables version check 
iptablesrestore version check 

iptables version check 
iptablesrestore version check 

iptables version check 
iptablesrestore version check 

iptables version check 
iptablesrestore version check 

The second Command call is checking the rule  Failure of that means create it 
The first Command call is checking the rule  Success of that exec means done 

The second Command call is checking the rule  Failure of that means create it 
The first Command call is checking the rule  Success of that exec means done 

The second Command call is checking the rule  Failure of that means create it 
The second Command call is checking the rule  Success of that means delete it 

The second Command call is checking the rule  Failure of that means create it 
The second Command call is checking the rule  Success of that means delete it 

The second Command call is checking the rule  Failure of that means create it 
The second Command call is checking the rule  Success of that exec means done 

iptables version check 
iptablesrestore version check 

iptables version check 
iptablesrestore version check 

iptables version check 
iptablesrestore version check 

iptables version check 
iptablesrestore version check 

iptables version check 
iptablesrestore version check 

iptables version check 
iptablesrestore version check 

iptables version check 
iptablesrestore version check 

iptables version check 
iptablesrestore version check 

iptables version check 
iptablesrestore version check 

Status 1 on the second call 
Status 1 on the first call 

Status 1 on the second call 
Status 1 on the first call 

Status 1 on the second call 
Status 1 on the first call 

Status 1 on the first call 
Status 1 on the second call 

Status 1 on the first call 
Status 2 on the first call 

Status 1 on the first call 
Status 1 on the second call 

Status 1 on the first call 
Status 2 on the first call 

iptables version check 
iptablesrestore version check 

iptables version check 
iptablesrestore version check 

iptables version check 
iptablesrestore version check 

iptables version check 
iptablesrestore version check 

iptables version check 
iptablesrestore version check 

iptables version check 
iptablesrestore version check 

iptables version check 
iptablesrestore version check 

iptables version check 
iptablesrestore version check 

iptables version check 
iptablesrestore version check 

The second Command call is checking the rule  Failure of that means create it 
The first Command call is checking the rule  Success of that exec means done 

The second Command call is checking the rule  Failure of that means create it 
The first Command call is checking the rule  Success of that exec means done 

The second Command call is checking the rule  Failure of that means create it 
The second Command call is checking the rule  Success of that means delete it 

The second Command call is checking the rule  Failure of that means create it 
The second Command call is checking the rule  Success of that means delete it 

The second Command call is checking the rule  Failure of that means create it 
The second Command call is checking the rule  Success of that exec means done 

iptables version check 
iptablesrestore version check 

iptables version check 
iptablesrestore version check 

iptables version check 
iptablesrestore version check 

iptables version check 
iptablesrestore version check 

iptables version check 
iptablesrestore version check 

iptables version check 
iptablesrestore version check 

iptables version check 
iptablesrestore version check 

iptables version check 
iptablesrestore version check 

iptables version check 
iptablesrestore version check 

Status 2 on the first call 
Status 1 on the first call 

Status 2 on the first call 
Status 1 on the first call 

Status 2 on the first call 
Status 1 on the first call 

iptables version check 
iptablesrestore version check 

iptables version check 
iptablesrestore version check 

iptables version check 
iptablesrestore version check 

iptables version check 
iptablesrestore version check 

iptables version check 
iptablesrestore version check 

iptables version check 
iptablesrestore version check 

iptables version check 
iptablesrestore version check 

iptables version check 
iptablesrestore version check 

iptables version check 
iptablesrestore version check 

The second Command call is checking the rule  Failure of that means create it 
The first Command call is checking the rule  Success of that exec means done 

The second Command call is checking the rule  Failure of that means create it 
The first Command call is checking the rule  Success of that exec means done 

The second Command call is checking the rule  Failure of that means create it 
The second Command call is checking the rule  Success of that means delete it 

The second Command call is checking the rule  Failure of that means create it 
The second Command call is checking the rule  Success of that means delete it 

The second Command call is checking the rule  Failure of that means create it 
The second Command call is checking the rule  Success of that exec means done 

iptables version check 
iptablesrestore version check 

iptables version check 
iptablesrestore version check 

iptables version check 
iptablesrestore version check 

iptables version check 
iptablesrestore version check 

iptables version check 
iptablesrestore version check 

iptables version check 
iptablesrestore version check 

iptables version check 
iptablesrestore version check 

iptables version check 
iptablesrestore version check 

iptables version check 
iptablesrestore version check 

Success on the second call 
Success on the first call 

Success on the second call 
Success on the first call 

Status 1 on the first call 
Status 1 on the second call 

Status 1 on the first call 
Status 2 on the first call 

Status 1 on the first call 
Status 1 on the second call 

Status 1 on the first call 
Status 2 on the first call 

iptables version check 
iptablesrestore version check 

iptables version check 
iptablesrestore version check 

iptables version check 
iptablesrestore version check 

iptables version check 
iptablesrestore version check 

iptables version check 
iptablesrestore version check 

iptables version check 
iptablesrestore version check 

iptables version check 
iptablesrestore version check 

iptables version check 
iptablesrestore version check 

iptables version check 
iptablesrestore version check 

The second Command call is checking the rule  Success of that exec means done 
The first Command call is checking the rule  Success of that exec means done 

The second Command call is checking the rule  Success of that exec means done 
The first Command call is checking the rule  Success of that exec means done 

The second Command call is checking the rule  Success of that exec means done 
The second Command call is checking the rule  Success of that means delete it 

The second Command call is checking the rule  Success of that exec means done 
The second Command call is checking the rule  Failure of that means create it 

The second Command call is checking the rule  Success of that exec means done 
The second Command call is checking the rule  Success of that means delete it 

The second Command call is checking the rule  Success of that exec means done 
The second Command call is checking the rule  Failure of that means create it 

The second Command call is checking the rule  Success of that exec means done 
The second Command call is checking the rule  Failure of that means create it 

The second Command call is checking the rule  Success of that exec means done 
The second Command call is checking the rule  Failure of that means create it 

iptables version check 
iptablesrestore version check 

iptables version check 
iptablesrestore version check 

iptables version check 
iptablesrestore version check 

iptables version check 
iptablesrestore version check 

iptables version check 
iptablesrestore version check 

iptables version check 
iptablesrestore version check 

iptables version check 
iptablesrestore version check 

iptables version check 
iptablesrestore version check 

iptables version check 
iptablesrestore version check 

iptables version check 
iptablesrestore version check 

iptables version check 
iptablesrestore version check 

iptables version check 
iptablesrestore version check 

iptables version check 
iptablesrestore version check 

iptables version check 
iptablesrestore version check 

iptables version check 
iptablesrestore version check 

iptables version check 
iptablesrestore version check 

iptables version check 
iptablesrestore version check 

iptables version check 
iptablesrestore version check 

iptables version check 
iptablesrestore version check 

iptables version check 
iptablesrestore version check 

iptables version check 
iptablesrestore version check 

iptables version check 
iptablesrestore version check 

iptables version check 
iptablesrestore version check 

iptables version check 
iptablesrestore version check 

iptables version check 
iptablesrestore version check 

iptables version check 
iptablesrestore version check 

iptables version check 
iptablesrestore version check 

iptables version check 
iptablesrestore version check 

iptables version check 
iptablesrestore version check 

iptables version check 
iptablesrestore version check 

iptables version check 
iptablesrestore version check 

iptables version check 
iptablesrestore version check 

iptables version check 
iptablesrestore version check 

iptables version check 
iptablesrestore version check 

iptables version check 
iptablesrestore version check 

iptables version check 
iptablesrestore version check 

iptables version check 
iptablesrestore version check 

iptables version check 
iptablesrestore version check 

iptables version check 
iptablesrestore version check 

iptables version check 
iptablesrestore version check 

iptables version check 
iptablesrestore version check 

iptables version check 
iptablesrestore version check 

iptables version check 
iptablesrestore version check 

iptables version check 
iptablesrestore version check 

iptables version check 
iptablesrestore version check 

AddReloadFunc is part of Interface 
ChainExists is part of Interface 

AddReloadFunc is part of Interface 
Monitor is part of Interface 

AddReloadFunc is part of Interface 
RestoreAll is part of Interface 

AddReloadFunc is part of Interface 
Restore is part of Interface 

AddReloadFunc is part of Interface 
SaveInto is part of Interface 

AddReloadFunc is part of Interface 
DeleteRule is part of Interface 

AddReloadFunc is part of Interface 
EnsureRule is part of Interface 

AddReloadFunc is part of Interface 
DeleteChain is part of Interface 

AddReloadFunc is part of Interface 
FlushChain is part of Interface 

AddReloadFunc is part of Interface 
EnsureChain is part of Interface 

getIptablesVersionString runs iptables version to get the version string 
getIPTablesRestoreVersionString runs iptablesrestore version to get the version string 

Checks if iptables version has a wait flag 
Checks if iptablesrestore has a wait flag 

Executes the rule check using the C flag 
Executes the rule check without using the C flag instead parsing iptablessave 

Executes the rule check without using the C flag instead parsing iptablessave 
Executes the rule check using the C flag 

RestoreAll is part of Interface 
ChainExists is part of Interface 

RestoreAll is part of Interface 
Monitor is part of Interface 

RestoreAll is part of Interface 
Restore is part of Interface 

RestoreAll is part of Interface 
SaveInto is part of Interface 

RestoreAll is part of Interface 
DeleteRule is part of Interface 

RestoreAll is part of Interface 
EnsureRule is part of Interface 

RestoreAll is part of Interface 
DeleteChain is part of Interface 

RestoreAll is part of Interface 
FlushChain is part of Interface 

RestoreAll is part of Interface 
EnsureChain is part of Interface 

Restore is part of Interface 
ChainExists is part of Interface 

Restore is part of Interface 
Monitor is part of Interface 

Restore is part of Interface 
RestoreAll is part of Interface 

Restore is part of Interface 
SaveInto is part of Interface 

Restore is part of Interface 
DeleteRule is part of Interface 

Restore is part of Interface 
EnsureRule is part of Interface 

Restore is part of Interface 
DeleteChain is part of Interface 

Restore is part of Interface 
FlushChain is part of Interface 

Restore is part of Interface 
EnsureChain is part of Interface 

SaveAll is part of Interface 
ChainExists is part of Interface 

SaveAll is part of Interface 
Monitor is part of Interface 

SaveAll is part of Interface 
RestoreAll is part of Interface 

SaveAll is part of Interface 
Restore is part of Interface 

SaveAll is part of Interface 
SaveInto is part of Interface 

SaveAll is part of Interface 
DeleteRule is part of Interface 

SaveAll is part of Interface 
EnsureRule is part of Interface 

SaveAll is part of Interface 
DeleteChain is part of Interface 

SaveAll is part of Interface 
FlushChain is part of Interface 

SaveAll is part of Interface 
EnsureChain is part of Interface 

Save is part of Interface 
ChainExists is part of Interface 

Save is part of Interface 
Monitor is part of Interface 

Save is part of Interface 
RestoreAll is part of Interface 

Save is part of Interface 
Restore is part of Interface 

Save is part of Interface 
SaveInto is part of Interface 

Save is part of Interface 
DeleteRule is part of Interface 

Save is part of Interface 
EnsureRule is part of Interface 

Save is part of Interface 
DeleteChain is part of Interface 

Save is part of Interface 
FlushChain is part of Interface 

Save is part of Interface 
EnsureChain is part of Interface 

DeleteRule is part of Interface 
ChainExists is part of Interface 

DeleteRule is part of Interface 
Monitor is part of Interface 

DeleteRule is part of Interface 
RestoreAll is part of Interface 

DeleteRule is part of Interface 
Restore is part of Interface 

DeleteRule is part of Interface 
SaveInto is part of Interface 

DeleteRule is part of Interface 
EnsureRule is part of Interface 

DeleteRule is part of Interface 
DeleteChain is part of Interface 

DeleteRule is part of Interface 
FlushChain is part of Interface 

DeleteRule is part of Interface 
EnsureChain is part of Interface 

EnsureRule is part of Interface 
ChainExists is part of Interface 

EnsureRule is part of Interface 
Monitor is part of Interface 

EnsureRule is part of Interface 
RestoreAll is part of Interface 

EnsureRule is part of Interface 
Restore is part of Interface 

EnsureRule is part of Interface 
SaveInto is part of Interface 

EnsureRule is part of Interface 
DeleteRule is part of Interface 

EnsureRule is part of Interface 
DeleteChain is part of Interface 

EnsureRule is part of Interface 
FlushChain is part of Interface 

EnsureRule is part of Interface 
EnsureChain is part of Interface 

DeleteChain is part of Interface 
ChainExists is part of Interface 

DeleteChain is part of Interface 
Monitor is part of Interface 

DeleteChain is part of Interface 
RestoreAll is part of Interface 

DeleteChain is part of Interface 
Restore is part of Interface 

DeleteChain is part of Interface 
SaveInto is part of Interface 

DeleteChain is part of Interface 
DeleteRule is part of Interface 

DeleteChain is part of Interface 
EnsureRule is part of Interface 

DeleteChain is part of Interface 
FlushChain is part of Interface 

DeleteChain is part of Interface 
EnsureChain is part of Interface 

FlushChain is part of Interface 
ChainExists is part of Interface 

FlushChain is part of Interface 
Monitor is part of Interface 

FlushChain is part of Interface 
RestoreAll is part of Interface 

FlushChain is part of Interface 
Restore is part of Interface 

FlushChain is part of Interface 
SaveInto is part of Interface 

FlushChain is part of Interface 
DeleteRule is part of Interface 

FlushChain is part of Interface 
EnsureRule is part of Interface 

FlushChain is part of Interface 
DeleteChain is part of Interface 

FlushChain is part of Interface 
EnsureChain is part of Interface 

EnsureChain is part of Interface 
ChainExists is part of Interface 

EnsureChain is part of Interface 
Monitor is part of Interface 

EnsureChain is part of Interface 
RestoreAll is part of Interface 

EnsureChain is part of Interface 
Restore is part of Interface 

EnsureChain is part of Interface 
SaveInto is part of Interface 

EnsureChain is part of Interface 
DeleteRule is part of Interface 

EnsureChain is part of Interface 
EnsureRule is part of Interface 

EnsureChain is part of Interface 
DeleteChain is part of Interface 

EnsureChain is part of Interface 
FlushChain is part of Interface 

Destroy is part of Interface 
ChainExists is part of Interface 

Destroy is part of Interface 
Monitor is part of Interface 

Destroy is part of Interface 
RestoreAll is part of Interface 

Destroy is part of Interface 
Restore is part of Interface 

Destroy is part of Interface 
SaveInto is part of Interface 

Destroy is part of Interface 
DeleteRule is part of Interface 

Destroy is part of Interface 
EnsureRule is part of Interface 

Destroy is part of Interface 
DeleteChain is part of Interface 

Destroy is part of Interface 
FlushChain is part of Interface 

Destroy is part of Interface 
EnsureChain is part of Interface 

data should be formatted like the output of Save 
data should be formatted like the output of SaveInto 

IsIpv6 returns true if this is managing ipv6 tables 
IsIPv6 returns true if this is managing ipv6 tables 

DeleteChain deletes the specified chain  If the chain did not exist return error EnsureRule checks if the specified rule is present and if not creates it  If the rule existed return true 
EnsureChain checks if the specified chain exists and if not creates it  If the chain existed return true 

DeleteChain deletes the specified chain  If the chain did not exist return error 
FlushChain clears the specified chain  If the chain did not exist return error 

FlushChain clears the specified chain  If the chain did not exist return error 
DeleteChain deletes the specified chain  If the chain did not exist return error 

EnsureChain checks if the specified chain exists and if not creates it  If the chain existed return true 
DeleteChain deletes the specified chain  If the chain did not exist return error ChainExists tests whether the specified chain exists returning an error if it does not or if it is unable to check EnsureRule checks if the specified rule is present and if not creates it  If the rule existed return true 

should be converted to an kubetypesADD kubetypesREMOVE and kubetypesUPDATE 
an kubetypesADD should be converted to kubetypesUPDATE 

an kubetypesADD should be converted to kubetypesUPDATE 
should be converted to an kubetypesADD kubetypesREMOVE and kubetypesUPDATE 

deterministically avoid syncing deployments that fight over ReplicaSets Currently we only ensure that the same deployment is synced for a given ReplicaSet When we periodically relist all deployments there will still be some ReplicaSet instability One way to handle this is by querying the store for all deployments that this deployment 
Only append if we care about this UID syncDeployment will sync the deployment with the given key This function is not meant to be invoked concurrently with the same key Deepcopy otherwise we are mutating our cache TODO Deepcopy only when needed List ReplicaSets owned by this Deployment while reconciling ControllerRef 

changed labels the new ReplicaSet will not be woken up till the periodic resync TODO Handle overlapping deployments better Either disallow them at admission time or deterministically avoid syncing deployments that fight over ReplicaSets Currently we only ensure that the same deployment is synced for a given ReplicaSet When we periodically relist all deployments there will still be some ReplicaSet instability One 
Pods from older versions are running before spinning up new Pods Only append if we care about this UID syncDeployment will sync the deployment with the given key This function is not meant to be invoked concurrently with the same key Deepcopy otherwise we are mutating our cache TODO Deepcopy only when needed 

changed labels the new ReplicaSet will not be woken up till the periodic 
changed labels the new deployment will not be woken up till the periodic resync 

changed labels the new ReplicaSet will not be woken up till the periodic 
changed labels the new deployment will not be woken up till the periodic resync 

the deleted keyvalue Note that this value might be stale If the pod 
the deleted keyvalue Note that this value might be stale If the Pod 

the deleted keyvalue Note that this value might be stale If the pod 
the deleted keyvalue Note that this value might be stale If the ReplicaSet 

in the list leading to the insertion of a tombstone object which contains the deleted keyvalue Note that this value might be stale If the pod changed labels the new ReplicaSet will not be woken up till the periodic resync TODO Handle overlapping deployments better Either disallow them at admission time or deterministically avoid syncing deployments that fight over ReplicaSets Currently we 
item in this list nevertheless constitutes user error updateReplicaSet figures out what deployments manage a ReplicaSet when the ReplicaSet is updated and wake them up If the anything of the ReplicaSets have changed we need to awaken both the old and new deployments old and cur must be appsReplicaSet types 

obj could be an apiPod or a DeletionFinalStateUnknown marker item When a delete is dropped the relist will notice a pod in the store not 
deleteReplicaSet enqueues the deployment that manages a ReplicaSet when the ReplicaSet is deleted obj could be an appsReplicaSet or a DeletionFinalStateUnknown marker item 

When a pod is deleted ensure its controller syncs obj could be an apiPod or a DeletionFinalStateUnknown marker item 
deleteReplicaSet enqueues the deployment that manages a ReplicaSet when the ReplicaSet is deleted obj could be an appsReplicaSet or a DeletionFinalStateUnknown marker item 

the old and new deployments old and cur must be apiPod types 
awaken both the old and new deployments old and cur must be appsReplicaSet 

is updated and wake them up If anything of the Pods have changed we need to awaken both 
is updated and wake them up If the anything of the ReplicaSets have changed we need to 

getDeploymentForPod returns the deployment managing the ReplicaSet that manages the given Pod 
getDeploymentForPod returns the deployment managing the given Pod 

the deleted keyvalue Note that this value might be stale If the ReplicaSet 
the deleted keyvalue Note that this value might be stale If the Pod 

the ReplicaSet is deleted obj could be an extensionsReplicaSet or 
the ReplicaSet is deleted obj could be an appsReplicaSet or 

awaken both the old and new deployments old and cur must be extensionsReplicaSet 
awaken both the old and new deployments old and cur must be appsReplicaSet 

trying to clean up one of the controllers for now we just return one of the two 
trying to clean up one of the controllers for now we just return the older one 

TODO Surface that we are ignoring multiple deployments for a given ReplicaSet Because all ReplicaSets belonging to a deployment should have a unique label key there should never be more than one deployment returned by the above method If that happens we should probably dynamically repair the situation by ultimately 
List all ReplicaSets to find those we own but that no longer match our selector They will be orphaned by ClaimReplicaSets If any adoptions are attempted we should first recheck for deletion with an uncached quorum read sometime after listing ReplicaSets see 42639 getPodMapForDeployment returns the Pods managed by a Deployment It returns a map from ReplicaSet UID to a list of Pods controlled by that RS 

getDeploymentForReplicaSet returns the deployment managing the given ReplicaSet 
getDeploymentForPod returns the deployment managing the given Pod 

podStoreSynced returns true if the pod store has been synced at least once 
podListerSynced returns true if the pod store has been synced at least once 

podStoreSynced returns true if the pod store has been synced at least once 
rsListerSynced returns true if the ReplicaSet store has been synced at least once 

podStoreSynced returns true if the pod store has been synced at least once 
dListerSynced returns true if the Deployment store has been synced at least once 

rsStoreSynced returns true if the ReplicaSet store has been synced at least once 
podListerSynced returns true if the pod store has been synced at least once 

rsStoreSynced returns true if the ReplicaSet store has been synced at least once 
rsListerSynced returns true if the ReplicaSet store has been synced at least once 

rsStoreSynced returns true if the ReplicaSet store has been synced at least once 
dListerSynced returns true if the Deployment store has been synced at least once 

More info httpreleasesk8siorelease12docsuserguidelabelsmdlabelselectors 
More info httpsgitk8siocommunitycontributorsdevelsigarchitectureapiconventionsmdspecandstatus 

More info httpreleasesk8siorelease12docsuserguidelabelsmdlabelselectors 
More info httpsgitk8siocommunitycontributorsdevelsigarchitectureapiconventionsmdmetadata 

More info httpreleasesk8siorelease12docsuserguidelabelsmdlabelselectors 
More info httpskubernetesiodocsconceptsoverviewworkingwithobjectslabelslabelselectors 

actual number of observed instances of the scaled object label query over pods that should match the replicas count This is same as the label selector but in the string format to avoid introspection 
Name is the name of the resource in question Target specifies the target value for the given metric ContainerResourceMetricSource indicates how to scale on a resource metric known to Kubernetes as specified in the requests and limits describing a single container in each of the pods of the current scale targeteg CPU or memory The values will be averaged together before being compared to the target Such metrics are built into 

actual number of observed instances of the scaled object label query over pods that should match the replicas count This is same as the label selector but in the string format to avoid introspection 
should be set Name is the name of the resource in question Target specifies the target value for the given metric ContainerResourceMetricSource indicates how to scale on a resource metric known to Kubernetes as specified in the requests and limits describing a single container in each of the pods of the current scale targeteg CPU or memory The values will be 

ScaleStatus represents the current status of a scale subresource actual number of observed instances of the scaled object label query over pods that should match the replicas count This is same as the label selector but in the string format to avoid introspection by clients The string will be in the same format as the queryparam syntax More info httpreleasesk8siorelease12docsuserguidelabelsmdlabelselectors 
should be set Name is the name of the resource in question Target specifies the target value for the given metric ContainerResourceMetricSource indicates how to scale on a resource metric known to Kubernetes as specified in the requests and limits describing a single container in 

ScaleStatus represents the current status of a scale subresource actual number of observed instances of the scaled object label query over pods that should match the replicas count This is same as the label selector but in the string format to avoid introspection by clients The string will be in the same format as the queryparam syntax 
Kubernetes as specified in the requests and limits describing a single container in each of the pods of the current scale targeteg CPU or memory The values will be averaged together before being compared to the target Such metrics are built into 

ScaleStatus represents the current status of a scale subresource actual number of observed instances of the scaled object label query over pods that should match the replicas count This is same as the label selector but in the string format to avoid introspection 
container is the name of the container in the pods of the scaling target target specifies the target value for the given metric ExternalMetricSource indicates how to scale on a metric not associated with 

ScaleStatus represents the current status of a scale subresource actual number of observed instances of the scaled object label query over pods that should match the replicas count This is same as the label selector but in the string format to avoid introspection 
Kubernetes and have special scaling options on top of those available to normal perpod metrics using the pods source Only one target type should be set name is the name of the of the resource container is the name of the container in the pods of the scaling target target specifies the target value for the given metric 

ScaleStatus represents the current status of a scale subresource actual number of observed instances of the scaled object label query over pods that should match the replicas count This is same as the label selector but in the string format to avoid introspection 
averaged together before being compared to the target Such metrics are built into Kubernetes and have special scaling options on top of those available to normal perpod metrics using the pods source Only one target type should be set name is the name of the of the resource container is the name of the container in the pods of the scaling target 

ScaleStatus represents the current status of a scale subresource actual number of observed instances of the scaled object label query over pods that should match the replicas count This is same as the label selector but in the string format to avoid introspection 
Target specifies the target value for the given metric ContainerResourceMetricSource indicates how to scale on a resource metric known to Kubernetes as specified in the requests and limits describing a single container in each of the pods of the current scale targeteg CPU or memory The values will be 

ScaleStatus represents the current status of a scale subresource actual number of observed instances of the scaled object label query over pods that should match the replicas count This is same 
name is the name of the of the resource container is the name of the container in the pods of the scaling target target specifies the target value for the given metric ExternalMetricSource indicates how to scale on a metric not associated with any Kubernetes object for example length of queue in cloud 

ScaleStatus represents the current status of a scale subresource 
MetricValueStatus indicates the current value of a metric 

ScaleStatus represents the current status of a scale subresource 
HorizontalPodAutoscalerStatus describes the current status of a horizontal pod autoscaler 

ScaleStatus represents the current status of a scale subresource 
ScaleSpec describes the attributes of a scale subresource 

desired number of instances for the scaled object ScaleStatus represents the current status of a scale subresource actual number of observed instances of the scaled object label query over pods that should match the replicas count This is same as the label selector but in the string format to avoid introspection by clients The string will be in the same format as the queryparam syntax 
Kubernetes as specified in the requests and limits describing a single container in each of the pods of the current scale targeteg CPU or memory The values will be 

desired number of instances for the scaled object ScaleStatus represents the current status of a scale subresource actual number of observed instances of the scaled object label query over pods that should match the replicas count This is same as the label selector but in the string format to avoid introspection 
Kubernetes and have special scaling options on top of those available to normal perpod metrics using the pods source Name is the name of the resource in question ExternalMetricStatus indicates the current value of a global metric not associated with any Kubernetes object 

desired number of instances for the scaled object ScaleStatus represents the current status of a scale subresource actual number of observed instances of the scaled object label query over pods that should match the replicas count This is same as the label selector but in the string format to avoid introspection 
Kubernetes and have special scaling options on top of those available to normal perpod metrics using the pods source Name is the name of the resource in question ContainerResourceMetricStatus indicates the current value of a resource metric known to Kubernetes as specified in requests and limits describing each pod in the 

desired number of instances for the scaled object ScaleStatus represents the current status of a scale subresource actual number of observed instances of the scaled object label query over pods that should match the replicas count This is same as the label selector but in the string format to avoid introspection 
Kubernetes and have special scaling options on top of those available to normal perpod metrics using the pods source Only one target type should be set name is the name of the of the resource container is the name of the container in the pods of the scaling target 

desired number of instances for the scaled object ScaleStatus represents the current status of a scale subresource actual number of observed instances of the scaled object label query over pods that should match the replicas count This is same 
name is the name of the of the resource container is the name of the container in the pods of the scaling target target specifies the target value for the given metric ExternalMetricSource indicates how to scale on a metric not associated with 

desired number of instances for the scaled object ScaleStatus represents the current status of a scale subresource 
AverageUtilization is the target value of the average of the resource metric across all relevant pods represented as a percentage of the requested value of the resource for the pods 

ScaleSpec describes the attributes of a scale subresource desired number of instances for the scaled object ScaleStatus represents the current status of a scale subresource actual number of observed instances of the scaled object label query over pods that should match the replicas count This is same as the label selector but in the string format to avoid introspection 
PodsMetricStatus indicates the current value of a metric describing each pod in the current scale target for example transactionsprocessedpersecond ResourceMetricStatus indicates the current value of a resource metric known to 

ScaleSpec describes the attributes of a scale subresource desired number of instances for the scaled object ScaleStatus represents the current status of a scale subresource actual number of observed instances of the scaled object label query over pods that should match the replicas count This is same as the label selector but in the string format to avoid introspection 
the transition optional MetricStatus describes the lastread state of a single metric Type is the type of metric source  It will be one of Object Pods or Resource each corresponds to a matching field in the object 

ScaleSpec describes the attributes of a scale subresource desired number of instances for the scaled object ScaleStatus represents the current status of a scale subresource 
QPS from loadbalancer running outside of cluster optional ObjectMetricStatus indicates the current value of a metric describing a 

ScaleSpec describes the attributes of a scale subresource 
ScaleStatus represents the current status of a scale subresource 

current status of the scale More info httpreleasesk8siorelease12docsdevelapiconventionsmdspecandstatus Readonly 
current status of the scale More info httpsgitk8siocommunitycontributorsdevelsigarchitectureapiconventionsmdspecandstatus Readonly 

defines the behavior of the scale More info httpreleasesk8siorelease12docsdevelapiconventionsmdspecandstatus 
defines the behavior of the scale More info httpsgitk8siocommunitycontributorsdevelsigarchitectureapiconventionsmdspecandstatus 

Standard object metadata More info httpreleasesk8siorelease12docsdevelapiconventionsmdmetadata 
Standard object metadata More info httpsgitk8siocommunitycontributorsdevelsigarchitectureapiconventionsmdmetadata 

ReplicaSetsByCreationTimestamp sorts a list of ReplicationSets by creation timestamp using their names as a tie breaker 
ReplicaSetsByCreationTimestamp sorts a list of ReplicaSet by creation timestamp using their names as a tie breaker 

ReplicaSetsByCreationTimestamp sorts a list of ReplicationSets by creation timestamp using their names as a tie breaker 
ControllersByCreationTimestamp sorts a list of ReplicationControllers by creation timestamp using their names as a tie breaker 

ControllersByCreationTimestamp sorts a list of ReplicationControllers by creation timestamp using their names as a tie breaker 
ReplicaSetsByCreationTimestamp sorts a list of ReplicaSet by creation timestamp using their names as a tie breaker 

expcept theres not possibility for error since we know the exact type 
except theres not possibility for error since we know the exact type 

6 Empty creation time pods  newer pods  older pods 
8 Empty creation time pods  newer pods  older pods 

5 Pods with containers with higher restart counts  lower restart counts 
7 Pods with containers with higher restart counts  lower restart counts 

4 Been ready for empty time  less time  more time 
6 Been ready for empty time  less time  more time 

4 Been ready for empty time  less time  more time 
4 Been ready for more time  less time  empty time 

2 PodPending  PodUnknown  PodRunning 
2 PodRunning  PodUnknown  PodPending 

If only one of the pods is unassigned the unassigned one is smaller 2 PodPending  PodUnknown  PodRunning 3 Not ready  ready If only one of the pods is not ready the not ready one is smaller 
3 If exactly one of the pods is ready the pod that is not ready comes 

1 Unassigned  assigned 
1 assigned  unassigned 

would be broken upon reading it This is explicitly hardcoded to v1 to guarantee predictable deployment We need to consistently handle this case of annotation versioning use the dash if the name isnt too long to make the pod name a bit prettier ActivePods type allows custom sorting of pods so a controller can pick the best ones to delete 1 Unassigned  assigned If only one of the pods is unassigned the unassigned one is smaller 
Warning if using KeyFunc it is not safe to use a single ControllerExpectationsInterface with different types of controllers because the keys might conflict across types ControllerExpectations is a cache mapping controllers to what they expect to see before being woken up for a sync GetExpectations returns the ControlleeExpectations of the given controller 

RealPodControl is the default implementation of PodControlInterface TODO this code was not safe previously  as soon as new code came along that switched to v2 old clients would be broken upon reading it This is explicitly hardcoded to v1 to guarantee predictable deployment We need to consistently handle this case of annotation versioning use the dash if the name isnt too long to make the pod name a bit prettier ActivePods type allows custom sorting of pods so a controller can pick the best ones to delete 
waiting for UIDTrackingControllerExpectations tracks the UID of the pods it deletes This cache is needed over plain old expectations to safely handle graceful deletion The desired behavior is to treat an update that sets the DeletionTimestamp on an object as a delete To do so consistently one needs to remember the expected deletes so they arent double counted 

RealPodControl is the default implementation of PodControlInterface 
RealControllerRevisionControl is the default implementation of ControllerRevisionControlInterface 

RealPodControl is the default implementation of PodControlInterface 
RealRSControl is the default implementation of RSControllerInterface 

PodControlInterface is an interface that knows how to add or delete pods 
RSControlInterface is an interface that knows how to add or delete 

DeletionTimestamp on an object as a delete To do so consistenly one needs 
DeletionTimestamp on an object as a delete To do so consistently one needs 

UIDSetKeyFunc to parse out the key from a UIDSet 
ExpKeyFunc to parse out the key from a ControlleeExpectation 

CreationObserved atomically decrements the add expecation count of the given controller DeletionObserved atomically decrements the del expectation count of the given controller 
Increments the expectation counts of the given controller 

CreationObserved atomically decrements the add expecation count of the given controller DeletionObserved atomically decrements the del expectation count of the given controller 
Decrements the expectation counts of the given controller 

CreationObserved atomically decrements the add expecation count of the given controller 
CreationObserved atomically decrements the add expectation count of the given controller 

Increments the expectation counts of the given controller 
CreationObserved atomically decrements the add expectation count of the given controller DeletionObserved atomically decrements the del expectation count of the given controller 

Increments the expectation counts of the given controller 
Decrements the expectation counts of the given controller 

Increments the expectation counts of the given controller 
DeleteExpectations deletes the expectations of the given controller from the TTLStore 

Increments the expectation counts of the given controller 
GetExpectations returns the ControlleeExpectations of the given controller 

Decrements the expectation counts of the given controller 
CreationObserved atomically decrements the add expectation count of the given controller DeletionObserved atomically decrements the del expectation count of the given controller 

Decrements the expectation counts of the given controller 
Increments the expectation counts of the given controller 

Decrements the expectation counts of the given controller 
DeleteExpectations deletes the expectations of the given controller from the TTLStore 

Decrements the expectation counts of the given controller 
GetExpectations returns the ControlleeExpectations of the given controller 

DeleteExpectations deletes the expectations of the given controller from the TTLStore 
Increments the expectation counts of the given controller 

DeleteExpectations deletes the expectations of the given controller from the TTLStore 
Decrements the expectation counts of the given controller 

DeleteExpectations deletes the expectations of the given controller from the TTLStore 
GetExpectations returns the ControlleeExpectations of the given controller 

GetExpectations returns the ControlleeExpectations of the given controller 
Increments the expectation counts of the given controller 

GetExpectations returns the ControlleeExpectations of the given controller 
Decrements the expectation counts of the given controller 

GetExpectations returns the ControlleeExpectations of the given controller 
DeleteExpectations deletes the expectations of the given controller from the TTLStore 

ExpKeyFunc to parse out the key from a ControlleeExpectation 
UIDSetKeyFunc to parse out the key from a UIDSet 

		controller2 expects  2 dels in 2 minutes 
		controller1 expects  2 adds in 2 minutes 

		controller1 expects  2 adds in 2 minutes 
		controller2 expects  2 dels in 2 minutes 

ValidSecurityContextWithContainerDefaults creates a valid security context provider based on 
ValidInternalSecurityContextWithContainerDefaults creates a valid security context provider based on 

If lookup is true the service account and secret referenced as claims inside the token are retrieved and verified with the provided ServiceAccountTokenGetter Attempt to verify with each key until we find one that works Not a JWT no point in continuing Signature error perhaps one of the other keys will verify the signature If not we want to return this error 
If we get here we have a token with a recognized signature and issuer string hasCorrectIssuer returns true if tokenData is a valid JWT in compact serialization format and the iss claim matches the iss field of this token authenticator and otherwise returns false Note gojose currently does not allow access to unverified JWS payloads 

If lookup is true the service account and secret referenced as claims inside the token are retrieved and verified with the provided ServiceAccountTokenGetter Attempt to verify with each key until we find one that works Not a JWT no point in continuing Signature error perhaps one of the other keys will verify the signature If not we want to return this error 
call This struct should contain fields for any private claims that the Validator requires to validate the JWT TODO Pick the key that has the same key ID as tok if one exists only apiserver audiences are allowed for legacy tokens default to apiserver audiences If we get here we have a token with a recognized signature and 

JWTTokenGenerator returns a TokenGenerator that generates signed JWT tokens using the given privateKey privateKey is a PEMencoded byte array of a private RSA key JWTTokenAuthenticator Identify the issuer Username Persist enough structured info for the authenticator to be able to look up the service account and secret 
Validator can assume that the issuer and signature of a token are already verified when this function is called NewPrivateClaims returns a struct that the authenticator should deserialize the JWT payload into The authenticator may then pass this struct back to the Validator as the private argument to a Validate call This struct should contain fields for any private claims that the 

JWTTokenGenerator returns a TokenGenerator that generates signed JWT tokens using the given privateKey privateKey is a PEMencoded byte array of a private RSA key JWTTokenAuthenticator Identify the issuer Username Persist enough structured info for the authenticator to be able to look up the service account and secret 
Validate validates a token and returns user information or an error Validator can assume that the issuer and signature of a token are already verified when this function is called NewPrivateClaims returns a struct that the authenticator should deserialize the JWT payload into The authenticator may then pass this struct back to the Validator as the private argument to a Validate 

Adds the list of known types to apiScheme 
Adds the list of known types to the given scheme 

Resource takes an unqualified resource and returns back a Group qualified GroupResource 
Resource takes an unqualified resource and returns a Group qualified GroupResource 

Kind takes an unqualified kind and returns back a Group qualified GroupKind 
Kind takes an unqualified kind and returns a Group qualified GroupKind 

Wrap EmptyDir let it do the teardown 
Wrap EmptyDir let it do the setup 

configMapVolumeCleaner handles cleaning up configMap volumes 
configMapVolumeUnmounter handles cleaning up configMap volumes 

configMapVolumeBuilder handles retrieving secrets from the API server 
configMapVolumeMounter handles retrieving secrets from the API server 

ProbeVolumePlugin is the entry point for plugin detection in a package 
ProbeVolumePlugins is the entry point for plugin detection in a package 

if device is no longer used see if need to logout the target 
If device is no longer used see if need to logout the target 

if device is no longer used see if need to logout the target 
If we arrive here device is no longer used see if need to logout the target 

if device is no longer used see if need to logout the target 
If device is no longer used see if need to logout the target 

make a directory like varlibkubeletpluginskubernetesioiscsiportalsome_iqnlunlun_id 
make a directory like varlibkubeletpluginskubernetesioiscsivolumeDevicesiface_nameportalsome_iqnlunlun_id 

make a directory like varlibkubeletpluginskubernetesioiscsiportalsome_iqnlunlun_id 
make a directory like varlibkubeletpluginskubernetesioiscsiiface_nameportalsome_iqnlunlun_id 

an iSCSI target could expose multiple LUNs through the same IQN and Linux iSCSI initiator creates disk paths that start the same prefix but end with different LUN number When we decide whether it is time to logout a target we have to see if none of the LUNs are used any more Thats where the prefix based ref count kicks in If we only count the disks using exact match we could log other disks out Find the number of references to the device 
If the initiator name was set the iface isnt created yet so create it and copy parameters from the preconfigured one Lock the target while we login to avoid races between 2 volumes that share the same target both logging in or one logging out while another logs in Build a map of SCSI hosts for each target portal We will need this to 

this function aggregates all references to a service based on the prefix pattern More specifically this prefix semantics is to aggregate disk paths that belong to the same iSCSI targetiqn pair an iSCSI target could expose multiple LUNs through the same IQN and Linux iSCSI initiator creates disk paths that start the same prefix but end with different LUN number When we decide whether it is time to logout a target we have to see if none of the LUNs are used any more 
If the initiator name was set the iface isnt created yet so create it and copy parameters from the preconfigured one Lock the target while we login to avoid races between 2 volumes that share the same target both logging in or one logging out while another logs in Build a map of SCSI hosts for each target portal We will need this to 

the status of the pod for the first pod worker sync See corresponding comment in syncPod If there is an undelivered work update for this pod we need to remove it since perpod goroutine wont be able to put it to the already closed channel when it finish processing the current work update Requeue the last update if the last sync returned error 
if syncPod indicated we are now terminal set the appropriate pod status to move to terminating queue a retry if necessary then put the next event in the channel if any acknowledgeTerminating sets the terminating flag on the pod status once the pod worker sees the termination state so that other components know no new containers will be started in this pod It then returns the status function if any that applies to this pod 

the status of the pod for the first pod worker sync See corresponding comment in syncPod If there is an undelivered work update for this pod we need to remove it since perpod goroutine wont be able to put it to the already closed 
pods that dont exist in config dont need to be terminated garbage collection will cover them otherwise we move to the terminating phase if syncPod indicated we are now terminal set the appropriate pod status to move to terminating queue a retry if necessary then put the next event in the channel if any acknowledgeTerminating sets the terminating flag on the pod status once the pod worker sees the termination state so that other components know no new containers will be started in this 

the status of the pod for the first pod worker sync See corresponding comment in syncPod If there is an undelivered work update for this pod we need to remove it since perpod goroutine wont be able to put it to the already closed 
KillPodOptions is not valid for sync actions outside of the terminating phase the desired work we want to be performing start the pod worker goroutine if it doesnt exist We need to have a buffer here because checkForUpdates method that puts an update into channel is called from the same goroutine where 

the status of the pod for the first pod worker sync See corresponding comment in syncPod If there is an undelivered work update for this pod we need to remove it since perpod goroutine wont be able to put it to the already closed 
due to housekeeping seeing an older cached version of the runtime pod simply ignore it until after the pod worker completes always set the grace period for syncTerminatingPod so we dont have to recalculate will never be zero 

kubelet just restarted In either case the kubelet is willing to believe the status of the pod for the first pod worker sync See corresponding comment in syncPod If there is an undelivered work update for this pod we need to remove it since perpod goroutine wont be able to put it to the already closed channel when it finish processing the current work update 
If this succeeds then record in the podWorker that it is terminated if an update is received that implies the pod should be running but we are already terminating a pod by that UID assume that two pods with the same UID were created in close temporal proximity usually static pod but its possible for an apiserver to extremely rarely do something similar  flag the sync status to indicate that after the pod terminates it should be reset to not running to allow a subsequent addupdate to start the pod worker again 

Creating a new pod worker either means this is a new pod or that the kubelet just restarted In either case the kubelet is willing to believe the status of the pod for the first pod worker sync See corresponding comment in syncPod If there is an undelivered work update for this pod we need to remove it since perpod goroutine wont be able to put it to the already closed 
the desired work we want to be performing start the pod worker goroutine if it doesnt exist We need to have a buffer here because checkForUpdates method that puts an update into channel is called from the same goroutine where the channel is consumed However it is guaranteed that in such case 

Creating a new pod worker either means this is a new pod or that the kubelet just restarted In either case the kubelet is willing to believe the status of the pod for the first pod worker sync See corresponding comment in syncPod If there is an undelivered work update for this pod we need to remove it since perpod goroutine wont be able to put it to the already closed 
check for a transition to terminating once a pod is terminating all updates are kills and the grace period can only decrease A terminated pod may still be waiting for cleanup  if we receive a runtime pod kill request due to housekeeping seeing an older cached version of the runtime pod simply ignore it until after the pod worker completes always set the grace period for syncTerminatingPod so we dont have to recalculate 

Creating a new pod worker either means this is a new pod or that the kubelet just restarted In either case the kubelet is willing to believe the status of the pod for the first pod worker sync See corresponding comment in syncPod If there is an undelivered work update for this pod we need to remove it since perpod goroutine wont be able to put it to the already closed 
pod but its possible for an apiserver to extremely rarely do something similar  flag the sync status to indicate that after the pod terminates it should be reset to not running to allow a subsequent addupdate to start the pod worker again once a pod is terminated by UID it cannot reenter the pod worker until the UID is purged by housekeeping check for a transition to terminating once a pod is terminating all updates are kills and the grace period can only decrease 

Creating a new pod worker either means this is a new pod or that the kubelet just restarted In either case the kubelet is willing to believe the status of the pod for the first pod worker sync See corresponding comment in syncPod If there is an undelivered work update for this pod we need to remove it since perpod goroutine wont be able to put it to the already closed 
if an update is received that implies the pod should be running but we are already terminating a pod by that UID assume that two pods with the same UID were created in close temporal proximity usually static pod but its possible for an apiserver to extremely rarely do something similar  flag the sync status to indicate that after the pod terminates it should be reset to not running to allow a subsequent addupdate to start the pod worker again 

Creating a new pod worker either means this is a new pod or that the kubelet just restarted In either case the kubelet is willing to believe the status of the pod for the first pod worker sync See corresponding comment in syncPod If there is an undelivered work update for this pod we need to remove it since perpod goroutine wont be able to put it to the already closed 
If this succeeds then record in the podWorker that it is terminated if an update is received that implies the pod should be running but we are already terminating a pod by that UID assume that two pods with the same UID were created in close temporal proximity usually static pod but its possible for an apiserver to extremely rarely do something similar  flag the sync status to indicate that after the pod terminates it should be reset to not running to allow a subsequent addupdate 

the channel is empty so buffer of size 1 is enough Creating a new pod worker either means this is a new pod or that the kubelet just restarted In either case the kubelet is willing to believe the status of the pod for the first pod worker sync See corresponding 
podSyncStatus tracks perpod transitions through the three phases of pod worker sync setup terminating terminated ctx is the context that is associated with the current pod sync cancelFn if set is expected to cancel the current syncPod operation working is true if a pod worker is currently in a sync method fullname of the pod 

the channel is consumed However it is guaranteed that in such case the channel is empty so buffer of size 1 is enough Creating a new pod worker either means this is a new pod or that the kubelet just restarted In either case the kubelet is willing to believe the status of the pod for the first pod worker sync See corresponding 
ctx is the context that is associated with the current pod sync cancelFn if set is expected to cancel the current syncPod operation working is true if a pod worker is currently in a sync method fullname of the pod 

the channel is consumed However it is guaranteed that in such case the channel is empty so buffer of size 1 is enough Creating a new pod worker either means this is a new pod or that the kubelet just restarted In either case the kubelet is willing to believe the status of the pod for the first pod worker sync See corresponding 
podSyncStatus tracks perpod transitions through the three phases of pod worker sync setup terminating terminated ctx is the context that is associated with the current pod sync cancelFn if set is expected to cancel the current syncPod operation working is true if a pod worker is currently in a sync method 

the channel is consumed However it is guaranteed that in such case the channel is empty so buffer of size 1 is enough Creating a new pod worker either means this is a new pod or that the kubelet just restarted In either case the kubelet is willing to believe the status of the pod for the first pod worker sync See corresponding 
Evict is true if this is a pod triggered eviction  once a pod is evicted some resources are more aggressively reaped than during normal pod operation stopped containers PodStatusFunc is invoked if set and overrides the status of the pod at the time the pod is killed The provided status is populated from the latest state PodTerminationGracePeriodSecondsOverride is optional override to use if a pod is being killed as part of kill operation UpdatePodOptions is an options struct to pass to a UpdatePod operation 

We need to have a buffer here because checkForUpdates method that puts an update into channel is called from the same goroutine where the channel is consumed However it is guaranteed that in such case the channel is empty so buffer of size 1 is enough Creating a new pod worker either means this is a new pod or that the kubelet just restarted In either case the kubelet is willing to believe 
startedTerminating is true once the pod worker has observed the request to stop a pod exited syncPod and observed a podWork with WorkType TerminatingPodWork Once this is set it is safe for other components of the kubelet to assume that no other containers may be started 

We need to have a buffer here because checkForUpdates method that puts an update into channel is called from the same goroutine where the channel is consumed However it is guaranteed that in such case the channel is empty so buffer of size 1 is enough Creating a new pod worker either means this is a new pod or that the kubelet just restarted In either case the kubelet is willing to believe 
RunningPod is a runtime pod that is no longer present in config Required if Pod is nil ignored if Pod is set KillPodOptions is used to override the default termination behavior of the pod or to update the pod status after an operation is completed Since a pod can be killed for multiple reasons PodStatusFunc is invoked in order and later kills have an opportunity to override the status ie a preemption 

Apply the new setting to the specified pod updateComplete is called when the update is completed We need to have a buffer here because checkForUpdates method that puts an update into channel is called from the same goroutine where the channel is consumed However it is guaranteed that in such case the channel is empty so buffer of size 1 is enough Creating a new pod worker either means this is a new pod or that the 
terminating startedTerminating is true once the pod worker has observed the request to stop a pod exited syncPod and observed a podWork with WorkType TerminatingPodWork Once this is set it is safe for other components 

Apply the new setting to the specified pod updateComplete is called when the update is completed We need to have a buffer here because checkForUpdates method that puts an update into channel is called from the same goroutine where the channel is consumed However it is guaranteed that in such case the channel is empty so buffer of size 1 is enough Creating a new pod worker either means this is a new pod or that the 
ctx is the context that is associated with the current pod sync cancelFn if set is expected to cancel the current syncPod operation working is true if a pod worker is currently in a sync method fullname of the pod 

Apply the new setting to the specified pod updateComplete is called when the update is completed We need to have a buffer here because checkForUpdates method that puts an update into channel is called from the same goroutine where the channel is consumed However it is guaranteed that in such case the channel is empty so buffer of size 1 is enough Creating a new pod worker either means this is a new pod or that the 
podSyncStatus tracks perpod transitions through the three phases of pod worker sync setup terminating terminated ctx is the context that is associated with the current pod sync cancelFn if set is expected to cancel the current syncPod operation working is true if a pod worker is currently in a sync method 

Apply the new setting to the specified pod updateComplete is called when the update is completed We need to have a buffer here because checkForUpdates method that puts an update into channel is called from the same goroutine where the channel is consumed However it is guaranteed that in such case the channel is empty so buffer of size 1 is enough Creating a new pod worker either means this is a new pod or that the 
Evict is true if this is a pod triggered eviction  once a pod is evicted some resources are more aggressively reaped than during normal pod operation stopped containers PodStatusFunc is invoked if set and overrides the status of the pod at the time the pod is killed The provided status is populated from the latest state PodTerminationGracePeriodSecondsOverride is optional override to use if a pod is being killed as part of kill operation UpdatePodOptions is an options struct to pass to a UpdatePod operation 

Apply the new setting to the specified pod updateComplete is called when the update is completed We need to have a buffer here because checkForUpdates method that puts an update into channel is called from the same goroutine where the channel is consumed However it is guaranteed that in such case the channel is empty so buffer of size 1 is enough Creating a new pod worker either means this is a new pod or that the 
See the License for the specific language governing permissions and limitations under the License OnCompleteFunc is a function that is invoked when an operation completes If err is nonnil the operation did not complete successfully PodStatusFunc is a function that is invoked to override the pod status when a pod is killed KillPodOptions are options when performing a pod update whose update type is kill 

Apply the new setting to the specified pod updateComplete is called when the update is completed We need to have a buffer here because checkForUpdates method that puts an update into channel is called from the same goroutine where the channel is consumed However it is guaranteed that in such case the channel is empty so buffer of size 1 is enough 
ctx is the context that is associated with the current pod sync cancelFn if set is expected to cancel the current syncPod operation working is true if a pod worker is currently in a sync method fullname of the pod syncedAt is the time at which the pod worker first observed this pod terminatingAt is set once the pod is requested to be killed  note that 

Apply the new setting to the specified pod updateComplete is called when the update is completed We need to have a buffer here because checkForUpdates method that puts an update into channel is called from the same goroutine where the channel is consumed However it is guaranteed that in such case 
The provided status is populated from the latest state PodTerminationGracePeriodSecondsOverride is optional override to use if a pod is being killed as part of kill operation UpdatePodOptions is an options struct to pass to a UpdatePod operation The type of update create update sync kill StartTime is an optional timestamp for when this update was created If set when this update is fully realized by the pod worker it will be recorded in 

the previous sync Apply the new setting to the specified pod updateComplete is called when the update is completed We need to have a buffer here because checkForUpdates method that puts an update into channel is called from the same goroutine where the channel is consumed However it is guaranteed that in such case 
terminating startedTerminating is true once the pod worker has observed the request to stop a pod exited syncPod and observed a podWork with WorkType TerminatingPodWork Once this is set it is safe for other components of the kubelet to assume that no other containers may be started deleted is true if the pod has been marked for deletion on the apiserver 

after the cache is at least newer than the finished time of the previous sync Apply the new setting to the specified pod updateComplete is called when the update is completed We need to have a buffer here because checkForUpdates method that puts an update into channel is called from the same goroutine where the channel is consumed However it is guaranteed that in such case 
ctx is the context that is associated with the current pod sync cancelFn if set is expected to cancel the current syncPod operation working is true if a pod worker is currently in a sync method fullname of the pod syncedAt is the time at which the pod worker first observed this pod terminatingAt is set once the pod is requested to be killed  note that 

after the cache is at least newer than the finished time of the previous sync Apply the new setting to the specified pod updateComplete is called when the update is completed We need to have a buffer here because checkForUpdates method that puts an update into channel is called from the same goroutine where the channel is consumed However it is guaranteed that in such case 
more aggressively reaped than during normal pod operation stopped containers PodStatusFunc is invoked if set and overrides the status of the pod at the time the pod is killed The provided status is populated from the latest state PodTerminationGracePeriodSecondsOverride is optional override to use if a pod is being killed as part of kill operation UpdatePodOptions is an options struct to pass to a UpdatePod operation The type of update create update sync kill 

after the cache is at least newer than the finished time of the previous sync Apply the new setting to the specified pod updateComplete is called when the update is completed We need to have a buffer here because checkForUpdates method that puts an update into channel is called from the same goroutine where the channel is consumed However it is guaranteed that in such case 
could take an arbitrary amount of time to be closed but is never left open once CouldHaveRunningContainers returns false Evict is true if this is a pod triggered eviction  once a pod is evicted some resources are more aggressively reaped than during normal pod operation stopped containers PodStatusFunc is invoked if set and overrides the status of the pod at the time the pod is killed The provided status is populated from the latest state 

Time This ensures the worker doesnt start syncing until after the cache is at least newer than the finished time of the previous sync Apply the new setting to the specified pod updateComplete is called when the update is completed We need to have a buffer here because checkForUpdates method that puts an update into channel is called from the same goroutine where 
in channel communication The function is invoked once each time a new worker goroutine starts The EventRecorder to use backOffPeriod is the duration to back off when there is a sync error resyncInterval is the duration to wait until the next sync podCache stores kubecontainerPodStatus for all pods 

This is a blocking call that would return only if the cache has an entry for the pod that is newer than minRuntimeCache Time This ensures the worker doesnt start syncing until after the cache is at least newer than the finished time of the previous sync Apply the new setting to the specified pod updateComplete is called when the update is completed 
could take an arbitrary amount of time to be closed but is never left open once CouldHaveRunningContainers returns false Evict is true if this is a pod triggered eviction  once a pod is evicted some resources are more aggressively reaped than during normal pod operation stopped containers PodStatusFunc is invoked if set and overrides the status of the pod at the time the pod is killed The provided status is populated from the latest state 

backOffPeriod is the duration to back off when there is a sync error resyncInterval is the duration to wait until the next sync podCache stores kubecontainerPodStatus for all pods The pod state to reflect 
provided pod name is currently terminating and has yet to complete It is intended to be used only during orphan mirror pod cleanup to prevent us from deleting a terminating static pod from the apiserver before the pod is shut down the function to invoke to perform a sync reconcile the kubelet state to the desired shape of the pod 

This function is run to sync the desired stated of pod NOTE This function has to be threadsafe  it can be called for different pods at the same time The EventRecorder to use backOffPeriod is the duration to back off when there is a sync error resyncInterval is the duration to wait until the next sync 
provided pod name is currently terminating and has yet to complete It is intended to be used only during orphan mirror pod cleanup to prevent us from deleting a terminating static pod from the apiserver before the pod is shut down the function to invoke to perform a sync reconcile the kubelet state to the desired shape of the pod 

This function is run to sync the desired stated of pod 
This function is run to sync the desired state of pod 

undelivered if it comes in while the worker is working This function is run to sync the desired stated of pod NOTE This function has to be threadsafe  it can be called for different pods at the same time The EventRecorder to use backOffPeriod is the duration to back off when there is a sync error 
check to see if the pod is not running and the pod is terminal If this succeeds then record in the podWorker that it is terminated if an update is received that implies the pod should be running but we are already terminating a pod by that UID assume that two pods with the same UID were created in close temporal proximity usually static pod but its possible for an apiserver to extremely rarely do something similar  flag the sync status to indicate that after the pod terminates it should be reset to not running to allow a subsequent addupdate 

undelivered if it comes in while the worker is working This function is run to sync the desired stated of pod NOTE This function has to be threadsafe  it can be called for different pods at the same time The EventRecorder to use backOffPeriod is the duration to back off when there is a sync error 
startedTerminating is true once the pod worker has observed the request to stop a pod exited syncPod and observed a podWork with WorkType TerminatingPodWork Once this is set it is safe for other components of the kubelet to assume that no other containers may be started 

undelivered if it comes in while the worker is working This function is run to sync the desired stated of pod NOTE This function has to be threadsafe  it can be called for different pods at the same time The EventRecorder to use backOffPeriod is the duration to back off when there is a sync error 
cancelFn if set is expected to cancel the current syncPod operation working is true if a pod worker is currently in a sync method fullname of the pod syncedAt is the time at which the pod worker first observed this pod terminatingAt is set once the pod is requested to be killed  note that this can be set before the pod worker starts terminating the pod see 

undelivered if it comes in while the worker is working This function is run to sync the desired stated of pod NOTE This function has to be threadsafe  it can be called for different pods at the same time The EventRecorder to use backOffPeriod is the duration to back off when there is a sync error 
IsPodForMirrorPodTerminatingByFullName returns true if a static pod with the provided pod name is currently terminating and has yet to complete It is intended to be used only during orphan mirror pod cleanup to prevent us from deleting a terminating static pod from the apiserver before the pod is shut 

undelivered if it comes in while the worker is working This function is run to sync the desired stated of pod NOTE This function has to be threadsafe  it can be called for different pods at the same time 
KillPodOptions is not valid for sync actions outside of the terminating phase the desired work we want to be performing start the pod worker goroutine if it doesnt exist We need to have a buffer here because checkForUpdates method that puts an update into channel is called from the same goroutine where the channel is consumed However it is guaranteed that in such case 

Tracks the last undelivered work item for this pod  a work item is undelivered if it comes in while the worker is working This function is run to sync the desired stated of pod NOTE This function has to be threadsafe  it can be called for different pods at the same time 
this can be set before the pod worker starts terminating the pod see terminating startedTerminating is true once the pod worker has observed the request to stop a pod exited syncPod and observed a podWork with WorkType TerminatingPodWork Once this is set it is safe for other components of the kubelet to assume that no other containers may be started 

Tracks the last undelivered work item for this pod  a work item is undelivered if it comes in while the worker is working This function is run to sync the desired stated of pod NOTE This function has to be threadsafe  it can be called for different pods at the same time 
cancelFn if set is expected to cancel the current syncPod operation working is true if a pod worker is currently in a sync method fullname of the pod syncedAt is the time at which the pod worker first observed this pod terminatingAt is set once the pod is requested to be killed  note that this can be set before the pod worker starts terminating the pod see 

Tracks the last undelivered work item for this pod  a work item is undelivered if it comes in while the worker is working This function is run to sync the desired stated of pod 
worker sync setup terminating terminated ctx is the context that is associated with the current pod sync cancelFn if set is expected to cancel the current syncPod operation working is true if a pod worker is currently in a sync method fullname of the pod syncedAt is the time at which the pod worker first observed this pod 

update of this pod is being processed are ignored Tracks the last undelivered work item for this pod  a work item is undelivered if it comes in while the worker is working This function is run to sync the desired stated of pod NOTE This function has to be threadsafe  it can be called for different pods at the same time 
startedTerminating is true once the pod worker has observed the request to stop a pod exited syncPod and observed a podWork with WorkType TerminatingPodWork Once this is set it is safe for other components of the kubelet to assume that no other containers may be started deleted is true if the pod has been marked for deletion on the apiserver 

update of this pod is being processed are ignored Tracks the last undelivered work item for this pod  a work item is undelivered if it comes in while the worker is working This function is run to sync the desired stated of pod 
ctx is the context that is associated with the current pod sync cancelFn if set is expected to cancel the current syncPod operation working is true if a pod worker is currently in a sync method fullname of the pod syncedAt is the time at which the pod worker first observed this pod terminatingAt is set once the pod is requested to be killed  note that 

update of this pod is being processed are ignored Tracks the last undelivered work item for this pod  a work item is undelivered if it comes in while the worker is working This function is run to sync the desired stated of pod 
podSyncStatus tracks perpod transitions through the three phases of pod worker sync setup terminating terminated ctx is the context that is associated with the current pod sync cancelFn if set is expected to cancel the current syncPod operation working is true if a pod worker is currently in a sync method 

Currently all update request for a given pod coming when another update of this pod is being processed are ignored Tracks the last undelivered work item for this pod  a work item is undelivered if it comes in while the worker is working This function is run to sync the desired stated of pod NOTE This function has to be threadsafe  it can be called for 
stop a pod exited syncPod and observed a podWork with WorkType TerminatingPodWork Once this is set it is safe for other components of the kubelet to assume that no other containers may be started deleted is true if the pod has been marked for deletion on the apiserver 

Currently all update request for a given pod coming when another update of this pod is being processed are ignored Tracks the last undelivered work item for this pod  a work item is undelivered if it comes in while the worker is working This function is run to sync the desired stated of pod NOTE This function has to be threadsafe  it can be called for 
startedTerminating is true once the pod worker has observed the request to stop a pod exited syncPod and observed a podWork with WorkType TerminatingPodWork Once this is set it is safe for other components of the kubelet to assume that no other containers may be started 

Currently all update request for a given pod coming when another update of this pod is being processed are ignored Tracks the last undelivered work item for this pod  a work item is undelivered if it comes in while the worker is working This function is run to sync the desired stated of pod NOTE This function has to be threadsafe  it can be called for 
terminatingAt is set once the pod is requested to be killed  note that this can be set before the pod worker starts terminating the pod see terminating startedTerminating is true once the pod worker has observed the request to stop a pod exited syncPod and observed a podWork with WorkType TerminatingPodWork Once this is set it is safe for other components 

Track the current state of perpod goroutines Currently all update request for a given pod coming when another update of this pod is being processed are ignored Tracks the last undelivered work item for this pod  a work item is undelivered if it comes in while the worker is working This function is run to sync the desired stated of pod 
the channel is consumed However it is guaranteed that in such case the channel is empty so buffer of size 1 is enough ensure that static pods start in the order they are received by UpdatePod allow testing of delays in the pod update channel Creating a new pod worker either means this is a new pod or that the kubelet just restarted In either case the kubelet is willing to believe 

jitter factor for backOffPeriod 
jitter factor for resyncInterval 

reason cache should not be set for SyncResult with other actions 
reason cache should not be set for SyncResult with StartContainer action but without error 

reason cache should not be set for SyncResult with StartContainer action but without error reason cache should not be set for SyncResult with other actions 
reason cache should be set for SyncResult with StartContainer action and error 

reason cache should not be set for SyncResult with StartContainer action but without error 
reason cache should not be set for SyncResult with other actions 

reason cache should be set for SyncResult with StartContainer action and error 
reason cache should not be set for SyncResult with StartContainer action but without error reason cache should not be set for SyncResult with other actions 

serviceAccountTokenSecretWithoutNamespaceData returns an existing ServiceAccountToken secret that lacks namespace data 
serviceAccountTokenSecretWithoutTokenData returns an existing ServiceAccountToken secret that lacks token data 

serviceAccountTokenSecretWithoutTokenData returns an existing ServiceAccountToken secret that lacks token data 
serviceAccountTokenSecretWithoutNamespaceData returns an existing ServiceAccountToken secret that lacks namespace data 

Named defaulttokenfplln since that is the first generated name after randSeed1 
Named defaulttokenxn8fg since that is the first generated name after randSeed1 

createdTokenSecret returns the ServiceAccountToken secret posted when creating a new token secret 
namedTokenSecret returns the ServiceAccountToken secret posted when creating a new token secret with the given name 

Delete endpoints2 
Delete endpoints1 

Delete endpoints2 
Delete service2 

Delete endpoints2 
Delete service1 

Delete endpoints1 
Delete endpoints2 

Delete endpoints1 
Delete service2 

Delete endpoints1 
Delete service1 

Modify endpoints1 
Modify service1 

Delete service2 
Delete endpoints2 

Delete service2 
Delete endpoints1 

Delete service2 
Delete service1 

Delete service1 
Delete endpoints2 

Delete service1 
Delete endpoints1 

Delete service1 
Delete service2 

Modify service1 
Modify endpoints1 

Echo handler that returns the contents of request headers in the body 
Echo handler that returns the contents of Host in the body 

Echo handler that returns the contents of request headers in the body 
Handler that returns the keys of request headers in the body 

Echo handler that returns the contents of request headers in the body 
Handler that returns the number of request headers in the body 

Sort the container statuses by creation time 
SortContainerStatusesByCreationTime sorts the container statuses by creation time 

Parse the pod full name 
ParsePodFullName parsed the pod full name 

Build the pod full name from pod name and namespace 
BuildPodFullName builds the pod full name from pod name and namespace 

ToAPIPod converts Pod to apiPod Note that if a field in apiPod has no 
ToAPIPod converts Pod to v1Pod Note that if a field in v1Pod has no 

When there are multiple containers with the same name the first match will be returned 
When there are multiple containers statuses with the same name the first match will be returned 

FindContainerByName returns a container in the pod with the given name When there are multiple containers with the same name the first match will 
When there are multiple containers statuses with the same name the first match will be returned GetRunningContainerStatuses returns container status of all the running containers in a pod 

FindContainerByName returns a container in the pod with the given name When there are multiple containers with the same name the first match will 
FindContainerStatusByName returns container status in the pod status with the given name When there are multiple containers statuses with the same name the first match will be returned 

FindContainerByName returns a container in the pod with the given name 
FindSandboxByID returns a sandbox in the pod with the given ContainerID 

FindContainerByName returns a container in the pod with the given name 
FindContainerByID returns a container in the pod with the given ContainerID 

FindPod combines FindPodByID and FindPodByFullName it finds and returns a pod in the pod list either by the full name or the pod ID It will return an empty pod if not found 
FindPodByFullName finds and returns a pod in the pod list by the full name It will return an empty pod if not found 

FindPod combines FindPodByID and FindPodByFullName it finds and returns a pod in the pod list either by the full name or the pod ID It will return an empty pod 
String formats the runtime condition into human readable string Pods represents the list of pods FindPodByID finds and returns a pod in the pod list by UID It will return an empty pod if not found FindPodByFullName finds and returns a pod in the pod list by the full name It will return an empty pod if not found 

It will return an empty pod if not found FindPod combines FindPodByID and FindPodByFullName it finds and returns a pod in the pod list either by the full name or the pod ID It will return an empty pod 
FindPodByID finds and returns a pod in the pod list by UID It will return an empty pod if not found 

FindPodByFullName finds and returns a pod in the pod list by the full name It will return an empty pod if not found 
FindPod combines FindPodByID and FindPodByFullName it finds and returns a pod in the pod list either by the full name or the pod ID It will return an empty pod if not found 

FindPodByFullName finds and returns a pod in the pod list by the full name It will return an empty pod if not found 
FindPodByID finds and returns a pod in the pod list by UID It will return an empty pod if not found 

FindPodByID finds and returns a pod in the pod list by UID It will return an empty pod if not found 
It will return an empty pod if not found FindPod combines FindPodByID and FindPodByFullName it finds and returns a pod in the pod list either by the full name or the pod ID It will return an empty pod 

FindPodByID finds and returns a pod in the pod list by UID It will return an empty pod if not found 
FindPodByFullName finds and returns a pod in the pod list by the full name It will return an empty pod if not found 

pods SELinux label applied to it or not FindPodByID finds and returns a pod in the pod list by UID It will return an empty pod if not found FindPodByFullName finds and returns a pod in the pod list by the full name 
FindPod combines FindPodByID and FindPodByFullName it finds and returns a pod in the pod list either by the full name or the pod ID It will return an empty pod 

pods SELinux label applied to it or not FindPodByID finds and returns a pod in the pod list by UID It will return an empty pod 
It will return an empty pod if not found FindPod combines FindPodByID and FindPodByFullName it finds and returns a pod in the pod list either by the full name or the pod ID It will return an empty pod 

The mounts for the containers 
The annotations for the container 

Name of the port mapping 
Protocol of the port mapping 

ID of the image Other names by which this image is known The size of the image in bytes Name of the volume mount Path of the mount within the container 
Exit code of the container Name of the image this also includes the tag of the image the expected form is NAMETAG 

Basic information about a container image ID of the image Other names by which this image is known The size of the image in bytes Name of the volume mount Path of the mount within the container 
The image name of the container this also includes the tag of the image the expected form is NAMETAG The id of the image used by the container Hash of the container used for comparison Optional for containers 

Basic information about a container image ID of the image Other names by which this image is known The size of the image in bytes Name of the volume mount Path of the mount within the container 
state of the container The ID of the container used by the container runtime to identify a container The name of the container which should be the same as specified by v1Container The image name of the container this also includes the tag of the image 

Get container status of all the running containers in a pod 
GetRunningContainerStatuses returns container status of all the running containers in a pod 

When there are multiple containers statuses with the same name the first match will be returned 
When there are multiple containers with the same name the first match will be returned 

FindContainerStatusByName returns container status in the pod status with the given name When there are multiple containers statuses with the same name the first match will be returned 
FindContainerByName returns a container in the pod with the given name When there are multiple containers with the same name the first match will 

Name of the image ID of the image Hash of the container used for comparison 
Exit code of the container Name of the image this also includes the tag of the image 

Name of the image ID of the image Hash of the container used for comparison 
ID of the container Name of the container Status of the container 

Exit code of the container Name of the image ID of the image 
ID of the container Name of the container 

Finish time of the container Exit code of the container Name of the image 
Name of the container Status of the container 

Finish time of the container 
Start time of the container 

Finish time of the container 
Creation time of the container 

Start time of the container 
Finish time of the container 

Start time of the container 
Creation time of the container 

Creation time of the container 
Finish time of the container 

Creation time of the container 
Start time of the container 

ID of the container 
ID of the pod 

ContainerStatus represents the status of a container ID of the container Name of the container Status of the container Creation time of the container Start time of the container 
RuntimeStatus contains the status of the runtime 

ContainerStatus represents the status of a container ID of the container Name of the container Status of the container Creation time of the container 
Exit code of the container Name of the image this also includes the tag of the image the expected form is NAMETAG ID of the image Hash of the container used for comparison 

Status of containers in the pod ContainerStatus represents the status of a container ID of the container Name of the container 
Status of the container 

Namspace of the pod 
Namespace of the pod 

Name of the pod Namspace of the pod 
Status of the pod sandbox 

Name of the pod 
Namespace of the pod 

ID of the pod 
ID of the container 

apiPodStatus can be derived from examining PodStatus and apiPod 
v1PodStatus can be derived from examining PodStatus and v1Pod 

PodStatus represents the status of the pod and its containers 
GetPodStatus retrieves the status of the pod including the 

The timestamp of the creation time of the container 
Finish time of the container Exit code of the container Name of the image this also includes the tag of the image 

The timestamp of the creation time of the container 
Creation time of the container 

not managed by kubelet The timestamp of the creation time of the container TODOyifan Consider to move it to apiContainerStatus State is the state of the container PodStatus represents the status of the pod and its containers apiPodStatus can be derived from examining PodStatus and apiPod 
and store the resulting archive to the checkpoint directory StreamingRuntime is the interface implemented by runtimes that handle the serving of the 

The identification of the container this is comsumable by the underlying container runtime Note that the container runtime interface still takes the whole struct as input 
Runtime interface defines the interfaces that should be implemented by a container runtime Thread safety is required from implementations of this interface Type returns the type of the container runtime Version returns the version information of the container runtime 

The identification of the container this is comsumable by the underlying container runtime Note that the container 
The ID of the container used by the container runtime to identify a container The name of the container which should be the same as specified by v1Container The image name of the container this also includes the tag of the image the expected form is NAMETAG 

The identification of the container this is comsumable by the underlying container runtime Note that the container 
StreamingRuntime is the interface implemented by runtimes that handle the serving of the 

The type of the container runtime eg docker rkt The identification of the container this is comsumable by the underlying container runtime Note that the container runtime interface still takes the whole struct as input 
a container The name of the container which should be the same as specified by v1Container The image name of the container this also includes the tag of the image the expected form is NAMETAG 

The type of the container runtime eg docker rkt The identification of the container this is comsumable by the underlying container runtime Note that the container 
The image name of the container this also includes the tag of the image the expected form is NAMETAG The id of the image used by the container Hash of the container used for comparison Optional for containers not managed by kubelet 

The type of the container runtime eg docker rkt The identification of the container this is comsumable by the underlying container runtime Note that the container 
The name of the container which should be the same as specified by v1Container The image name of the container this also includes the tag of the image the expected form is NAMETAG The id of the image used by the container 

The type of the container runtime eg docker rkt The identification of the container this is comsumable by the underlying container runtime Note that the container 
The ID of the container used by the container runtime to identify a container The name of the container which should be the same as specified by v1Container The image name of the container this also includes the tag of the image 

The type of the container runtime eg docker rkt The identification of the container this is comsumable by the underlying container runtime Note that the container 
by a container runtime Thread safety is required from implementations of this interface Type returns the type of the container runtime Version returns the version information of the container runtime 

RunningPod is the pod defined defined in pkgkubeletcontainerruntimePod 
RunningPod is the pod defined in pkgkubeletcontainerruntimePod 

APIPod is the apiPod 
APIPod is the v1Pod 

It will check the presence of the image and report the image pulling image pulled events correspondingly Pod is a group of containers The ID of the pod which can be used to retrieve a particular pod from the pod list returned by GetPods The name and namespace of the pod which is readable by human 
The ID of the container used by the container runtime to identify a container The name of the container which should be the same as specified by v1Container The image name of the container this also includes the tag of the image the expected form is NAMETAG 

Forward the specified port from the specified pod to the stream ImagePuller wraps RuntimePullImage to pull a container image It will check the presence of the image and report the image pulling 
This method just proxies a new runtimeConfig with the updated CIDR value down to the runtime shim CheckpointContainer tells the runtime to checkpoint a container and store the resulting archive to the checkpoint directory StreamingRuntime is the interface implemented by runtimes that handle the serving of the 

Attaches the processes stdin stdout and stderr Optionally uses a tty Forward the specified port from the specified pod to the stream ImagePuller wraps RuntimePullImage to pull a container image It will check the presence of the image and report the image pulling 
CIDR value down to the runtime shim CheckpointContainer tells the runtime to checkpoint a container and store the resulting archive to the checkpoint directory StreamingRuntime is the interface implemented by runtimes that handle the serving of the 

Runs the command in the container of the specified pod using nsenter Attaches the processes stdin stdout and stderr Optionally uses a tty Forward the specified port from the specified pod to the stream ImagePuller wraps RuntimePullImage to pull a container image It will check the presence of the image and report the image pulling 
CheckpointContainer tells the runtime to checkpoint a container and store the resulting archive to the checkpoint directory StreamingRuntime is the interface implemented by runtimes that handle the serving of the 

Runs the command in the container of the specified pod using nsenter Attaches the processes stdin stdout and stderr Optionally uses a tty Forward the specified port from the specified pod to the stream ImagePuller wraps RuntimePullImage to pull a container image 
RemoveImage removes the specified image ImageStats returns Image statistics Attacher interface allows to attach a container CommandRunner interface allows to run command in a container RunInContainer synchronously executes the command in the container and returns the output 

TODOvmarmol Merge RunInContainer and ExecInContainer Runs the command in the container of the specified pod using nsinit 
GetPodStatus retrieves the status of the pod including the information of all containers in the pod that are visible in Runtime TODOvmarmol Unify pod and containerID args 

Removes the specified image 
RemoveImage removes the specified image 

Gets all images currently on the machine 
ListImages gets all images currently on the machine 

information of all containers in the pod that are visble in Runtime 
information of all containers in the pod that are visible in Runtime 

GetPodStatus retrieves the status of the pod including the 
RuntimeStatus contains the status of the runtime 

GetPodStatus retrieves the status of the pod including the 
PodStatus represents the status of the pod and its containers 

Syncs the running pod into the desired pod 
SyncPod syncs the running pod into the desired pod 

APIVersion returns the API version information of the container 
APIVersion returns the cached API version information of the container 

APIVersion returns the API version information of the container 
Version returns the version information of the container runtime 

Version returns the version information of the container runtime 
APIVersion returns the cached API version information of the container 

Thread safety is required from implementations of this interface Type returns the type of the container runtime Version returns the version information of the container runtime 
String formats the runtime status into human readable string RuntimeCondition contains condition information for the runtime Type of runtime condition Status of the condition one of truefalse 

by a container runtime Thread safety is required from implementations of this interface Type returns the type of the container runtime Version returns the version information of the container runtime 
RuntimeStatus contains the status of the runtime Conditions is an array of current observed runtime conditions GetRuntimeCondition gets a specified runtime condition from the runtime status String formats the runtime status into human readable string RuntimeCondition contains condition information for the runtime Type of runtime condition 

by a container runtime Thread safety is required from implementations of this interface Type returns the type of the container runtime Version returns the version information of the container runtime 
The type of the container runtime eg docker The identification of the container this is comsumable by the underlying container runtime Note that the container 

Runtime interface defines the interfaces that should be implemented by a container runtime Thread safety is required from implementations of this interface Type returns the type of the container runtime Version returns the version information of the container runtime 
The identification of the container this is comsumable by the underlying container runtime Note that the container runtime interface still takes the whole struct as input 

without a container are moved to it The reason of leaving kernel threads at root cgroup is that we dont want to tie the execution of these threads with tobe defined system quota and create priority inversions Move nonkernel PIDs to the system container Only keep errors on latest attempt 
Err on the side of caution Avoid moving the docker daemon unless we are able to identify its context Process is running inside a container Dont touch that Also apply oomscoreadj to processes 

without a container are moved to it The reason of leaving kernel threads at root cgroup is that we dont want to tie the execution of these threads with tobe defined system quota and create priority inversions Move nonkernel PIDs to the system container 
Err on the side of caution Avoid moving the docker daemon unless we are able to identify its context Process is running inside a container Dont touch that Also apply oomscoreadj to processes getContainer returns the cgroup associated with the specified pid 

Docker daemon is running inside a container Dont touch that 
Process is running inside a container Dont touch that 

Err on the side of caution Avoid moving the docker daemon unless we are able to identify its context Docker daemon is running inside a container Dont touch that Also apply oomscoreadj to processes Gets the CPU container the specified pid is in 
in addition you would not get memory or cpu accounting for the runtime unless accounting was enabled on its unit or globally Ensures the system container is created and all nonkernel threads and process 1 without a container are moved to it The reason of leaving kernel threads at root cgroup is that we dont want to tie the 

Err on the side of caution Avoid moving the docker daemon unless we are able to identify its context Docker daemon is running inside a container Dont touch that Also apply oomscoreadj to processes 
without a container are moved to it The reason of leaving kernel threads at root cgroup is that we dont want to tie the execution of these threads with tobe defined system quota and create priority inversions Move nonkernel PIDs to the system container Only keep errors on latest attempt 

Move if the pid is not already in the desired container Err on the side of caution Avoid moving the docker daemon unless we are able to identify its context Docker daemon is running inside a container Dont touch that Also apply oomscoreadj to processes 
in addition you would not get memory or cpu accounting for the runtime unless accounting was enabled on its unit or globally Ensures the system container is created and all nonkernel threads and process 1 without a container are moved to it The reason of leaving kernel threads at root cgroup is that we dont want to tie the 

Takes the absolute name of the specified containers 
Absolute name of the container 

Absolute name of the container 
Takes the absolute name of the specified containers 

is set this function will also confirm that cbr0 is configured correctly Flannel is the authoritative source of pod CIDR if its running This is a short term compromise till we get flannel working in reservation mode Update the node resourceVersion so the status update doesnt fail 
which affects node ready status This function must be called before Kubelet is initialized so that the Node ReadyState is accurate with the storage state sockDir If the experimentalMounterPathFlag is set we do not want to check node capabilities since the mount path is not the default Replace the nameserver in containerizedmounters rootfsetcresolvconf with kubeletClusterDNS 

is set this function will also confirm that cbr0 is configured correctly Flannel is the authoritative source of pod CIDR if its running This is a short term compromise till we get flannel working in reservation mode Update the node resourceVersion so the status update doesnt fail 
NewInitializedVolumePluginMgr initializes some storageErrors on the Kubelet runtimeState in csi_plugingo init which affects node ready status This function must be called before Kubelet is initialized so that the Node ReadyState is accurate with the storage state sockDir If the experimentalMounterPathFlag is set we do not want to check node capabilities since the mount path is not the default 

tryUpdateNodeStatus tries to update node status to master If ReconcileCBR0 is set this function will also confirm that cbr0 is configured correctly Flannel is the authoritative source of pod CIDR if its running This is a short term compromise till we get flannel working in reservation mode Update the node resourceVersion so the status update doesnt fail 
which affects node ready status This function must be called before Kubelet is initialized so that the Node ReadyState is accurate with the storage state sockDir If the experimentalMounterPathFlag is set we do not want to check node capabilities since the mount path is not the default 

tryUpdateNodeStatus tries to update node status to master If ReconcileCBR0 is set this function will also confirm that cbr0 is configured correctly Flannel is the authoritative source of pod CIDR if its running This is a short term compromise till we get flannel working in reservation mode Update the node resourceVersion so the status update doesnt fail 
NewInitializedVolumePluginMgr initializes some storageErrors on the Kubelet runtimeState in csi_plugingo init which affects node ready status This function must be called before Kubelet is initialized so that the Node ReadyState is accurate with the storage state sockDir If the experimentalMounterPathFlag is set we do not want to 

initial set of node status update handlers can be modified by Options SetNodeStatus returns a functional Option that adds the given node status update handler to the Kubelet tryUpdateNodeStatus tries to update node status to master If ReconcileCBR0 is set this function will also confirm that cbr0 is configured correctly Flannel is the authoritative source of pod CIDR if its running 
a runtime update and a node status update Function returns after one successful node status update Function is executed only during Kubelet start which improves latency to ready node by updating pod CIDR runtime status and node statuses ASAP CheckpointContainer tries to checkpoint a container The parameters are used to look up the specified container If the container specified by the given parameters cannot be found an error is returned If the container is found the container 

initial set of node status update handlers can be modified by Options SetNodeStatus returns a functional Option that adds the given node status update handler to the Kubelet tryUpdateNodeStatus tries to update node status to master If ReconcileCBR0 is set this function will also confirm that cbr0 is configured correctly Flannel is the authoritative source of pod CIDR if its running 
This should only be enabled when the container runtime is performing user remapping AND if the experimental behavior is desired StatsProvider provides the node and the container stats This flag if set instructs the kubelet to keep volumes from terminated pods mounted to the node 

defaultNodeStatusFuncs is a factory that generates the default set of setNodeStatus funcs initial set of node status update handlers can be modified by Options SetNodeStatus returns a functional Option that adds the given node status update handler to the Kubelet tryUpdateNodeStatus tries to update node status to master If ReconcileCBR0 is set this function will also confirm that cbr0 is configured correctly Flannel is the authoritative source of pod CIDR if its running 
reasonCache caches the failure reason of the last creation of all containers which is used for generating ContainerStatus nodeStatusUpdateFrequency specifies how often kubelet computes node status If node lease feature is not enabled it is also the frequency that kubelet posts node status to master 

defaultNodeStatusFuncs is a factory that generates the default set of setNodeStatus funcs initial set of node status update handlers can be modified by Options SetNodeStatus returns a functional Option that adds the given node status update handler to the Kubelet tryUpdateNodeStatus tries to update node status to master If ReconcileCBR0 
a runtime update and a node status update Function returns after one successful node status update Function is executed only during Kubelet start which improves latency to ready node by updating pod CIDR runtime status and node statuses ASAP CheckpointContainer tries to checkpoint a container The parameters are used to look up the specified container If the container specified by the given parameters cannot be found an error is returned If the container is found the container 

refactor the node status condtion code out to a different file defaultNodeStatusFuncs is a factory that generates the default set of setNodeStatus funcs initial set of node status update handlers can be modified by Options SetNodeStatus returns a functional Option that adds the given node status update handler to the Kubelet tryUpdateNodeStatus tries to update node status to master If ReconcileCBR0 
a runtime update and a node status update Function returns after one successful node status update Function is executed only during Kubelet start which improves latency to ready node by updating pod CIDR runtime status and node statuses ASAP CheckpointContainer tries to checkpoint a container The parameters are used to 

TODOmadhusudancs Simplify the logic for setting node conditions and refactor the node status condtion code out to a different file defaultNodeStatusFuncs is a factory that generates the default set of setNodeStatus funcs initial set of node status update handlers can be modified by Options SetNodeStatus returns a functional Option that adds the given node status update handler to the Kubelet tryUpdateNodeStatus tries to update node status to master If ReconcileCBR0 
plugins need to be registeredunregistered based on this node and makes it so This flag sets a maximum number of images to report in the node status Handles RuntimeClass objects for the Kubelet Handles node shutdown events for the Node Manage user namespaces ListPodStats is delegated to StatsProvider which implements statsProvider interface 

TODOmadhusudancs Simplify the logic for setting node conditions and refactor the node status condtion code out to a different file defaultNodeStatusFuncs is a factory that generates the default set of setNodeStatus funcs initial set of node status update handlers can be modified by Options SetNodeStatus returns a functional Option that adds the given node status update handler to the Kubelet 
a runtime update and a node status update Function returns after one successful node status update Function is executed only during Kubelet start which improves latency to ready node by updating pod CIDR runtime status and node statuses ASAP CheckpointContainer tries to checkpoint a container The parameters are used to look up the specified container If the container specified by the given parameters 

any fields that are currently set TODOmadhusudancs Simplify the logic for setting node conditions and refactor the node status condtion code out to a different file defaultNodeStatusFuncs is a factory that generates the default set of setNodeStatus funcs initial set of node status update handlers can be modified by Options SetNodeStatus returns a functional Option that adds the given node status update handler to the Kubelet 
experimental behavior is desired StatsProvider provides the node and the container stats This flag if set instructs the kubelet to keep volumes from terminated pods mounted to the node This can be useful for debugging volume related issues DEPRECATED pluginmanager runs a set of asynchronous loops that figure out which 

setNodeStatus fills in the Status fields of the given Node overwriting any fields that are currently set TODOmadhusudancs Simplify the logic for setting node conditions and refactor the node status condtion code out to a different file 
enableControllerAttachDetach indicates the AttachDetach controller should manage attachmentdetachment of volumes scheduled to this node and disable kubelet from executing any attachdetach operations trigger deleting containers in a pod config iptables util rules The bit of the fwmark space to mark packets for SNAT 

Maintains NodeSpecUnschedulable value from previous run of tryUpdateNodeStatus 
maintains NodeSpecUnschedulable value from previous run of tryUpdateNodeStatus 

copied to the slice So if we append nodeOODCondition to the slice here none of the updates we make to nodeOODCondition below are reflected in the slice Update the heartbeat time irrespective of all the conditions Note The conditions below take care of the case when a new NodeOutOfDisk condition is created and as well as the case when the condition already exists When a new condition is created its status is set to apiConditionUnknown which matches either 
The workflow is to read from one of the channels handle that event and update the timestamp in the sync loop monitor Here is an appropriate place to note that despite the syntactical similarity to the switch statement the case statements in a select are evaluated in a pseudorandom order if there are multiple channels ready to 

If no match is found it uses the IP of the interface with gateway on it TODO Post NotReady if we cannot get MachineInfo from cAdvisor This needs to start cAdvisor locally eg for testcmdsh and in integration test TODOroberthbailey This is required for testcmdsh to pass See if the test should be updated instead TODO This requires a transaction either both node status is updated 
NewInitializedVolumePluginMgr initializes some storageErrors on the Kubelet runtimeState in csi_plugingo init which affects node ready status This function must be called before Kubelet is initialized so that the Node ReadyState is accurate with the storage state sockDir If the experimentalMounterPathFlag is set we do not want to 

If there is no cached status use the status from the apiserver This is useful if kubelet has recently been restarted GetHostname Returns the hostname as the kubelet sees it Returns host IP or nil in case of error GetPods returns all pods bound to the kubelet and their spec and the mirror 
manager as the source of truth for the desired state If a pod does not exist in the pod manager it means that it has been deleted in the apiserver and no action other than cleanup is required Only go through the admission process if the pod is not requested for termination by another part of the kubelet If the pod is already 

TODO this method is returning logs of random container attempts when it should be returning the most recent attempt or all of them TODOvmarmol Refactor to not need the pod status and verification Pod workers periodically write status to statusManager If status is not cached there something is wrong or kubelet just restarted and hasnt caught up yet Just assume the pod is not ready yet 
the apiserver and no action other than cleanup is required Only go through the admission process if the pod is not requested for termination by another part of the kubelet If the pod is already using resources previously admitted the pod worker is going to be shutting it down If the pod hasnt started yet we know that when the pod worker is invoked it will also avoid setting up the pod so 

validateContainerLogStatus returns the container ID for the desired container to retrieve logs for based on the state of the container The previous flag will only return the logs for the the last terminated container otherwise the current running container is preferred over a previous termination If info about the container is not available then a specific error is returned to the end user output some info for the most common pending failures unrecognized state 
See the License for the specific language governing permissions and limitations under the License Max amount of time to wait for the container runtime to come up nodeStatusUpdateRetry specifies how many times kubelet retries when posting node status failed ContainerLogsDir is the location of container logs MaxContainerBackOff is the max backoff period exported for the e2e test 

validateContainerLogStatus returns the container ID for the desired container to retrieve logs for based on the state of the container The previous flag will only return the logs for the the last terminated container otherwise the current running container is preferred over a previous termination If info about the container is not available then a specific error is returned to the end user output some info for the most common pending failures 
 Fetch the pull secrets for the pod  Call the container runtimes SyncPod callback  Update the traffic shaping for the pods ingress and egress limits If any step of this workflow errors the error is returned and is repeated on the next syncPod call This operation writes all events that are dispatched in order to provide 

validateContainerLogStatus returns the container ID for the desired container to retrieve logs for based on the state of the container The previous flag will only return the logs for the the last terminated container otherwise the current running container is preferred over a previous termination If info about the container is not available then a specific error is returned to the end user output some info for the most common pending failures 
 Wait for volumes to attachmount  Fetch the pull secrets for the pod  Call the container runtimes SyncPod callback  Update the traffic shaping for the pods ingress and egress limits If any step of this workflow errors the error is returned and is repeated on the next syncPod call 

to the pod manager validateContainerLogStatus returns the container ID for the desired container to retrieve logs for based on the state of the container The previous flag will only return the logs for the the last terminated container otherwise the current running container is preferred over a previous termination If info about the container is not available then a specific error is returned to the end user output some info for the most common pending failures 
See the License for the specific language governing permissions and limitations under the License Max amount of time to wait for the container runtime to come up nodeStatusUpdateRetry specifies how many times kubelet retries when posting node status failed ContainerLogsDir is the location of container logs MaxContainerBackOff is the max backoff period exported for the e2e test 

If the pod is in a termianted state there is no pod worker to handle the work item Check if the DeletionTimestamp has been set and force a status update to trigger a pod deletion request to the apiserver Run the sync in an async worker Note the number of containers for new pods 
Mirror pod ADDUPDATEDELETE operations are considered an UPDATE to the corresponding static pod Send update to the pod worker if the static pod exists HandlePodAdditions is the callback in SyncHandler for pods being added from a config source 

If the sources arent ready skip housekeeping as we may accidentally delete pods from unready sources If the pod is in a termianted state there is no pod worker to handle the work item Check if the DeletionTimestamp has been 
If the sources arent ready or volume manager has not yet synced the states skip housekeeping as we may accidentally delete pods from unready sources We should not use the pod from manager because it is never updated after initialization If the pod no longer exists ignore the update 

If the sources arent ready skip housekeeping as we may 
If the sources arent ready skip deletion as we may accidentally delete pods 

If the pod no longer exists ignore the update 
If the pod no longer exists ignore the event 

We should not use the pod from livenessManager because it is never updated after 
We should not use the pod from manager because it is never updated after initialization 

We only care about failures signalling container death here We should not use the pod from livenessManager because it is never updated after initialization If the pod no longer exists ignore the update If the sources arent ready skip housekeeping as we may 
we simply avoid doing any work We failed pods that we rejected so activePods include all admitted pods that are alive Check if we can admit the pod if not reject it HandlePodUpdates is the callback in the SyncHandler interface for pods being updated from a config source 

If the pod no longer exists ignore the event 
If the pod no longer exists ignore the update 

admission process and may be rejcted This can be resolved once we have checkpointing TODO Do we want to support this If the pod no longer exists ignore the event Force the container runtime cache to update 
If the pod is terminal we dont need to continue to setup the pod If the pod should not be running we request the pods containers be stopped This is not the same as termination we want to stop the pod but potentially restart it later if soft admission allows 

admission process and may be rejcted This can be resolved 
admission process and may be rejected This can be resolved 

The resyncTicker wakes up kubelet to checks if there are any pod workers 
The syncTicker wakes up kubelet to checks if there are any pod workers 

cannot pod is new pod while pods include all admitted pods plus the new pod The function returns a boolean value indicating whether the pod can be admitted a brief singleword reason and a message explaining why the pod cannot be admitted This needs to be kept in sync with the schedulers and daemonsets fit predicates otherwise there will inevitably be pod delete create loops This will be fixed 
updateType  whether this is a create first time or an update should 	only be used for metrics since this method must be reentrant pod  the pod that is being set up mirrorPod  the mirror pod known to the kubelet for this pod if any podStatus  the most recent pod status observed for this pod which can 	be used to determine the set of actions that should be taken during 

cannot pod is new pod while pods include all admitted pods plus the new pod The function returns a boolean value indicating whether the pod can be admitted a brief singleword reason and a message explaining why the pod cannot be admitted This needs to be kept in sync with the schedulers and daemonsets fit predicates 
and updates the pod to the failed phase in the status manage canAdmitPod determines if a pod can be admitted and gives a reason if it cannot pod is new pod while pods are all admitted pods 

canAdmitPod determines if a pod can be admitted and gives a reason if it cannot pod is new pod while pods include all admitted pods plus the new pod The function returns a boolean value indicating whether the pod can be admitted a brief singleword reason and a message explaining why 
 Stop the pods containers if it should not be running due to soft admission  Ensure any background tracking for a runnable pod is started  Create a mirror pod if the pod is a static pod and does not 

canAdmitPod determines if a pod can be admitted and gives a reason if it cannot pod is new pod while pods include all admitted pods plus the new pod The function returns a boolean value indicating whether the pod 
 Stop the pods containers if it should not be running due to soft admission  Ensure any background tracking for a runnable pod is started  Create a mirror pod if the pod is a static pod and does not already have a mirror pod 

Pod phase progresses monotonically Once a pod has reached a final state it should never leave regardless of the restart policy The statuses of such pods should not be changed and there is no need to sync them TODO the logic here does not handle two cases 1 If the containers were removed immediately after they died kubelet may fail to generate correct statuses let alone filtering correctly 
three channels file apiserver and http and creates a union of them For any new change seen will run a sync against desired state and running state If no changes are seen to the configuration will synchronize the last known desired state every syncfrequency seconds Never returns The syncTicker wakes up kubelet to checks if there are any pod workers that need to be syncd A onesecond period is sufficient because the 

If the sources arent ready skip deletion as we may accidentally delete pods for sources that havent reported yet Runtime cache may not have been updated to with the pod but its okay 
If the sources arent ready or volume manager has not yet synced the states skip housekeeping as we may accidentally delete pods from unready sources 

the pod is no longer considered bound to this node If the sources arent ready skip deletion as we may accidentally delete pods for sources that havent reported yet 
If the sources arent ready or volume manager has not yet synced the states skip housekeeping as we may accidentally delete pods from unready sources We should not use the pod from manager because it is never updated after initialization If the pod no longer exists ignore the update 

the pod is no longer considered bound to this node If the sources arent ready skip deletion as we may accidentally delete pods for sources that havent reported yet 
If the pod no longer exists ignore the event Sync pods waiting for sync If the sources arent ready or volume manager has not yet synced the states skip housekeeping as we may accidentally delete pods from unready sources 

If there is no cached status use the status from the apiserver This is useful if kubelet has recently been restarted removeOrphanedPodStatuses removes obsolete entries in podStatus where the pod is no longer considered bound to this node If the sources arent ready skip deletion as we may accidentally delete pods 
If the sources arent ready or volume manager has not yet synced the states skip housekeeping as we may accidentally delete pods from unready sources We should not use the pod from manager because it is never updated after initialization If the pod no longer exists ignore the update dispatchWork starts the asynchronous sync of the pod in a pod worker If the pod has completed termination dispatchWork will perform no action 

If there is no cached status use the status from the apiserver This is useful if kubelet has recently been restarted removeOrphanedPodStatuses removes obsolete entries in podStatus where the pod is no longer considered bound to this node If the sources arent ready skip deletion as we may accidentally delete pods 
If the pod no longer exists ignore the event Sync pods waiting for sync If the sources arent ready or volume manager has not yet synced the states skip housekeeping as we may accidentally delete pods from unready sources We should not use the pod from manager because it is never updated after initialization If the pod no longer exists ignore the update 

Ignore the error and hope its resolved next time cleanupOrphanedPodDirs removes a pod directory if the pod is not in the desired set of pods and there is no running containers in the pod TODOrandomliu Cleanup status get functions issue 20477 Compares the map of current volumes to the map of desired volumes If an active volume does not have a respective desired volume clean it up 
a config source Always add the pod to the pod manager Kubelet relies on the pod manager as the source of truth for the desired state If a pod does not exist in the pod manager it means that it has been deleted in the apiserver and no action other than cleanup is required 

Ignore the error and hope its resolved next time cleanupOrphanedPodDirs removes a pod directory if the pod is not in the desired set of pods and there is no running containers in the pod TODOrandomliu Cleanup status get functions issue 20477 Compares the map of current volumes to the map of desired volumes If an active volume does not have a respective desired volume clean it up 
We should not use the pod from manager because it is never updated after initialization If the pod no longer exists ignore the update dispatchWork starts the asynchronous sync of the pod in a pod worker If the pod has completed termination dispatchWork will perform no action Run the sync in an async worker Note the number of containers for new pods 

Ignore the error and hope its resolved next time cleanupOrphanedPodDirs removes a pod directory if the pod is not in the desired set of pods and there is no running containers in the pod TODOrandomliu Cleanup status get functions issue 20477 
Always add the pod to the pod manager Kubelet relies on the pod manager as the source of truth for the desired state If a pod does not exist in the pod manager it means that it has been deleted in the apiserver and no action other than cleanup is required Only go through the admission process if the pod is not requested 

Ignore the error and hope its resolved next time cleanupOrphanedPodDirs removes a pod directory if the pod is not in the desired set of pods and there is no running containers in the pod 
HandlePodAdditions is the callback in SyncHandler for pods being added from a config source Always add the pod to the pod manager Kubelet relies on the pod manager as the source of truth for the desired state If a pod does not exist in the pod manager it means that it has been deleted in the apiserver and no action other than cleanup is required 

to volumes that are bound to them Keys for each entry are in the format POD_IDVOLUME_NAME Ignore the error and hope its resolved next time cleanupOrphanedPodDirs removes a pod directory if the pod is not in the 
update the timestamp in the sync loop monitor Here is an appropriate place to note that despite the syntactical similarity to the switch statement the case statements in a select are evaluated in a pseudorandom order if there are multiple channels ready to 

It stores real volumes there ie persistent volume claims are resolved to volumes that are bound to them Keys for each entry are in the format POD_IDVOLUME_NAME Ignore the error and hope its resolved next time cleanupOrphanedPodDirs removes a pod directory if the pod is not in the 
Note that even though we set the period to 1s the relisting itself can take more than 1s to finish if the container runtime responds slowly andor when there are many container changes in one cycle backOffPeriod is the period to back off when pod syncing results in an error It is also used as the base period for the exponential backoff 

TODO Modify containerRuntimeKillPod to accept the right arguments makePodDataDirs creates the dirs for the pod datas 
 configCh dispatch the pods for the config change to the appropriate 

One of the following aruguements must be nonnil runningPod status TODO Modify containerRuntimeKillPod to accept the right arguments makePodDataDirs creates the dirs for the pod datas This is the first time we are syncing the pod Record the latency since kubelet first saw the pod if firstSeenTime is set Record the time it takes for the pod to become running 
mirrorPod  the mirror pod known to the kubelet for this pod if any podStatus  the most recent pod status observed for this pod which can 	be used to determine the set of actions that should be taken during 	this loop of syncPod The workflow is  If the pod is being created record pod worker start latency 

One of the following aruguements must be nonnil runningPod status TODO Modify containerRuntimeKillPod to accept the right arguments makePodDataDirs creates the dirs for the pod datas This is the first time we are syncing the pod Record the latency since kubelet first saw the pod if firstSeenTime is set 
pod  the pod that is being set up mirrorPod  the mirror pod known to the kubelet for this pod if any podStatus  the most recent pod status observed for this pod which can 	be used to determine the set of actions that should be taken during 	this loop of syncPod The workflow is 

Give the cloudprovider a chance to postprocess DNS settings One of the following aruguements must be nonnil runningPod status TODO Modify containerRuntimeKillPod to accept the right arguments makePodDataDirs creates the dirs for the pod datas This is the first time we are syncing the pod Record the latency since kubelet first saw the pod if firstSeenTime is set 
mirrorPod  the mirror pod known to the kubelet for this pod if any podStatus  the most recent pod status observed for this pod which can 	be used to determine the set of actions that should be taken during 	this loop of syncPod The workflow is  If the pod is being created record pod worker start latency 

local machine A nameserver setting of localhost is equivalent to this documented behavior for a pod with DNSClusterFirst policy the cluster DNS server is the only nameserver configured for the pod The cluster DNS server itself will forward queries to other nameservers that is configured to use 
List of taints to add to a node object when the kubelet registers itself Set to true to have the node register itself as schedulable for internal book keeping access only from within registerWithApiserver dnsConfigurer is used for setting up DNS resolver configuration when launching pods masterServiceNamespace is the namespace that the master service is exposed in 

etcresolvconf and effectivly disable DNS lookups According to the bind documentation the behavior of the DNS client library when nameservers are not specified is to use the nameserver on the local machine A nameserver setting of localhost is equivalent to this documented behavior for a pod with DNSClusterFirst policy the cluster DNS server is the only nameserver configured for 
List of taints to add to a node object when the kubelet registers itself Set to true to have the node register itself as schedulable for internal book keeping access only from within registerWithApiserver dnsConfigurer is used for setting up DNS resolver configuration when launching pods masterServiceNamespace is the namespace that the master service is exposed in 

When the kubelet resolvconf flag is set to the empty string use DNS settings that override the docker default which is to use etcresolvconf and effectivly disable DNS lookups According to the bind documentation the behavior of the DNS client library when nameservers are not specified is to use the nameserver on the local machine A nameserver setting of localhost is equivalent to 
List of taints to add to a node object when the kubelet registers itself Set to true to have the node register itself as schedulable for internal book keeping access only from within registerWithApiserver dnsConfigurer is used for setting up DNS resolver configuration when launching pods masterServiceNamespace is the namespace that the master service is exposed in 

fallback to DNSDefault When the kubelet resolvconf flag is set to the empty string use DNS settings that override the docker default which is to use etcresolvconf and effectivly disable DNS lookups According to the bind documentation the behavior of the DNS client library when nameservers are not specified is to use the nameserver on the 
Set to true to have the node register itself with the apiserver List of taints to add to a node object when the kubelet registers itself Set to true to have the node register itself as schedulable for internal book keeping access only from within registerWithApiserver dnsConfigurer is used for setting up DNS resolver configuration when launching pods masterServiceNamespace is the namespace that the master service is exposed in 

Make the service environment variables for a pod in the given namespace Note  These are added to the dockerConfig but are not included in the checksum computed by dockertoolsBuildDockerName  That way we can still determine whether an 
update the timestamp in the sync loop monitor Here is an appropriate place to note that despite the syntactical similarity to the switch statement the case statements in a select are evaluated in a pseudorandom order if there are multiple channels ready to read from when the select is evaluated  In other words case statements are evaluated in random order and you can not assume that the case 

ordering of the case clauses below enforces this Make the service environment variables for a pod in the given namespace Note  These are added to the dockerConfig but are not included in the checksum computed by dockertoolsBuildDockerName  That way we can still determine whether an 
The workflow is to read from one of the channels handle that event and update the timestamp in the sync loop monitor Here is an appropriate place to note that despite the syntactical similarity to the switch statement the case statements in a select are evaluated in a pseudorandom order if there are multiple channels ready to 

for the case whether the master service namespace is the namespace the pod is in the pod should receive all the services in the namespace ordering of the case clauses below enforces this Make the service environment variables for a pod in the given namespace Note  These are added to the dockerConfig but are not included in the checksum computed by dockertoolsBuildDockerName  That way we can still determine whether an 
The workflow is to read from one of the channels handle that event and update the timestamp in the sync loop monitor Here is an appropriate place to note that despite the syntactical similarity to the switch statement the case statements in a select are 

GenerateRunContainerOptions generates the RunContainerOptions which can be used by the container runtime to set parameters for launching a container Docker does not relabel volumes if the container is running in the host pid or ipc namespaces so the kubelet must 
CheckpointContainer tries to checkpoint a container The parameters are used to look up the specified container If the container specified by the given parameters cannot be found an error is returned If the container is found the container engine will be asked to checkpoint the given container into the kubelets default checkpoint directory isSyncPodWorthy filters out events that are not worthy of pod syncing 

GenerateRunContainerOptions generates the RunContainerOptions which can be used by the container runtime to set parameters for launching a container Docker does not relabel volumes if the container is running in the host pid or ipc namespaces so the kubelet must 
or using host path volumes This should only be enabled when the container runtime is performing user remapping AND if the experimental behavior is desired StatsProvider provides the node and the container stats This flag if set instructs the kubelet to keep volumes from terminated pods mounted to the node 

GenerateRunContainerOptions generates the RunContainerOptions which can be used by the container runtime to set parameters for launching a container Docker does not relabel volumes if the container is running 
Function is executed only during Kubelet start which improves latency to ready node by updating pod CIDR runtime status and node statuses ASAP CheckpointContainer tries to checkpoint a container The parameters are used to look up the specified container If the container specified by the given parameters cannot be found an error is returned If the container is found the container 

GenerateRunContainerOptions generates the RunContainerOptions which can be used by the container runtime to set parameters for launching a container Docker does not relabel volumes if the container is running 
or using host path volumes This should only be enabled when the container runtime is performing user remapping AND if the experimental behavior is desired StatsProvider provides the node and the container stats This flag if set instructs the kubelet to keep volumes from terminated pods mounted to the node This can be useful for debugging volume related issues 

GenerateRunContainerOptions generates the RunContainerOptions which can be used by the container runtime to set parameters for launching a container Docker does not relabel volumes if the container is running 
are using nonnamespaced capabilities mknod sys_time sys_module the pod contains a privileged container or using host path volumes This should only be enabled when the container runtime is performing user remapping AND if the experimental behavior is desired StatsProvider provides the node and the container stats This flag if set instructs the kubelet to keep volumes from terminated pods mounted to the node 

use hostname annotation if specified GenerateRunContainerOptions generates the RunContainerOptions which can be used by the container runtime to set parameters for launching a container Docker does not relabel volumes if the container is running 
This should only be enabled when the container runtime is performing user remapping AND if the experimental behavior is desired StatsProvider provides the node and the container stats This flag if set instructs the kubelet to keep volumes from terminated pods mounted to the node This can be useful for debugging volume related issues 

use hostname annotation if specified GenerateRunContainerOptions generates the RunContainerOptions which can be used by the container runtime to set parameters for launching a container Docker does not relabel volumes if the container is running 
or using host path volumes This should only be enabled when the container runtime is performing user remapping AND if the experimental behavior is desired StatsProvider provides the node and the container stats This flag if set instructs the kubelet to keep volumes from terminated pods mounted to the node 

initializeRuntimeDependentModules will initialize internal modules that require the container runtime to be up 
initializeModules will initialize internal modules that do not require the container runtime to be up 

initializeModules will initialize internal modules that do not require the container runtime to be up 
initializeRuntimeDependentModules will initialize internal modules that require the container runtime to be up 

that may need to know where to write data without getting a whole kubelet instance getPodsDir returns the full path to the directory under which pod directories are created 
podManager is a facade that abstracts away the various sources of pods this Kubelet services Needed to observe and respond to situations that could impact node stability Optional defaults to logs from varlog 

store data  These functions are useful to pass interfaces to other modules that may need to know where to write data without getting a whole kubelet instance getPodsDir returns the full path to the directory under which pod 
Needed to observe and respond to situations that could impact node stability Optional defaults to logs from varlog Optional defaults to simple Docker implementation cAdvisor used for container information Set to true to have the node register itself with the apiserver 

getRootDir returns the full path to the directory under which kubelet can store data  These functions are useful to pass interfaces to other modules that may need to know where to write data without getting a whole kubelet instance getPodsDir returns the full path to the directory under which pod 
podManager is a facade that abstracts away the various sources of pods this Kubelet services Needed to observe and respond to situations that could impact node stability Optional defaults to logs from varlog Optional defaults to simple Docker implementation 

getRootDir returns the full path to the directory under which kubelet can store data  These functions are useful to pass interfaces to other modules that may need to know where to write data without getting a whole kubelet 
this Kubelet services Needed to observe and respond to situations that could impact node stability Optional defaults to logs from varlog Optional defaults to simple Docker implementation cAdvisor used for container information Set to true to have the node register itself with the apiserver 

Make a copy of the sourcesSeen list because its not threadsafe getRootDir returns the full path to the directory under which kubelet can store data  These functions are useful to pass interfaces to other modules that may need to know where to write data without getting a whole kubelet instance getPodsDir returns the full path to the directory under which pod 
cAdvisor used for container information Set to true to have the node register itself with the apiserver List of taints to add to a node object when the kubelet registers itself 

Make a copy of the sourcesSeen list because its not threadsafe getRootDir returns the full path to the directory under which kubelet can store data  These functions are useful to pass interfaces to other modules that may need to know where to write data without getting a whole kubelet 
podManager is a facade that abstracts away the various sources of pods this Kubelet services Needed to observe and respond to situations that could impact node stability Optional defaults to logs from varlog 

Make a copy of the sourcesSeen list because its not threadsafe getRootDir returns the full path to the directory under which kubelet can store data  These functions are useful to pass interfaces to other modules 
podManager is a facade that abstracts away the various sources of pods this Kubelet services Needed to observe and respond to situations that could impact node stability Optional defaults to logs from varlog Optional defaults to simple Docker implementation cAdvisor used for container information 

Honor IP limitations set in setNodeStatus Make a copy of the sourcesSeen list because its not threadsafe getRootDir returns the full path to the directory under which kubelet can store data  These functions are useful to pass interfaces to other modules that may need to know where to write data without getting a whole kubelet 
Optional defaults to simple Docker implementation cAdvisor used for container information Set to true to have the node register itself with the apiserver List of taints to add to a node object when the kubelet registers itself Set to true to have the node register itself as schedulable for internal book keeping access only from within registerWithApiserver 

Honor IP limitations set in setNodeStatus Make a copy of the sourcesSeen list because its not threadsafe getRootDir returns the full path to the directory under which kubelet can store data  These functions are useful to pass interfaces to other modules that may need to know where to write data without getting a whole kubelet 
Needed to observe and respond to situations that could impact node stability Optional defaults to logs from varlog Optional defaults to simple Docker implementation cAdvisor used for container information Set to true to have the node register itself with the apiserver List of taints to add to a node object when the kubelet registers itself 

Validate given node IP belongs to the current host Honor IP limitations set in setNodeStatus Make a copy of the sourcesSeen list because its not threadsafe getRootDir returns the full path to the directory under which kubelet can store data  These functions are useful to pass interfaces to other modules that may need to know where to write data without getting a whole kubelet 
cAdvisor used for container information Set to true to have the node register itself with the apiserver List of taints to add to a node object when the kubelet registers itself Set to true to have the node register itself as schedulable for internal book keeping access only from within registerWithApiserver 

Validate given node IP belongs to the current host Honor IP limitations set in setNodeStatus Make a copy of the sourcesSeen list because its not threadsafe getRootDir returns the full path to the directory under which kubelet can store data  These functions are useful to pass interfaces to other modules that may need to know where to write data without getting a whole kubelet 
Optional defaults to simple Docker implementation cAdvisor used for container information Set to true to have the node register itself with the apiserver List of taints to add to a node object when the kubelet registers itself 

outOfDiskTransitionFrequency specifies the amount of time the kubelet has to be actually not out of disk before it can transition the node condition status from outofdisk to notoutofdisk This prevents a pod that causes outofdisk condition from repeatedly getting rescheduled onto the node reservation specifies resources which are reserved for nonpod usage including kubernetes and nonkubernetes system processes 
nodeStatusUpdateRetry specifies how many times kubelet retries when posting node status failed ContainerLogsDir is the location of container logs MaxContainerBackOff is the max backoff period exported for the e2e test Period for performing global cleanup tasks Duration at which housekeeping failed to satisfy the invariant that housekeeping should be fast to avoid blocking pod config while 

outOfDiskTransitionFrequency specifies the amount of time the kubelet has to be actually not out of disk before it can transition the node condition status from outofdisk to notoutofdisk This prevents a pod that causes outofdisk condition from repeatedly getting rescheduled onto the node reservation specifies resources which are reserved for nonpod usage including kubernetes and 
pods on this node sourcesReady records the sources seen by the kubelet it is threadsafe podManager is a facade that abstracts away the various sources of pods this Kubelet services Needed to observe and respond to situations that could impact node stability 

easy to test the code outOfDiskTransitionFrequency specifies the amount of time the kubelet has to be actually not out of disk before it can transition the node condition status from outofdisk to notoutofdisk This prevents a pod that causes outofdisk condition from repeatedly getting rescheduled onto the node reservation specifies resources which are reserved for nonpod usage including kubernetes and 
plugins need to be registeredunregistered based on this node and makes it so This flag sets a maximum number of images to report in the node status Handles RuntimeClass objects for the Kubelet Handles node shutdown events for the Node Manage user namespaces ListPodStats is delegated to StatsProvider which implements statsProvider interface 

easy to test the code outOfDiskTransitionFrequency specifies the amount of time the kubelet has to be actually not out of disk before it can transition the node condition status from outofdisk to notoutofdisk This prevents a pod that causes outofdisk condition from repeatedly getting rescheduled onto the node reservation specifies resources which are reserved for nonpod usage including kubernetes and 
experimental behavior is desired StatsProvider provides the node and the container stats This flag if set instructs the kubelet to keep volumes from terminated pods mounted to the node This can be useful for debugging volume related issues DEPRECATED pluginmanager runs a set of asynchronous loops that figure out which 

easy to test the code outOfDiskTransitionFrequency specifies the amount of time the kubelet has to be actually not out of disk before it can transition the node condition status from outofdisk to notoutofdisk This prevents a pod that causes outofdisk condition from repeatedly getting rescheduled onto the node reservation specifies resources which are reserved for nonpod usage including kubernetes and 
sourcesReady records the sources seen by the kubelet it is threadsafe podManager is a facade that abstracts away the various sources of pods this Kubelet services Needed to observe and respond to situations that could impact node stability Optional defaults to logs from varlog 

easy to test the code outOfDiskTransitionFrequency specifies the amount of time the kubelet has to be actually not out of disk before it can transition the node condition status from outofdisk to notoutofdisk This prevents a pod that causes outofdisk condition from repeatedly getting rescheduled onto the node reservation specifies resources which are reserved for nonpod usage including kubernetes and 
nodeStatusUpdateRetry specifies how many times kubelet retries when posting node status failed ContainerLogsDir is the location of container logs MaxContainerBackOff is the max backoff period exported for the e2e test Period for performing global cleanup tasks Duration at which housekeeping failed to satisfy the invariant that housekeeping should be fast to avoid blocking pod config while 

easy to test the code outOfDiskTransitionFrequency specifies the amount of time the kubelet has to be actually not out of disk before it can transition the node condition status from outofdisk to notoutofdisk This prevents a pod that causes outofdisk condition from repeatedly 
pods on this node sourcesReady records the sources seen by the kubelet it is threadsafe podManager is a facade that abstracts away the various sources of pods this Kubelet services Needed to observe and respond to situations that could impact node stability Optional defaults to logs from varlog 

clock is an interface that provides time related functionality in a way that makes it easy to test the code outOfDiskTransitionFrequency specifies the amount of time the kubelet has to be actually not out of disk before it can transition the node condition status from outofdisk to notoutofdisk This prevents a pod that causes outofdisk condition from repeatedly 
pods on this node sourcesReady records the sources seen by the kubelet it is threadsafe podManager is a facade that abstracts away the various sources of pods this Kubelet services Needed to observe and respond to situations that could impact node stability 

put the system under duress If nonnil use this IP address for the node clock is an interface that provides time related functionality in a way that makes it easy to test the code outOfDiskTransitionFrequency specifies the amount of time the kubelet has to be actually not out of disk before it can transition the node condition status from outofdisk to 
This should only be enabled when the container runtime is performing user remapping AND if the experimental behavior is desired StatsProvider provides the node and the container stats This flag if set instructs the kubelet to keep volumes from terminated pods mounted to the node 

put the system under duress If nonnil use this IP address for the node clock is an interface that provides time related functionality in a way that makes it easy to test the code outOfDiskTransitionFrequency specifies the amount of time the kubelet has to be actually not out of disk before it can transition the node condition status from outofdisk to 
reasonCache caches the failure reason of the last creation of all containers which is used for generating ContainerStatus nodeStatusUpdateFrequency specifies how often kubelet computes node status If node lease feature is not enabled it is also the frequency that kubelet posts node status to master In that case be cautious when changing the constant it must work with nodeMonitorGracePeriod 

on the fly if were confident the dbus connetions it opens doesnt put the system under duress If nonnil use this IP address for the node clock is an interface that provides time related functionality in a way that makes it easy to test the code outOfDiskTransitionFrequency specifies the amount of time the kubelet has to be actually 
This should only be enabled when the container runtime is performing user remapping AND if the experimental behavior is desired StatsProvider provides the node and the container stats This flag if set instructs the kubelet to keep volumes from terminated pods mounted to the node 

Optionally shape the bandwidth of a pod TODO remove when kubenet plugin is ready True if container cpu limits should be enforced via cgroup CFS quota Information about the ports which are opened by daemons on Node running this Kubelet server A queue used to trigger pod workers oneTimeInitializer is used to initialize modules that are dependent on the runtime to be up 
Function is executed only during Kubelet start which improves latency to ready node by updating pod CIDR runtime status and node statuses ASAP CheckpointContainer tries to checkpoint a container The parameters are used to look up the specified container If the container specified by the given parameters cannot be found an error is returned If the container is found the container engine will be asked to checkpoint the given container into the kubelets default 

Optionally shape the bandwidth of a pod TODO remove when kubenet plugin is ready True if container cpu limits should be enforced via cgroup CFS quota Information about the ports which are opened by daemons on Node running this Kubelet server A queue used to trigger pod workers 
common provider to get host file system usage associated with a pod managed by kubelet setup containerGC setup imageManager NewInitializedVolumePluginMgr initializes some storageErrors on the Kubelet runtimeState in csi_plugingo init which affects node ready status This function must be called before Kubelet is initialized so that the Node 

DNS resolver configuration file This can be used in conjunction with clusterDomain and clusterDNS Optionally shape the bandwidth of a pod TODO remove when kubenet plugin is ready True if container cpu limits should be enforced via cgroup CFS quota Information about the ports which are opened by daemons on Node running this Kubelet server 
common provider to get host file system usage associated with a pod managed by kubelet setup containerGC setup imageManager NewInitializedVolumePluginMgr initializes some storageErrors on the Kubelet runtimeState in csi_plugingo init which affects node ready status This function must be called before Kubelet is initialized so that the Node 

Reference to this node Container runtime reasonCache caches the failure reason of the last creation of all containers which is used for generating ContainerStatus nodeStatusUpdateFrequency specifies how often kubelet posts node status to master Note be cautious when changing the constant it must work with nodeMonitorGracePeriod 
nodeStatusUpdateRetry specifies how many times kubelet retries when posting node status failed ContainerLogsDir is the location of container logs MaxContainerBackOff is the max backoff period exported for the e2e test Period for performing global cleanup tasks Duration at which housekeeping failed to satisfy the invariant that housekeeping should be fast to avoid blocking pod config while 

Reference to this node Container runtime reasonCache caches the failure reason of the last creation of all containers which is used for generating ContainerStatus nodeStatusUpdateFrequency specifies how often kubelet posts node status to master 
See the License for the specific language governing permissions and limitations under the License Max amount of time to wait for the container runtime to come up nodeStatusUpdateRetry specifies how many times kubelet retries when posting node status failed ContainerLogsDir is the location of container logs 

Diskspace manager 
ConfigMap manager 

Set to true if the kubelet is in standalone mode ie setup without an apiserver If nonempty use this for container DNS search If nonnil use this for container DNS server a list of node labels to register 
If set use this IP address or addresses for the node use this function to validate the kubelet nodeIP If nonnil this is a unique identifier for the node in an external database eg cloudprovider 

setup imageManager 
setup volumeManager 

setup imageManager 
setup containerGC 

setup containerGC 
setup volumeManager 

setup containerGC 
setup imageManager 

New instantiates a new Kubelet object along with all the required internal modules 
NewMainKubelet instantiates a new Kubelet object along with all the required internal modules 

backOffPeriod is the period to back off when pod syncing resulting in an 
backOffPeriod is the period to back off when pod syncing results in an 

Capacity of the channel for storing pods to kill A small number should suffice because a goroutine is dedicated to check the channel and does not block on anything else Period for performing global cleanup tasks Capacity of the channel for receiving pod lifecycle events This number is a bit arbitrary and may be adjusted in the future 
nodeLeaseController claims and renews the node lease for this Kubelet Generates pod events Store kubecontainerPodStatus for all pods os is a facade for various syscalls that need to be mocked during testing Watcher of out of memory events Monitor resource usage 

max backoff period exported for the e2e test Capacity of the channel for storing pods to kill A small number should suffice because a goroutine is dedicated to check the channel and does not block on anything else Period for performing global cleanup tasks 
Capacity of the channel for receiving pod lifecycle events This number is a bit arbitrary and may be adjusted in the future Generic PLEG relies on relisting for discovering container events A longer period means that kubelet will take longer to detect container changes and to update pod status On the other hand a shorter period 

Location of container logs max backoff period exported for the e2e test Capacity of the channel for storing pods to kill A small number should suffice because a goroutine is dedicated to check the channel and does 
and disable kubelet from executing any attachdetach operations trigger deleting containers in a pod config iptables util rules The bit of the fwmark space to mark packets for SNAT The bit of the fwmark space to mark packets for dropping The AppArmor validator for checking whether AppArmor is supported 

nodeStatusUpdateRetry specifies how many times kubelet retries when posting node status failed Location of container logs max backoff period exported for the e2e test Capacity of the channel for storing pods to kill A small number should suffice because a goroutine is dedicated to check the channel and does not block on anything else 
Capacity of the channel for receiving pod lifecycle events This number is a bit arbitrary and may be adjusted in the future Generic PLEG relies on relisting for discovering container events A longer period means that kubelet will take longer to detect container changes and to update pod status On the other hand a shorter period will cause more frequent relisting eg container runtime operations 

bar           databar 
bar  databar 

the following symlinks and subdirectories are created 
the following symlinks are created 

to avoid attempting to remove nonempty dirs newTimestampDir creates a new timestamp directory writePayloadToDir writes the given payload to the given directory  The 
NewAtomicWriter creates a new AtomicWriter configured to write to the given 

pathsToRemove walks the uservisible portion of the target directory and determines which paths should be removed if any after the payload is written to the target directory add all subpaths for the payload to the set of new paths to avoid attempting to remove nonempty dirs newTimestampDir creates a new timestamp directory 
directory tree  do a chmod here to ensure that permissions are set correctly regardless of the process umask writePayloadToDir writes the given payload to the given directory  The 

pathsToRemove walks the uservisible portion of the target directory and determines which paths should be removed if any after the payload is written to the target directory add all subpaths for the payload to the set of new paths to avoid attempting to remove nonempty dirs 
0755 permissions are needed to allow group and other to recurse the directory tree  do a chmod here to ensure that permissions are set correctly regardless of the process umask writePayloadToDir writes the given payload to the given directory  The 

pathsToRemove walks the uservisible portion of the target directory and 
removeUserVisiblePaths removes the set of paths from the uservisible portion of the writers target directory 

refactoring to reuse shouldWritePayload returns whether the payload should be written to disk shouldWriteFile returns whether a new version of a file should be written to disk pathsToRemove walks the uservisible portion of the target directory and 
pathsToRemove walks the current version of the data directory and determines which paths should be removed if any after the payload is written to the target directory add all subpaths for the payload to the set of new paths to avoid attempting to remove nonempty dirs newTimestampDir creates a new timestamp directory 

from this that it was timeprohibitive trying to find the right refactoring to reuse shouldWritePayload returns whether the payload should be written to disk shouldWriteFile returns whether a new version of a file should be written to disk pathsToRemove walks the uservisible portion of the target directory and 
determines which paths should be removed if any after the payload is written to the target directory add all subpaths for the payload to the set of new paths to avoid attempting to remove nonempty dirs newTimestampDir creates a new timestamp directory 

validatePayload returns an error if any path in the payload  returns a copy of the payload with the paths cleaned 
validatePayload returns an error if any path in the payload returns a copy of the payload with the paths cleaned 

8 9 
7 

8 
6 7 

7 
8 9 

6 7 
8 

4 
3 

3 
4 

3 
2 

2 
3 

9  The new data directory symlink is renamed to the data directory rename is atomic 
8  The new data directory symlink is renamed to the data directory rename is atomic 

8  A symlink to the new timestamped directory data_tmp is created that will become the new data directory 9  The new data directory symlink is renamed to the data directory rename is atomic 10  Old paths are removed from the uservisible portion of the target directory 11  The previous timestamped directory is removed if it exists 
portion of the payload was deleted and is still present on disk 4 The data in the current timestamped directory is compared to the projected data to determine if an update is required 5  A new timestamped dir is created 6 The payload is written to the new timestamped directory 

8  A symlink to the new timestamped directory data_tmp is created that will 
7  A symlink to the new timestamped directory data_tmp is created that will 

7  The current timestamped directory is detected by reading the data directory 
2  The current timestamped directory is detected by reading the data directory 

Relative links are created into the data directory for files in subdirectories The data directory itself is a link to a timestamped directory with the real data targetdirdata           2016_02_01_15_04_0512345678 7  The current timestamped directory is detected by reading the data directory 
directory  Actual files are stored in a hidden timestamped directory which is symlinked to by the data directory The timestamped directory and 

6  Symlinks and directory for new uservisible files are created if needed 
9  Symlinks and directory for new uservisible files are created if needed 

5  The payload is written to the new timestamped directory 
6 The payload is written to the new timestamped directory 

4  A new timestamped dir is created 
5  A new timestamped dir is created 

2  The uservisible portion of the volume is walked to determine whether any 
3 The old version of the volume is walked to determine whether any 

1  The payload is validated if the payload is invalid the function returns 
1 The payload is validated if the payload is invalid the function returns 

Consumers of the target directory can monitor the data symlink using inotify or fanotify to receive events when the content in the volume is updated NewAtomicWriter creates a new AtomicWriter configured to write to the given 
pathsToRemove walks the current version of the data directory and determines which paths should be removed if any after the payload is written to the target directory add all subpaths for the payload to the set of new paths to avoid attempting to remove nonempty dirs newTimestampDir creates a new timestamp directory 

allows the files to be atomically updated by changing the target of the data directory symlink Consumers of the target directory can monitor the data symlink using inotify or fanotify to receive events when the content in the volume is updated NewAtomicWriter creates a new AtomicWriter configured to write to the given 
pathsToRemove walks the current version of the data directory and determines which paths should be removed if any after the payload is written to the target directory add all subpaths for the payload to the set of new paths 

data directory symlink are created in the writers target dir  This scheme allows the files to be atomically updated by changing the target of the data directory symlink 
2 AtomicWriter offers no concurrency guarantees and must be synchronized by the caller The visible files in this volume are symlinks to files in the writers data directory  Actual files are stored in a hidden timestamped directory which is symlinked to by the data directory The timestamped directory and 

2 AtomicWriter offers no concurrency guarantees and must be synchronized by the caller The visible files in this volume are symlinks to files in the writers data directory  Actual files are stored in a hidden timestamped directory which is symlinked to by the data directory The timestamped directory and 
data directory symlink are created in the writers target dir  This scheme allows the files to be atomically updated by changing the target of the data directory symlink 

GenerateContainerRef returns an apiObjectReference which references the given container 
GenerateContainerRef returns an v1ObjectReference which references the given container 

Detaches the disk from the kubelets host machine 
Detaches the block disk from the kubelets host machine 

Gets the time since the specified start in microseconds 
SinceInSeconds gets the time since the specified start in seconds 

Register all metrics 
Register registers all metrics 

lock the volume and thus wait for any concurrrent SetUpAt to finish 
lock the volume and thus wait for any concurrent SetUpAt to finish 

NewBuilder We could then find volumeID there without probing MountRefs 
NewMounter We could then find volumeID there without probing MountRefs 

TODO refactor VolumePluginNewCleaner to get full volumeSpec just like 
TODO refactor VolumePluginNewUnmounter to get full volumeSpec just like 

diskMounter provides the interface that is used to mount the actual block device SetUp attaches the disk and bind mounts to the volume path TODO handle failed mounts here TODO we should really eject the attachdetach out into its own control loop 
Mounter interface that provides system calls to mount the global path to the pod local path SetUp bind mounts to the volume path Perform a bind mount to the full path to allow duplicate mounts of the same PD This is very odd we dont expect it  Well try again next sync loop Unmounts the bind mount and detaches the disk only if the PD resource was the last reference to that disk on the kubelet 

diskMounter provides the interface that is used to mount the actual block device SetUp attaches the disk and bind mounts to the volume path 
Unique identifier of the volume used to find the disk resource in the provider Filesystem type optional Utility interface that provides API calls to the provider to attachdetach disks Mounter interface that provides system calls to mount the global path to the pod local path SetUp bind mounts to the volume path 

This is the primary entrypoint for volume plugins 
ProbeVolumePlugins is the primary entrypoint for volume plugins 

getFullContainerName gets the container name given the root process id of the container 
GetFullContainerName gets the container name given the root process id of the container 

for storing Minions Pods Schedulers and Services 
for storing Nodes Pods Schedulers and Services 

package qos contains helper functions for quality of service 
Package qos contains helper functions for quality of service 

Namespace is the name of a namespace 
Resource is the name of a resource 

Resource is the name of a resource 
Namespace is the name of a namespace 

readOnly bool is supplied by persistentclaim volume source when its builder creates other volumes 
readOnly bool is supplied by persistentclaim volume source when its mounter creates other volumes 

metricsDu represents a MetricsProvider that calculates the used and available Volume space by executing the du command and gathering filesystem info for the Volume path 
NewMetricsDu creates a new metricsDu with the Volume path GetMetrics calculates the volume usage and device free space by executing du and gathering filesystem info for the Volume path 

metricsDu represents a MetricsProvider that calculates the used and available Volume space by executing the du command and gathering filesystem info for the Volume path 
available Volume space by calling fsDiskUsage and gathering filesystem info for the Volume path the directory path the volume is mounted to NewMetricsDu creates a new metricsDu with the Volume path GetMetrics calculates the volume usage and device free space by executing du 

readOnly bool is supplied by persistentclaim volume source when its builder creates other volumes 
readOnly bool is supplied by persistentclaim volume source when its mounter creates other volumes 

This is the primary entrypoint for volume plugins 
ProbeVolumePlugins is the primary entrypoint for volume plugins 

This is the primary entrypoint for volume plugins 
ProbeVolumePlugins is the primary entrypoint for volume plugins 

Resource takes an unqualified resource and returns back a Group qualified GroupResource 
Resource takes an unqualified resource and returns a Group qualified GroupResource 

Kind takes an unqualified kind and returns back a Group qualified GroupKind 
Kind takes an unqualified kind and returns a Group qualified GroupKind 

Allowed is required  True if the action would be allowed false otherwise 
Denied is optional True if the action would be denied otherwise 

Allowed is required  True if the action would be allowed false otherwise 
Allowed is required True if the action would be allowed false otherwise 

NonResourceAttributes describes information for a nonresource access request 
ResourceAttributes describes information for a resource access request 

NonResourceAttributes describes information for a nonresource access request 
ResourceAttributes describes information for a resource access request 

ResourceAttributes describes information for a resource access request 
NonResourceAttributes describes information for a nonresource access request 

ResourceAttributes describes information for a resource access request 
NonResourceAttributes describes information for a nonresource access request 

NonResourceAttributes describes information for a nonresource access request 
ResourceAttributes describes information for a resource access request 

NonResourceAttributes describes information for a nonresource access request 
ResourceAttributes describes information for a resource access request 

ResourceAttributes describes information for a resource access request 
NonResourceAttributes describes information for a nonresource access request 

ResourceAttributes describes information for a resource access request 
NonResourceAttributes describes information for a nonresource access request 

NonResourceAttributes includes the authorization attributes available for nonresource requests to the Authorizer interface Path is the URL path of the request Verb is the standard HTTP verb SubjectAccessReviewSpec is a description of the access request  Exactly one of ResourceAttributes and NonResourceAttributes must be set ResourceAttributes describes information for a resource access request 
Spec holds information about the request being evaluated Status is filled in by the server and indicates the set of actions a user can perform SelfSubjectRulesReviewSpec defines the specification for SelfSubjectRulesReview Namespace to evaluate rules for Required SubjectRulesReviewStatus contains the result of a rules check This check can be incomplete depending on the set of authorizers the server is configured with and any errors experienced during evaluation 

NonResourceAttributes includes the authorization attributes available for nonresource requests to the Authorizer interface 
ResourceAttributes includes the authorization attributes available for resource requests to the Authorizer interface 

Subresource is one of the existing resource types   means none Name is the name of the resource being requested for a get or deleted for a delete  empty means all NonResourceAttributes includes the authorization attributes available for nonresource requests to the Authorizer interface 
Resources is a list of resources this rule applies to   means all in the specified apiGroups foo represents the subresource foo for all resources in the specified apiGroups ResourceNames is an optional white list of names that the rule applies to  An empty set means that everything is allowed   means all NonResourceRule holds information that describes a rule for the nonresource Verb is a list of kubernetes nonresource API verbs like get post put delete patch head options   means all NonResourceURLs is a set of partial urls that a user should have access to  s are allowed but only as the full 

Subresource is one of the existing resource types   means none Name is the name of the resource being requested for a get or deleted for a delete  empty means all NonResourceAttributes includes the authorization attributes available for nonresource requests to the Authorizer interface 
APIGroups is the name of the APIGroup that contains the resources  If multiple API groups are specified any action requested against one of the enumerated resources in any API group will be allowed   means all Resources is a list of resources this rule applies to   means all in the specified apiGroups foo represents the subresource foo for all resources in the specified apiGroups ResourceNames is an optional white list of names that the rule applies to  An empty set means that everything is allowed   means all NonResourceRule holds information that describes a rule for the nonresource 

Subresource is one of the existing resource types   means none 
Resource is one of the existing resource types   means all 

Resource is one of the existing resource types   means all Subresource is one of the existing resource types   means none Name is the name of the resource being requested for a get or deleted for a delete  empty means all NonResourceAttributes includes the authorization attributes available for nonresource requests to the Authorizer interface 
Verb is a list of kubernetes resource API verbs like get list watch create update delete proxy   means all APIGroups is the name of the APIGroup that contains the resources  If multiple API groups are specified any action requested against one of the enumerated resources in any API group will be allowed   means all Resources is a list of resources this rule applies to   means all in the specified apiGroups foo represents the subresource foo for all resources in the specified apiGroups ResourceNames is an optional white list of names that the rule applies to  An empty set means that everything is allowed   means all 

Resource is one of the existing resource types   means all 
Subresource is one of the existing resource types   means none 

Verb is a kubernetes resource API verb like get list watch create update delete proxy   means all 
Verb is a list of kubernetes resource API verbs like get list watch create update delete proxy   means all 

ResourceAttributes includes the authorization attributes available for resource requests to the Authorizer interface 
NonResourceAttributes includes the authorization attributes available for nonresource requests to the Authorizer interface 

Status is filled in by the server and indicates whether the request is allowed or not ResourceAttributes includes the authorization attributes available for resource requests to the Authorizer interface 
Spec holds information about the request being evaluated Status is filled in by the server and indicates the set of actions a user can perform SelfSubjectRulesReviewSpec defines the specification for SelfSubjectRulesReview Namespace to evaluate rules for Required 

Status is filled in by the server and indicates whether the request is allowed or not ResourceAttributes includes the authorization attributes available for resource requests to the Authorizer interface 
SubjectAccessReview and LocalAccessReview are the correct way to defer authorization decisions to the API server Spec holds information about the request being evaluated Status is filled in by the server and indicates the set of actions a user can perform SelfSubjectRulesReviewSpec defines the specification for SelfSubjectRulesReview 

you made the request against  If empty it is defaulted Status is filled in by the server and indicates whether the request is allowed or not ResourceAttributes includes the authorization attributes available for resource requests to the Authorizer interface Namespace is the namespace of the action being requested  Currently there is no distinction between no namespace and all namespaces 
the set of authorizers the server is configured with and any errors experienced during evaluation Because authorization rules are additive if a rule appears in a list its safe to assume the subject has that permission even if that list is incomplete ResourceRules is the list of actions the subject is allowed to perform on resources 

Spec holds information about the request being evaluated  specnamespace must be equal to the namespace you made the request against  If empty it is defaulted Status is filled in by the server and indicates whether the request is allowed or not ResourceAttributes includes the authorization attributes available for resource requests to the Authorizer interface Namespace is the namespace of the action being requested  Currently there is no distinction between no namespace and all namespaces  empty is defaulted for LocalSubjectAccessReviews 
Because authorization rules are additive if a rule appears in a list its safe to assume the subject has that permission even if that list is incomplete ResourceRules is the list of actions the subject is allowed to perform on resources The list ordering isnt significant may contain duplicates and possibly be incomplete NonResourceRules is the list of actions the subject is allowed to perform on nonresources 

Spec holds information about the request being evaluated  specnamespace must be equal to the namespace you made the request against  If empty it is defaulted Status is filled in by the server and indicates whether the request is allowed or not ResourceAttributes includes the authorization attributes available for resource requests to the Authorizer interface Namespace is the namespace of the action being requested  Currently there is no distinction between no namespace and all namespaces 
APIGroups is the name of the APIGroup that contains the resources  If multiple API groups are specified any action requested against one of the enumerated resources in any API group will be allowed   means all Resources is a list of resources this rule applies to   means all in the specified apiGroups foo represents the subresource foo for all resources in the specified apiGroups ResourceNames is an optional white list of names that the rule applies to  An empty set means that everything is allowed   means all 

Spec holds information about the request being evaluated  specnamespace must be equal to the namespace you made the request against  If empty it is defaulted Status is filled in by the server and indicates whether the request is allowed or not ResourceAttributes includes the authorization attributes available for resource requests to the Authorizer interface Namespace is the namespace of the action being requested  Currently there is no distinction between no namespace and all namespaces 
the set of authorizers the server is configured with and any errors experienced during evaluation Because authorization rules are additive if a rule appears in a list its safe to assume the subject has that permission even if that list is incomplete 

Spec holds information about the request being evaluated  specnamespace must be equal to the namespace you made the request against  If empty it is defaulted Status is filled in by the server and indicates whether the request is allowed or not ResourceAttributes includes the authorization attributes available for resource requests to the Authorizer interface 
the set of authorizers the server is configured with and any errors experienced during evaluation Because authorization rules are additive if a rule appears in a list its safe to assume the subject has that permission even if that list is incomplete ResourceRules is the list of actions the subject is allowed to perform on resources 

Having a namespace scoped resource makes it much easier to grant namespace scoped policy that includes permissions checking Spec holds information about the request being evaluated  specnamespace must be equal to the namespace you made the request against  If empty it is defaulted Status is filled in by the server and indicates whether the request is allowed or not ResourceAttributes includes the authorization attributes available for resource requests to the Authorizer interface 
or to quickly let an end user reason about their permissions It should NOT Be used by external systems to drive authorization decisions as this raises confused deputy cache lifetimerevocation and correctness concerns SubjectAccessReview and LocalAccessReview are the correct way to defer authorization decisions to the API server Spec holds information about the request being evaluated Status is filled in by the server and indicates the set of actions a user can perform 

Having a namespace scoped resource makes it much easier to grant namespace scoped policy that includes permissions checking Spec holds information about the request being evaluated  specnamespace must be equal to the namespace you made the request against  If empty it is defaulted Status is filled in by the server and indicates whether the request is allowed or not ResourceAttributes includes the authorization attributes available for resource requests to the Authorizer interface 
It is entirely possible to get an error and be able to continue determine authorization status in spite of it For instance RBAC can be missing a role but enough roles are still present and bound to reason about the request k8sdeepcopygeninterfacesk8sioapimachinerypkgruntimeObject SelfSubjectRulesReview enumerates the set of actions the current user can perform within a namespace The returned list of actions may be incomplete depending on the servers authorization mode and any errors experienced during the evaluation SelfSubjectRulesReview should be used by UIs to showhide actions 

Having a namespace scoped resource makes it much easier to grant namespace scoped policy that includes permissions checking Spec holds information about the request being evaluated  specnamespace must be equal to the namespace you made the request against  If empty it is defaulted Status is filled in by the server and indicates whether the request is allowed or not ResourceAttributes includes the authorization attributes available for resource requests to the Authorizer interface 
EvaluationError is an indication that some error occurred during the authorization check It is entirely possible to get an error and be able to continue determine authorization status in spite of it For instance RBAC can be missing a role but enough roles are still present and bound to reason about the request k8sdeepcopygeninterfacesk8sioapimachinerypkgruntimeObject SelfSubjectRulesReview enumerates the set of actions the current user can perform within a namespace The returned list of actions may be incomplete depending on the servers authorization mode 

Having a namespace scoped resource makes it much easier to grant namespace scoped policy that includes permissions checking Spec holds information about the request being evaluated  specnamespace must be equal to the namespace you made the request against  If empty it is defaulted Status is filled in by the server and indicates whether the request is allowed or not 
the set of authorizers the server is configured with and any errors experienced during evaluation Because authorization rules are additive if a rule appears in a list its safe to assume the subject has that permission even if that list is incomplete 

Having a namespace scoped resource makes it much easier to grant namespace scoped policy that includes permissions checking Spec holds information about the request being evaluated  specnamespace must be equal to the namespace you made the request against  If empty it is defaulted Status is filled in by the server and indicates whether the request is allowed or not 
Reason is optional  It indicates why a request was allowed or denied EvaluationError is an indication that some error occurred during the authorization check It is entirely possible to get an error and be able to continue determine authorization status in spite of it For instance RBAC can be missing a role but enough roles are still present and bound to reason about the request k8sdeepcopygeninterfacesk8sioapimachinerypkgruntimeObject SelfSubjectRulesReview enumerates the set of actions the current user can perform within a namespace 

Having a namespace scoped resource makes it much easier to grant namespace scoped policy that includes permissions checking Spec holds information about the request being evaluated  specnamespace must be equal to the namespace you made the request against  If empty it is defaulted Status is filled in by the server and indicates whether the request is allowed or not 
If you specify User but not Group then is it interpreted as What if User were not a member of any groups Groups is the groups youre testing for Extra corresponds to the userInfoGetExtra method from the authenticator  Since that is input to the authorizer it needs a reflection here UID information about the requesting user 

Having a namespace scoped resource makes it much easier to grant namespace scoped policy that includes permissions checking Spec holds information about the request being evaluated  specnamespace must be equal to the namespace you made the request against  If empty it is defaulted 
If you specify User but not Group then is it interpreted as What if User were not a member of any groups Groups is the groups youre testing for Extra corresponds to the userInfoGetExtra method from the authenticator  Since that is input to the authorizer it needs a reflection here UID information about the requesting user ExtraValue masks the value so protobuf can generate 

LocalSubjectAccessReview checks whether or not a user or group can perform an action in a given namespace Having a namespace scoped resource makes it much easier to grant namespace scoped policy that includes permissions checking Spec holds information about the request being evaluated  specnamespace must be equal to the namespace you made the request against  If empty it is defaulted 
If you specify User but not Group then is it interpreted as What if User were not a member of any groups Groups is the groups youre testing for Extra corresponds to the userInfoGetExtra method from the authenticator  Since that is input to the authorizer it needs a reflection here UID information about the requesting user 

LocalSubjectAccessReview checks whether or not a user or group can perform an action in a given namespace 
SelfSubjectAccessReview checks whether or the current user can perform an action  Not filling in a 

LocalSubjectAccessReview checks whether or not a user or group can perform an action in a given namespace 
SubjectAccessReview checks whether or not a user or group can perform an action  Not filling in a 

Status is filled in by the server and indicates whether the request is allowed or not LocalSubjectAccessReview checks whether or not a user or group can perform an action in a given namespace Having a namespace scoped resource makes it much easier to grant namespace scoped policy that includes permissions checking Spec holds information about the request being evaluated  specnamespace must be equal to the namespace 
Reason is optional  It indicates why a request was allowed or denied EvaluationError is an indication that some error occurred during the authorization check It is entirely possible to get an error and be able to continue determine authorization status in spite of it For instance RBAC can be missing a role but enough roles are still present and bound to reason about the request k8sdeepcopygeninterfacesk8sioapimachinerypkgruntimeObject SelfSubjectRulesReview enumerates the set of actions the current user can perform within a namespace 

specnamespace means in all namespaces  Self is a special case because users should always be able to check whether they can perform an action Spec holds information about the request being evaluated Status is filled in by the server and indicates whether the request is allowed or not 
Reason is optional  It indicates why a request was allowed or denied EvaluationError is an indication that some error occurred during the authorization check It is entirely possible to get an error and be able to continue determine authorization status in spite of it For instance RBAC can be missing a role but enough roles are still present and bound to reason about the request k8sdeepcopygeninterfacesk8sioapimachinerypkgruntimeObject SelfSubjectRulesReview enumerates the set of actions the current user can perform within a namespace 

SelfSubjectAccessReview checks whether or the current user can perform an action  Not filling in a 
LocalSubjectAccessReview checks whether or not a user or group can perform an action in a given namespace 

SelfSubjectAccessReview checks whether or the current user can perform an action  Not filling in a 
SubjectAccessReview checks whether or not a user or group can perform an action  Not filling in a 

Status is filled in by the server and indicates whether the request is allowed or not SelfSubjectAccessReview checks whether or the current user can perform an action  Not filling in a specnamespace means in all namespaces  Self is a special case because users should always be able to check whether they can perform an action Spec holds information about the request being evaluated 
Reason is optional  It indicates why a request was allowed or denied EvaluationError is an indication that some error occurred during the authorization check It is entirely possible to get an error and be able to continue determine authorization status in spite of it For instance RBAC can be missing a role but enough roles are still present and bound to reason about the request k8sdeepcopygeninterfacesk8sioapimachinerypkgruntimeObject SelfSubjectRulesReview enumerates the set of actions the current user can perform within a namespace 

Spec holds information about the request being evaluated Status is filled in by the server and indicates whether the request is allowed or not SelfSubjectAccessReview checks whether or the current user can perform an action  Not filling in a specnamespace means in all namespaces  Self is a special case because users should always be able to check whether they can perform an action 
Reason is optional  It indicates why a request was allowed or denied EvaluationError is an indication that some error occurred during the authorization check It is entirely possible to get an error and be able to continue determine authorization status in spite of it For instance RBAC can be missing a role but enough roles are still present and bound to reason about the request k8sdeepcopygeninterfacesk8sioapimachinerypkgruntimeObject SelfSubjectRulesReview enumerates the set of actions the current user can perform within a namespace 

SubjectAccessReview checks whether or not a user or group can perform an action  Not filling in a 
LocalSubjectAccessReview checks whether or not a user or group can perform an action in a given namespace 

SubjectAccessReview checks whether or not a user or group can perform an action  Not filling in a 
SelfSubjectAccessReview checks whether or the current user can perform an action  Not filling in a 

reasonInfo is the cached item in ReasonCache 
ReasonItem is the cached item in ReasonCache 

Get as many pod configs as we can from a directory Return an error if and only if something 
Get as many pod manifests as we can from a directory Return an error if and only if something 

Returns usage information about the filesystem holding Docker images 
Returns usage information about the filesystem holding container images 

A resource policy cannot match a nonresource request 
A nonresource policy cannot match a resource request 

A nonresource policy cannot match a resource request 
A resource policy cannot match a nonresource request 

If the policy specified a group ensure it matches 
If the policy specified a user ensure it matches 

If the policy specified a user ensure it matches 
If the policy specified a group ensure it matches 

File format is one map per line  This allows easy concatentation of files 
File format is one map per line  This allows easy concatenation of files 

Policy authorizes Kubernetes API actions using an Attributebased access 
Package abac authorizes Kubernetes API actions using an Attributebased access control scheme 

formatMap formats mapstringstring to a string 
FormatMap formats mapstringstring to a string 

AddLabelToSelector returns a selector with the given key and value added to the given selectors MatchLabels 
Clones the given selector and returns a new selector with the given key and value added 

Clones the given selector and returns a new selector with the given key and value added 
AddLabelToSelector returns a selector with the given key and value added to the given selectors MatchLabels 

AddLabel returns a map with the given key and value added to the given map 
Clones the given map and returns a new map with the given key and value added 

CloneAndRemoveLabel clones the given map and returns a new map with the given key removed 
Clones the given map and returns a new map with the given key and value added 

Clones the given map and returns a new map with the given key and value added 
AddLabel returns a map with the given key and value added to the given map 

Clones the given map and returns a new map with the given key and value added 
CloneAndRemoveLabel clones the given map and returns a new map with the given key removed 

SyncAction indicates different kind of actions in SyncPod and KillPod Now there are only actions about startkill container and setupteardown network SyncResult is the result of sync action The associated action of the result 
Fail fails the SyncResult with specific error and message PodSyncResult is the summary result of SyncPod and KillPod Result of different sync actions Error encountered in SyncPod and KillPod that is not already included in SyncResults 

check if the quota controller can evaluate this kind if not ignore it altogether 
check if the quota controller can evaluate this groupResource if not ignore it altogether 

replenishQuota is a replenishment function invoked by a controller to notify that a quota should be recalculated check if the quota controller can evaluate this kind if not ignore it altogether check if this namespace even has a quota only queue those quotas that are tracking a resource associated with this kind 
do initial quota monitor setup  If we have a discovery failure here its ok Well discover more resources when a later sync happens only start quota once all informers synced enqueueAll is called at the fullResyncPeriod interval to force a full recalculation of quota usage statistics obj could be an v1ResourceQuota or a DeletionFinalStateUnknown marker item if we declared an intent that is not yet captured in status prioritize it if we declared a constraint that has no usage which this controller can calculate prioritize it 

there was a change observed by this controller that requires we update quota replenishQuota is a replenishment function invoked by a controller to notify that a quota should be recalculated check if the quota controller can evaluate this kind if not ignore it altogether check if this namespace even has a quota only queue those quotas that are tracking a resource associated with this kind TODO make this support targeted replenishment to a specific kind right now it does a full recalc on that quota 
do initial quota monitor setup  If we have a discovery failure here its ok Well discover more resources when a later sync happens only start quota once all informers synced enqueueAll is called at the fullResyncPeriod interval to force a full recalculation of quota usage statistics obj could be an v1ResourceQuota or a DeletionFinalStateUnknown marker item 

there was a change observed by this controller that requires we update quota replenishQuota is a replenishment function invoked by a controller to notify that a quota should be recalculated check if the quota controller can evaluate this kind if not ignore it altogether check if this namespace even has a quota only queue those quotas that are tracking a resource associated with this kind TODO make this support targeted replenishment to a specific kind right now it does a full recalc on that quota 
Note that deleting a controller immediately after scaling it to 0 will not work The recommended way of achieving this is by performing a stop operation on the controller do initial quota monitor setup  If we have a discovery failure here its ok Well discover more resources when a later sync happens only start quota once all informers synced 

if the new usage is different than the last usage we will need to do an update there was a change observed by this controller that requires we update quota replenishQuota is a replenishment function invoked by a controller to notify that a quota should be recalculated check if the quota controller can evaluate this kind if not ignore it altogether check if this namespace even has a quota only queue those quotas that are tracking a resource associated with this kind 
if so we send a new usage with latest status if this is our first sync it will be dirty by default since we need track usage if err is nonnil remember it to return but continue updating status with any resources in newUsage ensure set of used values match those that have hard constraints Create a usage object that is based on the quota resource version that will handle updates 

if the new usage is different than the last usage we will need to do an update there was a change observed by this controller that requires we update quota replenishQuota is a replenishment function invoked by a controller to notify that a quota should be recalculated check if the quota controller can evaluate this kind if not ignore it altogether check if this namespace even has a quota only queue those quotas that are tracking a resource associated with this kind 
do initial quota monitor setup  If we have a discovery failure here its ok Well discover more resources when a later sync happens only start quota once all informers synced enqueueAll is called at the fullResyncPeriod interval to force a full recalculation of quota usage statistics obj could be an v1ResourceQuota or a DeletionFinalStateUnknown marker item 

if the new usage is different than the last usage we will need to do an update there was a change observed by this controller that requires we update quota replenishQuota is a replenishment function invoked by a controller to notify that a quota should be recalculated check if the quota controller can evaluate this kind if not ignore it altogether 
enqueueAll is called at the fullResyncPeriod interval to force a full recalculation of quota usage statistics obj could be an v1ResourceQuota or a DeletionFinalStateUnknown marker item if we declared an intent that is not yet captured in status prioritize it if we declared a constraint that has no usage which this controller can calculate prioritize it 

if the new usage is different than the last usage we will need to do an update there was a change observed by this controller that requires we update quota replenishQuota is a replenishment function invoked by a controller to notify that a quota should be recalculated check if the quota controller can evaluate this kind if not ignore it altogether 
do initial quota monitor setup  If we have a discovery failure here its ok Well discover more resources when a later sync happens only start quota once all informers synced enqueueAll is called at the fullResyncPeriod interval to force a full recalculation of quota usage statistics obj could be an v1ResourceQuota or a DeletionFinalStateUnknown marker item if we declared an intent that is not yet captured in status prioritize it 

if the new usage is different than the last usage we will need to do an update there was a change observed by this controller that requires we update quota replenishQuota is a replenishment function invoked by a controller to notify that a quota should be recalculated 
way of achieving this is by performing a stop operation on the controller do initial quota monitor setup  If we have a discovery failure here its ok Well discover more resources when a later sync happens only start quota once all informers synced enqueueAll is called at the fullResyncPeriod interval to force a full recalculation of quota usage statistics obj could be an v1ResourceQuota or a DeletionFinalStateUnknown marker item if we declared an intent that is not yet captured in status prioritize it 

merge our observed usage with the quota usage status if the new usage is different than the last usage we will need to do an update there was a change observed by this controller that requires we update quota replenishQuota is a replenishment function invoked by a controller to notify that a quota should be recalculated check if the quota controller can evaluate this kind if not ignore it altogether 
if so we send a new usage with latest status if this is our first sync it will be dirty by default since we need track usage if err is nonnil remember it to return but continue updating status with any resources in newUsage ensure set of used values match those that have hard constraints Create a usage object that is based on the quota resource version that will handle updates 

merge our observed usage with the quota usage status if the new usage is different than the last usage we will need to do an update there was a change observed by this controller that requires we update quota replenishQuota is a replenishment function invoked by a controller to notify that a quota should be recalculated check if the quota controller can evaluate this kind if not ignore it altogether 
do initial quota monitor setup  If we have a discovery failure here its ok Well discover more resources when a later sync happens only start quota once all informers synced enqueueAll is called at the fullResyncPeriod interval to force a full recalculation of quota usage statistics obj could be an v1ResourceQuota or a DeletionFinalStateUnknown marker item 

merge our observed usage with the quota usage status if the new usage is different than the last usage we will need to do an update there was a change observed by this controller that requires we update quota replenishQuota is a replenishment function invoked by a controller to notify that a quota should be recalculated 
way of achieving this is by performing a stop operation on the controller do initial quota monitor setup  If we have a discovery failure here its ok Well discover more resources when a later sync happens only start quota once all informers synced enqueueAll is called at the fullResyncPeriod interval to force a full recalculation of quota usage statistics obj could be an v1ResourceQuota or a DeletionFinalStateUnknown marker item 

merge our observed usage with the quota usage status if the new usage is different than the last usage we will need to do an update there was a change observed by this controller that requires we update quota 
if so we send a new usage with latest status if this is our first sync it will be dirty by default since we need track usage if err is nonnil remember it to return but continue updating status with any resources in newUsage ensure set of used values match those that have hard constraints Create a usage object that is based on the quota resource version that will handle updates by default we preserve the past usage observation and set hard to the current spec 

mask the observed usage to only the set of resources tracked by this quota merge our observed usage with the quota usage status if the new usage is different than the last usage we will need to do an update there was a change observed by this controller that requires we update quota replenishQuota is a replenishment function invoked by a controller to notify that a quota should be recalculated 
Note that deleting a controller immediately after scaling it to 0 will not work The recommended way of achieving this is by performing a stop operation on the controller do initial quota monitor setup  If we have a discovery failure here its ok Well discover more resources when a later sync happens only start quota once all informers synced enqueueAll is called at the fullResyncPeriod interval to force a full recalculation of quota usage statistics 

mask the observed usage to only the set of resources tracked by this quota merge our observed usage with the quota usage status if the new usage is different than the last usage we will need to do an update there was a change observed by this controller that requires we update quota replenishQuota is a replenishment function invoked by a controller to notify that a quota should be recalculated 
that cannot be backed by a cache and result in a full query of a namespaces content we do not want to pay the price on spurious status updates  As a result we have a separate routine that is responsible for enqueue of all resource quotas when doing a full resync enqueueAll This will enter the sync loop and noop because the controller has been deleted from the store Note that deleting a controller immediately after scaling it to 0 will not work The recommended 

mask the observed usage to only the set of resources tracked by this quota merge our observed usage with the quota usage status if the new usage is different than the last usage we will need to do an update there was a change observed by this controller that requires we update quota 
if err is nonnil remember it to return but continue updating status with any resources in newUsage ensure set of used values match those that have hard constraints Create a usage object that is based on the quota resource version that will handle updates by default we preserve the past usage observation and set hard to the current spec 

and the resources this controller can track to know what we can look to measure updated usage stats for sum the observed usage from each evaluator mask the observed usage to only the set of resources tracked by this quota 
function that controls full recalculation of quota usage knows how to calculate usage knows how to monitor all the resources tracked by quota and trigger replenishment controls the workers that process quotas this lock is acquired to control write access to the monitors and ensures that all monitors are synced before the controller can process quotas 

syncResourceQuota runs a complete sync of resource quota status across all known kinds quota is dirty if any part of spec hard limits differs from the status hard limits dirty tracks if the usage status differs from the previous sync if so we send a new usage with latest status if this is our first sync it will be dirty by default since we need track usage Create a usage object that is based on the quota resource version that will handle updates 
check if the quota controller can evaluate this groupResource if not ignore it altogether check if this namespace even has a quota only queue those quotas that are tracking a resource associated with this kind TODO make this support targeted replenishment to a specific kind right now it does a full recalc on that quota Sync periodically resyncs the controller when new resources are observed from discovery Something has changed so track the new state and perform a sync 

worker runs a worker thread that just dequeues items processes them and marks them done It enforces that the syncHandler is never invoked concurrently with the same key Run begins quota controller using the specified number of workers the controllers that replenish other resources to respond rapidly to state changes 
printDiff returns a humanreadable summary of what resources were added and removed waitForStopOrTimeout returns a stop channel that closes when the provided stop channel closes or when the specified timeout is reached resyncMonitors starts or stops quota monitors as needed to ensure that all and only those resources present in the map are monitored GetQuotableResources returns all resources that the quota system should recognize It requires a resource supports the following verbs createlistdelete 

obj could be an apiResourceQuota or a DeletionFinalStateUnknown marker item worker runs a worker thread that just dequeues items processes them and marks them done It enforces that the syncHandler is never invoked concurrently with the same key Run begins quota controller using the specified number of workers the controllers that replenish other resources to respond rapidly to state changes the workers that chug through the quota calculation backlog 
waitForStopOrTimeout returns a stop channel that closes when the provided stop channel closes or when the specified timeout is reached resyncMonitors starts or stops quota monitors as needed to ensure that all and only those resources present in the map are monitored GetQuotableResources returns all resources that the quota system should recognize It requires a resource supports the following verbs createlistdelete 

obj could be an apiResourceQuota or a DeletionFinalStateUnknown marker item worker runs a worker thread that just dequeues items processes them and marks them done It enforces that the syncHandler is never invoked concurrently with the same key Run begins quota controller using the specified number of workers the controllers that replenish other resources to respond rapidly to state changes 
and only those resources present in the map are monitored GetQuotableResources returns all resources that the quota system should recognize It requires a resource supports the following verbs createlistdelete This function may return both results and an error  If that happens it means that the discovery calls were only partially successful  A decision about whether to proceed or not is left to the caller 

obj could be an apiResourceQuota or a DeletionFinalStateUnknown marker item worker runs a worker thread that just dequeues items processes them and marks them done It enforces that the syncHandler is never invoked concurrently with the same key Run begins quota controller using the specified number of workers the controllers that replenish other resources to respond rapidly to state changes 
waitForStopOrTimeout returns a stop channel that closes when the provided stop channel closes or when the specified timeout is reached resyncMonitors starts or stops quota monitors as needed to ensure that all and only those resources present in the map are monitored GetQuotableResources returns all resources that the quota system should recognize It requires a resource supports the following verbs createlistdelete This function may return both results and an error  If that happens it means that the discovery calls were only 

obj could be an apiResourceQuota or a DeletionFinalStateUnknown marker item 
obj could be an v1ResourceQuota or a DeletionFinalStateUnknown marker item 

enqueueAll is called at the fullResyncPeriod interval to force a full recalculation of quota usage statistics obj could be an apiResourceQuota or a DeletionFinalStateUnknown marker item worker runs a worker thread that just dequeues items processes them and marks them done It enforces that the syncHandler is never invoked concurrently with the same key Run begins quota controller using the specified number of workers the controllers that replenish other resources to respond rapidly to state changes 
waitForStopOrTimeout returns a stop channel that closes when the provided stop channel closes or when the specified timeout is reached resyncMonitors starts or stops quota monitors as needed to ensure that all and only those resources present in the map are monitored GetQuotableResources returns all resources that the quota system should recognize It requires a resource supports the following verbs createlistdelete 

enqueueAll is called at the fullResyncPeriod interval to force a full recalculation of quota usage statistics obj could be an apiResourceQuota or a DeletionFinalStateUnknown marker item worker runs a worker thread that just dequeues items processes them and marks them done It enforces that the syncHandler is never invoked concurrently with the same key Run begins quota controller using the specified number of workers the controllers that replenish other resources to respond rapidly to state changes 
want to pay the price on spurious status updates  As a result we have a separate routine that is responsible for enqueue of all resource quotas when doing a full resync enqueueAll This will enter the sync loop and noop because the controller has been deleted from the store Note that deleting a controller immediately after scaling it to 0 will not work The recommended way of achieving this is by performing a stop operation on the controller 

want to pay the price on spurious status updates  As a result we have a separate routine that is responsible for enqueue of all resource quotas when doing a full resync enqueueAll This will enter the sync loop and noop because the controller has been deleted from the store Note that deleting a controller immediately after scaling it to 0 will not work The recommended way of achieving this is by performing a stop operation on the controller 
the call to resyncMonitors on the reattempt will noop for resources that still exist success remember newly synced resources printDiff returns a humanreadable summary of what resources were added and removed waitForStopOrTimeout returns a stop channel that closes when the provided stop channel closes or when the specified timeout is reached 

want to pay the price on spurious status updates  As a result we have a separate routine that is responsible for enqueue of all resource quotas when doing a full resync enqueueAll This will enter the sync loop and noop because the controller has been deleted from the store Note that deleting a controller immediately after scaling it to 0 will not work The recommended way of achieving this is by performing a stop operation on the controller 
ensure set of used values match those that have hard constraints Create a usage object that is based on the quota resource version that will handle updates by default we preserve the past usage observation and set hard to the current spec there was a change observed by this controller that requires we update quota replenishQuota is a replenishment function invoked by a controller to notify that a quota should be recalculated 

want to pay the price on spurious status updates  As a result we have a separate routine that is responsible for enqueue of all resource quotas when doing a full resync enqueueAll This will enter the sync loop and noop because the controller has been deleted from the store Note that deleting a controller immediately after scaling it to 0 will not work The recommended 
TODO make this support targeted replenishment to a specific kind right now it does a full recalc on that quota Sync periodically resyncs the controller when new resources are observed from discovery 

want to pay the price on spurious status updates  As a result we have a separate routine that is responsible for enqueue of all resource quotas when doing a full resync enqueueAll This will enter the sync loop and noop because the controller has been deleted from the store Note that deleting a controller immediately after scaling it to 0 will not work The recommended 
if err is nonnil remember it to return but continue updating status with any resources in newUsage ensure set of used values match those that have hard constraints Create a usage object that is based on the quota resource version that will handle updates by default we preserve the past usage observation and set hard to the current spec there was a change observed by this controller that requires we update quota replenishQuota is a replenishment function invoked by a controller to notify that a quota should be recalculated 

want to pay the price on spurious status updates  As a result we have a separate routine that is responsible for enqueue of all resource quotas when doing a full resync enqueueAll This will enter the sync loop and noop because the controller has been deleted from the store 
TODO make this support targeted replenishment to a specific kind right now it does a full recalc on that quota Sync periodically resyncs the controller when new resources are observed from discovery Something has changed so track the new state and perform a sync Get the current resource list from discovery 

want to pay the price on spurious status updates  As a result we have a separate routine that is responsible for enqueue of all resource quotas when doing a full resync enqueueAll This will enter the sync loop and noop because the controller has been deleted from the store 
way of achieving this is by performing a stop operation on the controller do initial quota monitor setup  If we have a discovery failure here its ok Well discover more resources when a later sync happens only start quota once all informers synced enqueueAll is called at the fullResyncPeriod interval to force a full recalculation of quota usage statistics 

that cannot be backed by a cache and result in a full query of a namespaces content we do not want to pay the price on spurious status updates  As a result we have a separate routine that is responsible for enqueue of all resource quotas when doing a full resync enqueueAll This will enter the sync loop and noop because the controller has been deleted from the store Note that deleting a controller immediately after scaling it to 0 will not work The recommended way of achieving this is by performing a stop operation on the controller 
Create a usage object that is based on the quota resource version that will handle updates by default we preserve the past usage observation and set hard to the current spec there was a change observed by this controller that requires we update quota replenishQuota is a replenishment function invoked by a controller to notify that a quota should be recalculated 

that cannot be backed by a cache and result in a full query of a namespaces content we do not want to pay the price on spurious status updates  As a result we have a separate routine that is responsible for enqueue of all resource quotas when doing a full resync enqueueAll This will enter the sync loop and noop because the controller has been deleted from the store Note that deleting a controller immediately after scaling it to 0 will not work The recommended 
by default we preserve the past usage observation and set hard to the current spec there was a change observed by this controller that requires we update quota replenishQuota is a replenishment function invoked by a controller to notify that a quota should be recalculated check if the quota controller can evaluate this groupResource if not ignore it altogether check if this namespace even has a quota only queue those quotas that are tracking a resource associated with this kind 

that cannot be backed by a cache and result in a full query of a namespaces content we do not want to pay the price on spurious status updates  As a result we have a separate routine that is responsible for enqueue of all resource quotas when doing a full resync enqueueAll This will enter the sync loop and noop because the controller has been deleted from the store Note that deleting a controller immediately after scaling it to 0 will not work The recommended 
Create a usage object that is based on the quota resource version that will handle updates by default we preserve the past usage observation and set hard to the current spec there was a change observed by this controller that requires we update quota replenishQuota is a replenishment function invoked by a controller to notify that a quota should be recalculated check if the quota controller can evaluate this groupResource if not ignore it altogether 

that cannot be backed by a cache and result in a full query of a namespaces content we do not want to pay the price on spurious status updates  As a result we have a separate routine that is responsible for enqueue of all resource quotas when doing a full resync enqueueAll This will enter the sync loop and noop because the controller has been deleted from the store Note that deleting a controller immediately after scaling it to 0 will not work The recommended 
if err is nonnil remember it to return but continue updating status with any resources in newUsage ensure set of used values match those that have hard constraints Create a usage object that is based on the quota resource version that will handle updates by default we preserve the past usage observation and set hard to the current spec there was a change observed by this controller that requires we update quota 

that cannot be backed by a cache and result in a full query of a namespaces content we do not want to pay the price on spurious status updates  As a result we have a separate routine that is responsible for enqueue of all resource quotas when doing a full resync enqueueAll This will enter the sync loop and noop because the controller has been deleted from the store 
check if this namespace even has a quota only queue those quotas that are tracking a resource associated with this kind TODO make this support targeted replenishment to a specific kind right now it does a full recalc on that quota Sync periodically resyncs the controller when new resources are observed from discovery Something has changed so track the new state and perform a sync 

that cannot be backed by a cache and result in a full query of a namespaces content we do not want to pay the price on spurious status updates  As a result we have a separate routine that is responsible for enqueue of all resource quotas when doing a full resync enqueueAll This will enter the sync loop and noop because the controller has been deleted from the store 
ensure set of used values match those that have hard constraints Create a usage object that is based on the quota resource version that will handle updates by default we preserve the past usage observation and set hard to the current spec there was a change observed by this controller that requires we update quota replenishQuota is a replenishment function invoked by a controller to notify that a quota should be recalculated 

that cannot be backed by a cache and result in a full query of a namespaces content we do not want to pay the price on spurious status updates  As a result we have a separate routine that is responsible for enqueue of all resource quotas when doing a full resync enqueueAll 
way of achieving this is by performing a stop operation on the controller do initial quota monitor setup  If we have a discovery failure here its ok Well discover more resources when a later sync happens only start quota once all informers synced enqueueAll is called at the fullResyncPeriod interval to force a full recalculation of quota usage statistics obj could be an v1ResourceQuota or a DeletionFinalStateUnknown marker item 

us to enqueue all quotaStatus updates and since quotaStatus updates involve additional queries that cannot be backed by a cache and result in a full query of a namespaces content we do not want to pay the price on spurious status updates  As a result we have a separate routine that is responsible for enqueue of all resource quotas when doing a full resync enqueueAll This will enter the sync loop and noop because the controller has been deleted from the store 
only queue those quotas that are tracking a resource associated with this kind TODO make this support targeted replenishment to a specific kind right now it does a full recalc on that quota Sync periodically resyncs the controller when new resources are observed from discovery Something has changed so track the new state and perform a sync 

us to enqueue all quotaStatus updates and since quotaStatus updates involve additional queries that cannot be backed by a cache and result in a full query of a namespaces content we do not want to pay the price on spurious status updates  As a result we have a separate routine that is responsible for enqueue of all resource quotas when doing a full resync enqueueAll This will enter the sync loop and noop because the controller has been deleted from the store 
check if this namespace even has a quota only queue those quotas that are tracking a resource associated with this kind TODO make this support targeted replenishment to a specific kind right now it does a full recalc on that quota Sync periodically resyncs the controller when new resources are observed from discovery 

us to enqueue all quotaStatus updates and since quotaStatus updates involve additional queries that cannot be backed by a cache and result in a full query of a namespaces content we do not want to pay the price on spurious status updates  As a result we have a separate routine that is responsible for enqueue of all resource quotas when doing a full resync enqueueAll This will enter the sync loop and noop because the controller has been deleted from the store 
if err is nonnil remember it to return but continue updating status with any resources in newUsage ensure set of used values match those that have hard constraints Create a usage object that is based on the quota resource version that will handle updates by default we preserve the past usage observation and set hard to the current spec there was a change observed by this controller that requires we update quota 

us to enqueue all quotaStatus updates and since quotaStatus updates involve additional queries that cannot be backed by a cache and result in a full query of a namespaces content we do not want to pay the price on spurious status updates  As a result we have a separate routine that is responsible for enqueue of all resource quotas when doing a full resync enqueueAll This will enter the sync loop and noop because the controller has been deleted from the store 
Note that deleting a controller immediately after scaling it to 0 will not work The recommended way of achieving this is by performing a stop operation on the controller do initial quota monitor setup  If we have a discovery failure here its ok Well discover more resources when a later sync happens 

us to enqueue all quotaStatus updates and since quotaStatus updates involve additional queries that cannot be backed by a cache and result in a full query of a namespaces content we do not want to pay the price on spurious status updates  As a result we have a separate routine that is responsible for enqueue of all resource quotas when doing a full resync enqueueAll 
Note that deleting a controller immediately after scaling it to 0 will not work The recommended way of achieving this is by performing a stop operation on the controller do initial quota monitor setup  If we have a discovery failure here its ok Well discover more resources when a later sync happens only start quota once all informers synced enqueueAll is called at the fullResyncPeriod interval to force a full recalculation of quota usage statistics 

function that controls full recalculation of quota usage knows how to calculate usage 
Controls full recalculation of quota usage Maintains evaluators that know how to calculate usage for group resource 

Controls full resync of objects monitored for replenihsment 
Controls full resync of objects monitored for replenishment 

Knows how to calculate usage 
knows how to calculate usage 

Controls full recalculation of quota usage Knows how to calculate usage Knows how to build controllers that notify replenishment events Controls full resync of objects monitored for replenihsment List of GroupKind objects that should be monitored for replenishment at a faster frequency than the quota controller recalculation interval 
ResourceQuota objects that need to be synchronized missingUsageQueue holds objects that are missing the initial usage information To allow injection of syncUsage for testing function that controls full recalculation of quota usage 

Package nfs contains the internal representation of Ceph file system 
Package cephfs contains the internal representation of Ceph file system 

parseImageName parses a docker image string into two parts repo and tag 
ParseImageName parses a docker image string into three parts repo tag and digest 

Wrap EmptyDir let it do the teardown 
Wrap EmptyDir let it do the setup 

TearDownAt simply deletes everything in the directory 
TearDown simply deletes everything in the directory 

TearDown simply deletes everything in the directory 
TearDownAt simply deletes everything in the directory 

gitRepoVolumeCleaner cleans git repo volumes 
gitRepoVolumeUnmounter cleans git repo volumes 

SetUpAt creates new directory and clones a git repo 
SetUp creates new directory and clones a git repo 

SetUp creates new directory and clones a git repo 
SetUpAt creates new directory and clones a git repo 

gitRepoVolumeBuilder builds git repo volumes 
gitRepoVolumeMounter builds git repo volumes 

Generate the token 
Generate the ECDSA token 

Call OnEndpointsUpdate before NewService 
Call NewService before OnEndpointsUpdate 

Call OnEndpointsUpdate before NewService 
Call NewService before OnEndpointsUpdate 

Call NewService before OnEndpointsUpdate 
Call OnEndpointsUpdate before NewService 

OnEndpointsUpdate can be called without NewService being called externally 
OnEndpointsAdd can be called without NewService being called externally 

build cgo linux 
build linux 

HttpError wraps a nonStatusOK error code as an error 
HTTPError wraps a nonStatusOK error code as an error 

DockerConfigJson represents dockerconfigjson file info 
DockerConfigJSON represents dockerconfigjson file info 

Specified a policy for garbage collecting containers 
GCPolicy specifies a policy for garbage collecting containers 

this is an update 
this is not an update 

Now needUpdate and needReconcile should never be both true 
Now needUpdate needGracefulDelete and needReconcile should never be both true 

this is a delete 
this is a noop 

this is a noop 
this is a delete 

this is a noop 
this is a delete 

this is a delete 
this is a noop 

no ADD or UPDATE pods from the source This signals kubelet that 
no ADD or UPDATE or DELETE pods from the source This signals kubelet that 

TODO allow initialization of the current state of the store with snapshotted version Merge normalizes a set of incoming changes from different sources into a map of all Pods and ensures that redundant changes are filtered out and then pushes zero or more minimal updates onto the update channel  Ensures that updates are delivered in order 
PodConfig is a configuration mux that merges many sources of pod configuration into a single consistent structure and then delivers incremental change notifications to listeners in order the channel of denormalized changes passed to listeners contains the list of all configured sources 

TODO PodConfigNotificationMode could be handled by a listener to the updates channel in the future especially with multiple listeners TODO allow initialization of the current state of the store with snapshotted version Merge normalizes a set of incoming changes from different sources into a map of all Pods and ensures that redundant changes are filtered out and then pushes zero or more minimal updates onto the update channel  Ensures that updates are delivered in order 
config and also this config has received a SET message from each source Updates returns a channel of updates to the configuration properly denormalized Sync requests the full configuration be delivered to the update channel podStorage manages the current pod state at any point in time and ensures updates to the channel are delivered in order  Note that this object is an inmemory source of truth and on creation contains zero entries  Once all previously read sources are 

TODO PodConfigNotificationMode could be handled by a listener to the updates channel in the future especially with multiple listeners TODO allow initialization of the current state of the store with snapshotted version Merge normalizes a set of incoming changes from different sources into a map of all Pods 
PodConfig is a configuration mux that merges many sources of pod configuration into a single consistent structure and then delivers incremental change notifications to listeners in order the channel of denormalized changes passed to listeners contains the list of all configured sources 

the EventRecorder to use TODO PodConfigNotificationMode could be handled by a listener to the updates channel in the future especially with multiple listeners TODO allow initialization of the current state of the store with snapshotted version Merge normalizes a set of incoming changes from different sources into a map of all Pods and ensures that redundant changes are filtered out and then pushes zero or more minimal 
consistent structure and then delivers incremental change notifications to listeners in order the channel of denormalized changes passed to listeners contains the list of all configured sources NewPodConfig creates an object that can merge many configuration sources into a stream 

the EventRecorder to use TODO PodConfigNotificationMode could be handled by a listener to the updates channel in the future especially with multiple listeners TODO allow initialization of the current state of the store with snapshotted version Merge normalizes a set of incoming changes from different sources into a map of all Pods 
in order the channel of denormalized changes passed to listeners contains the list of all configured sources NewPodConfig creates an object that can merge many configuration sources into a stream of normalized updates to a pod configuration 

the EventRecorder to use TODO PodConfigNotificationMode could be handled by a listener to the updates channel in the future especially with multiple listeners TODO allow initialization of the current state of the store with snapshotted version Merge normalizes a set of incoming changes from different sources into a map of all Pods 
PodConfig is a configuration mux that merges many sources of pod configuration into a single consistent structure and then delivers incremental change notifications to listeners in order the channel of denormalized changes passed to listeners 

on the updates channel contains the set of all sources that have sent at least one SET the EventRecorder to use TODO PodConfigNotificationMode could be handled by a listener to the updates channel in the future especially with multiple listeners 
config and also this config has received a SET message from each source Updates returns a channel of updates to the configuration properly denormalized Sync requests the full configuration be delivered to the update channel podStorage manages the current pod state at any point in time and ensures updates to the channel are delivered in order  Note that this object is an inmemory source of truth and on creation contains zero entries  Once all previously read sources are 

on the updates channel contains the set of all sources that have sent at least one SET the EventRecorder to use TODO PodConfigNotificationMode could be handled by a listener to the updates channel in the future especially with multiple listeners 
SeenAllSources returns true if seenSources contains all sources in the config and also this config has received a SET message from each source Updates returns a channel of updates to the configuration properly denormalized Sync requests the full configuration be delivered to the update channel podStorage manages the current pod state at any point in time and ensures updates 

ensures that updates are delivered in strict order on the updates channel contains the set of all sources that have sent at least one SET the EventRecorder to use TODO PodConfigNotificationMode could be handled by a listener to the updates channel 
Sync requests the full configuration be delivered to the update channel podStorage manages the current pod state at any point in time and ensures updates to the channel are delivered in order  Note that this object is an inmemory source of truth and on creation contains zero entries  Once all previously read sources are 

map of source name to pod name to pod reference 
map of source name to pod uid to pod reference 

available then this object should be considered authoritative map of source name to pod name to pod reference ensures that updates are delivered in strict order on the updates channel contains the set of all sources that have sent at least one SET the EventRecorder to use 
Sync requests the full configuration be delivered to the update channel podStorage manages the current pod state at any point in time and ensures updates to the channel are delivered in order  Note that this object is an inmemory source of truth and on creation contains zero entries  Once all previously read sources are 

Sync requests the full configuration be delivered to the update channel podStorage manages the current pod state at any point in time and ensures updates to the channel are delivered in order  Note that this object is an inmemory source of truth and on creation contains zero entries  Once all previously read sources are 
ensures that updates are delivered in strict order on the updates channel contains the set of all sources that have sent at least one SET the EventRecorder to use TODO PodConfigNotificationMode could be handled by a listener to the updates channel 

Sync requests the full configuration be delivered to the update channel podStorage manages the current pod state at any point in time and ensures updates to the channel are delivered in order  Note that this object is an inmemory source of truth and on creation contains zero entries  Once all previously read sources are 
available then this object should be considered authoritative map of source name to pod uid to pod reference ensures that updates are delivered in strict order on the updates channel contains the set of all sources that have sent at least one SET 

Updates returns a channel of updates to the configuration properly denormalized Sync requests the full configuration be delivered to the update channel podStorage manages the current pod state at any point in time and ensures updates to the channel are delivered in order  Note that this object is an inmemory source of truth and on creation contains zero entries  Once all previously read sources are 
consistent structure and then delivers incremental change notifications to listeners in order the channel of denormalized changes passed to listeners contains the list of all configured sources NewPodConfig creates an object that can merge many configuration sources into a stream 

Updates returns a channel of updates to the configuration properly denormalized Sync requests the full configuration be delivered to the update channel podStorage manages the current pod state at any point in time and ensures updates to the channel are delivered in order  Note that this object is an inmemory source of 
PodConfigNotificationIncremental delivers ADD UPDATE DELETE REMOVE RECONCILE to the update channel PodConfig is a configuration mux that merges many sources of pod configuration into a single consistent structure and then delivers incremental change notifications to listeners in order the channel of denormalized changes passed to listeners 

config and also this config has received a SET message from each source Updates returns a channel of updates to the configuration properly denormalized Sync requests the full configuration be delivered to the update channel podStorage manages the current pod state at any point in time and ensures updates to the channel are delivered in order  Note that this object is an inmemory source of truth and on creation contains zero entries  Once all previously read sources are 
TODO PodConfigNotificationMode could be handled by a listener to the updates channel in the future especially with multiple listeners TODO allow initialization of the current state of the store with snapshotted version Merge normalizes a set of incoming changes from different sources into a map of all Pods and ensures that redundant changes are filtered out and then pushes zero or more minimal updates onto the update channel  Ensures that updates are delivered in order 

config and also this config has received a SET message from each source Updates returns a channel of updates to the configuration properly denormalized Sync requests the full configuration be delivered to the update channel podStorage manages the current pod state at any point in time and ensures updates to the channel are delivered in order  Note that this object is an inmemory source of truth and on creation contains zero entries  Once all previously read sources are 
on the updates channel contains the set of all sources that have sent at least one SET the EventRecorder to use TODO PodConfigNotificationMode could be handled by a listener to the updates channel in the future especially with multiple listeners 

config and also this config has received a SET message from each source Updates returns a channel of updates to the configuration properly denormalized Sync requests the full configuration be delivered to the update channel podStorage manages the current pod state at any point in time and ensures updates to the channel are delivered in order  Note that this object is an inmemory source of 
consistent structure and then delivers incremental change notifications to listeners in order the channel of denormalized changes passed to listeners contains the list of all configured sources NewPodConfig creates an object that can merge many configuration sources into a stream of normalized updates to a pod configuration 

config and also this config has received a SET message from each source Updates returns a channel of updates to the configuration properly denormalized Sync requests the full configuration be delivered to the update channel podStorage manages the current pod state at any point in time and ensures updates to the channel are delivered in order  Note that this object is an inmemory source of 
PodConfigNotificationIncremental delivers ADD UPDATE DELETE REMOVE RECONCILE to the update channel PodConfig is a configuration mux that merges many sources of pod configuration into a single consistent structure and then delivers incremental change notifications to listeners in order 

SeenAllSources returns true if seenSources contains all sources in the config and also this config has received a SET message from each source Updates returns a channel of updates to the configuration properly denormalized Sync requests the full configuration be delivered to the update channel podStorage manages the current pod state at any point in time and ensures updates to the channel are delivered in order  Note that this object is an inmemory source of 
map of source name to pod uid to pod reference ensures that updates are delivered in strict order on the updates channel contains the set of all sources that have sent at least one SET the EventRecorder to use TODO PodConfigNotificationMode could be handled by a listener to the updates channel 

SeenAllSources returns true if seenSources contains all sources in the config and also this config has received a SET message from each source Updates returns a channel of updates to the configuration properly denormalized Sync requests the full configuration be delivered to the update channel podStorage manages the current pod state at any point in time and ensures updates to the channel are delivered in order  Note that this object is an inmemory source of 
consistent structure and then delivers incremental change notifications to listeners in order the channel of denormalized changes passed to listeners contains the list of all configured sources NewPodConfig creates an object that can merge many configuration sources into a stream 

SeenAllSources returns true if seenSources contains all sources in the config and also this config has received a SET message from each source Updates returns a channel of updates to the configuration properly denormalized Sync requests the full configuration be delivered to the update channel podStorage manages the current pod state at any point in time and ensures updates 
on the updates channel contains the set of all sources that have sent at least one SET the EventRecorder to use TODO PodConfigNotificationMode could be handled by a listener to the updates channel in the future especially with multiple listeners 

SeenAllSources returns true if seenSources contains all sources in the config and also this config has received a SET message from each source Updates returns a channel of updates to the configuration properly denormalized Sync requests the full configuration be delivered to the update channel 
changed and a SET message if there are any additions or removals PodConfigNotificationIncremental delivers ADD UPDATE DELETE REMOVE RECONCILE to the update channel PodConfig is a configuration mux that merges many sources of pod configuration into a single consistent structure and then delivers incremental change notifications to listeners in order the channel of denormalized changes passed to listeners 

of normalized updates to a pod configuration Channel creates or returns a config source channel  The channel only accepts PodUpdates SeenAllSources returns true if seenSources contains all sources in the config and also this config has received a SET message from each source Updates returns a channel of updates to the configuration properly denormalized 
PodConfigNotificationIncremental delivers ADD UPDATE DELETE REMOVE RECONCILE to the update channel PodConfig is a configuration mux that merges many sources of pod configuration into a single consistent structure and then delivers incremental change notifications to listeners in order 

NewPodConfig creates an object that can merge many configuration sources into a stream of normalized updates to a pod configuration Channel creates or returns a config source channel  The channel only accepts PodUpdates SeenAllSources returns true if seenSources contains all sources in the config and also this config has received a SET message from each source 
PodConfigNotificationSnapshot delivers the full configuration as a SET whenever any change occurs PodConfigNotificationSnapshotAndUpdates delivers an UPDATE and DELETE message whenever pods are changed and a SET message if there are any additions or removals PodConfigNotificationIncremental delivers ADD UPDATE DELETE REMOVE RECONCILE to the update channel PodConfig is a configuration mux that merges many sources of pod configuration into a single 

in order the channel of denormalized changes passed to listeners contains the list of all configured sources NewPodConfig creates an object that can merge many configuration sources into a stream of normalized updates to a pod configuration 
the EventRecorder to use TODO PodConfigNotificationMode could be handled by a listener to the updates channel in the future especially with multiple listeners TODO allow initialization of the current state of the store with snapshotted version Merge normalizes a set of incoming changes from different sources into a map of all Pods 

consistent structure and then delivers incremental change notifications to listeners in order the channel of denormalized changes passed to listeners contains the list of all configured sources NewPodConfig creates an object that can merge many configuration sources into a stream of normalized updates to a pod configuration 
config and also this config has received a SET message from each source Updates returns a channel of updates to the configuration properly denormalized Sync requests the full configuration be delivered to the update channel podStorage manages the current pod state at any point in time and ensures updates to the channel are delivered in order  Note that this object is an inmemory source of 

consistent structure and then delivers incremental change notifications to listeners in order the channel of denormalized changes passed to listeners contains the list of all configured sources NewPodConfig creates an object that can merge many configuration sources into a stream 
the EventRecorder to use TODO PodConfigNotificationMode could be handled by a listener to the updates channel in the future especially with multiple listeners TODO allow initialization of the current state of the store with snapshotted version Merge normalizes a set of incoming changes from different sources into a map of all Pods and ensures that redundant changes are filtered out and then pushes zero or more minimal 

consistent structure and then delivers incremental change notifications to listeners in order the channel of denormalized changes passed to listeners contains the list of all configured sources NewPodConfig creates an object that can merge many configuration sources into a stream 
Updates returns a channel of updates to the configuration properly denormalized Sync requests the full configuration be delivered to the update channel podStorage manages the current pod state at any point in time and ensures updates to the channel are delivered in order  Note that this object is an inmemory source of truth and on creation contains zero entries  Once all previously read sources are 

consistent structure and then delivers incremental change notifications to listeners in order the channel of denormalized changes passed to listeners contains the list of all configured sources NewPodConfig creates an object that can merge many configuration sources into a stream 
SeenAllSources returns true if seenSources contains all sources in the config and also this config has received a SET message from each source Updates returns a channel of updates to the configuration properly denormalized Sync requests the full configuration be delivered to the update channel podStorage manages the current pod state at any point in time and ensures updates to the channel are delivered in order  Note that this object is an inmemory source of 

PodConfig is a configuration mux that merges many sources of pod configuration into a single consistent structure and then delivers incremental change notifications to listeners in order the channel of denormalized changes passed to listeners contains the list of all configured sources 
TODO allow initialization of the current state of the store with snapshotted version Merge normalizes a set of incoming changes from different sources into a map of all Pods and ensures that redundant changes are filtered out and then pushes zero or more minimal updates onto the update channel  Ensures that updates are delivered in order 

PodConfig is a configuration mux that merges many sources of pod configuration into a single consistent structure and then delivers incremental change notifications to listeners in order the channel of denormalized changes passed to listeners contains the list of all configured sources 
TODO PodConfigNotificationMode could be handled by a listener to the updates channel in the future especially with multiple listeners TODO allow initialization of the current state of the store with snapshotted version Merge normalizes a set of incoming changes from different sources into a map of all Pods 

PodConfig is a configuration mux that merges many sources of pod configuration into a single consistent structure and then delivers incremental change notifications to listeners in order the channel of denormalized changes passed to listeners 
the EventRecorder to use TODO PodConfigNotificationMode could be handled by a listener to the updates channel in the future especially with multiple listeners TODO allow initialization of the current state of the store with snapshotted version Merge normalizes a set of incoming changes from different sources into a map of all Pods 

PodConfigNotificationIncremental delivers ADD UPDATE REMOVE RECONCILE to the update channel PodConfig is a configuration mux that merges many sources of pod configuration into a single consistent structure and then delivers incremental change notifications to listeners in order the channel of denormalized changes passed to listeners 
Updates returns a channel of updates to the configuration properly denormalized Sync requests the full configuration be delivered to the update channel podStorage manages the current pod state at any point in time and ensures updates to the channel are delivered in order  Note that this object is an inmemory source of 

PodConfigNotificationIncremental delivers ADD UPDATE REMOVE RECONCILE to the update channel PodConfig is a configuration mux that merges many sources of pod configuration into a single consistent structure and then delivers incremental change notifications to listeners in order 
config and also this config has received a SET message from each source Updates returns a channel of updates to the configuration properly denormalized Sync requests the full configuration be delivered to the update channel podStorage manages the current pod state at any point in time and ensures updates to the channel are delivered in order  Note that this object is an inmemory source of 

PodConfigNotificationIncremental delivers ADD UPDATE REMOVE RECONCILE to the update channel 
PodConfigNotificationIncremental delivers ADD UPDATE DELETE REMOVE RECONCILE to the update channel 

changed and a SET message if there are any additions or removals PodConfigNotificationIncremental delivers ADD UPDATE REMOVE RECONCILE to the update channel PodConfig is a configuration mux that merges many sources of pod configuration into a single consistent structure and then delivers incremental change notifications to listeners in order the channel of denormalized changes passed to listeners 
SeenAllSources returns true if seenSources contains all sources in the config and also this config has received a SET message from each source Updates returns a channel of updates to the configuration properly denormalized Sync requests the full configuration be delivered to the update channel 

changed and a SET message if there are any additions or removals PodConfigNotificationIncremental delivers ADD UPDATE REMOVE RECONCILE to the update channel PodConfig is a configuration mux that merges many sources of pod configuration into a single consistent structure and then delivers incremental change notifications to listeners in order the channel of denormalized changes passed to listeners 
of normalized updates to a pod configuration Channel creates or returns a config source channel  The channel only accepts PodUpdates SeenAllSources returns true if seenSources contains all sources in the config and also this config has received a SET message from each source Updates returns a channel of updates to the configuration properly denormalized 

PodConfigNotificationSnapshotAndUpdates delivers an UPDATE message whenever pods are changed and a SET message if there are any additions or removals PodConfigNotificationIncremental delivers ADD UPDATE REMOVE RECONCILE to the update channel PodConfig is a configuration mux that merges many sources of pod configuration into a single consistent structure and then delivers incremental change notifications to listeners in order 
NewPodConfig creates an object that can merge many configuration sources into a stream of normalized updates to a pod configuration Channel creates or returns a config source channel  The channel only accepts PodUpdates SeenAllSources returns true if seenSources contains all sources in the 

PodConfigNotificationSnapshotAndUpdates delivers an UPDATE message whenever pods are 
PodConfigNotificationSnapshotAndUpdates delivers an UPDATE and DELETE message whenever pods are 

PodConfigNotificationSnapshot delivers the full configuration as a SET whenever any change occurs PodConfigNotificationSnapshotAndUpdates delivers an UPDATE message whenever pods are changed and a SET message if there are any additions or removals PodConfigNotificationIncremental delivers ADD UPDATE REMOVE RECONCILE to the update channel PodConfig is a configuration mux that merges many sources of pod configuration into a single 
NewPodConfig creates an object that can merge many configuration sources into a stream of normalized updates to a pod configuration Channel creates or returns a config source channel  The channel only accepts PodUpdates SeenAllSources returns true if seenSources contains all sources in the config and also this config has received a SET message from each source 

Contains utility code for use by volume plugins 
Package util contains utility code for use by volume plugins 

it mounts it it to globalPDPath 
it mounts it to globalPDPath 

Adds the list of known types to apiScheme 
Adds the list of known types to the given scheme 

Resource takes an unqualified resource and returns back a Group qualified GroupResource 
Resource takes an unqualified resource and returns a Group qualified GroupResource 

Kind takes an unqualified kind and returns back a Group qualified GroupKind 
Kind takes an unqualified kind and returns a Group qualified GroupKind 

Deletes a mirror pod 
DeleteMirrorPod deletes a mirror pod 

MachineInfo is a mock implementation of InterfaceMachineInfo 
MockImageFsInfoProvider is a mock of ImageFsInfoProvider interface 

MachineInfo is a mock implementation of InterfaceMachineInfo 
MockInterface is a mock of Interface interface 

DockerContainer is a mock implementation of InterfaceDockerContainer 
MockImageFsInfoProvider is a mock of ImageFsInfoProvider interface 

DockerContainer is a mock implementation of InterfaceDockerContainer 
MockInterface is a mock of Interface interface 

ContainerInfoV2 is a mock implementation of InterfaceContainerInfoV2 
MockImageFsInfoProvider is a mock of ImageFsInfoProvider interface 

ContainerInfoV2 is a mock implementation of InterfaceContainerInfoV2 
MockInterface is a mock of Interface interface 

ContainerInfo is a mock implementation of InterfaceContainerInfo 
MockImageFsInfoProvider is a mock of ImageFsInfoProvider interface 

ContainerInfo is a mock implementation of InterfaceContainerInfo 
MockInterface is a mock of Interface interface 

read the the manifest URL passed to kubelet 
read the manifest URL passed to kubelet 

This is used in a few places outside of Kubelet such as indexing 
PodInfraContainerName is used in a few places outside of Kubelet such as indexing 

GatePath creates global mount path 
GetPath creates global mount path 

TearDownAt unmounts the bind mount 
TearDown unmounts the bind mount 

TearDown unmounts the bind mount 
TearDownAt unmounts the bind mount 

SetUpAt attaches the disk and bind mounts to the volume path 
SetUp attaches the disk and bind mounts to the volume path 

SetUp attaches the disk and bind mounts to the volume path 
SetUpAt attaches the disk and bind mounts to the volume path 

This is the primary entrypoint for volume plugins 
ProbeVolumePlugins is the primary entrypoint for volume plugins 

We dont want to update the caches copy of the service account 
We dont want to update the caches copy of the secret 

removeSecretReferenceIfNeeded updates the given ServiceAccount to remove a reference to the given secretName if needed Returns whether an update was performed and any error that occurred We dont want to update the caches copy of the service account so remove the secret from a freshly retrieved copy of the service account Doublecheck to see if the account still references the secret 
if the namespace is being terminated create will fail no matter what retriable error Manually add the new token to the cache store This prevents the service account update below triggering another token creation if the referenced token couldnt be found in the store Try to add a reference to the newly created token to the service account 

removeSecretReferenceIfNeeded updates the given ServiceAccount to remove a reference to the given secretName if needed Returns whether an update was performed and any error that occurred We dont want to update the caches copy of the service account so remove the secret from a freshly retrieved copy of the service account 
This prevents the service account update below triggering another token creation if the referenced token couldnt be found in the store Try to add a reference to the newly created token to the service account refresh liveServiceAccount on every retry fetch the live service account if needed and verify the UID matches and that we still need a token 

removeSecretReferenceIfNeeded updates the given ServiceAccount to remove a reference to the given secretName if needed Returns whether an update was performed and any error that occurred We dont want to update the caches copy of the service account so remove the secret from a freshly retrieved copy of the service account 
Save the secret if the namespace is being terminated create will fail no matter what retriable error Manually add the new token to the cache store This prevents the service account update below triggering another token creation if the referenced token couldnt be found in the store Try to add a reference to the newly created token to the service account 

removeSecretReferenceIfNeeded updates the given ServiceAccount to remove a reference to the given secretName if needed 
removeSecretReference updates the given ServiceAccount to remove a reference to the given secretName if needed 

nothing to do  We got a conflict that means that the service account was updated  We simply need to return because well get an update notification later generateTokenIfNeeded populates the token data for the given Secret if not already set 
if we got a Conflict error the secret was updated by someone else and well get an update notification later if we got a NotFound error the secret no longer exists and we dont need to populate a token removeSecretReference updates the given ServiceAccount to remove a reference to the given secretName if needed We dont want to update the caches copy of the service account 

This prevents the service account update below triggering another token creation if the referenced token couldnt be found in the store we werent able to use the token try to clean it up if we fail just log it nothing to do  We got a conflict that means that the service account was updated  We simply need to return because well get an update notification later generateTokenIfNeeded populates the token data for the given Secret if not already set 
fetch the live service account if needed and verify the UID matches and that we still need a token If we dont have the same service account stop trying to add a reference to the token made for the old service account 

This prevents the service account update below triggering another token creation if the referenced token couldnt be found in the store we werent able to use the token try to clean it up if we fail just log it nothing to do  We got a conflict that means that the service account was updated  We simply need to return because well get an update notification later 
if we got a Conflict error the secret was updated by someone else and well get an update notification later if we got a NotFound error the secret no longer exists and we dont need to populate a token removeSecretReference updates the given ServiceAccount to remove a reference to the given secretName if needed We dont want to update the caches copy of the service account 

Save the secret Manually add the new token to the cache store This prevents the service account update below triggering another token creation if the referenced token couldnt be found in the store we werent able to use the token try to clean it up if we fail just log it nothing to do  We got a conflict that means that the service account was updated  We simply need to return because well get an update notification later 
fetch the live service account if needed and verify the UID matches and that we still need a token If we dont have the same service account stop trying to add a reference to the token made for the old service account 

our view of the service account is not up to date 
our view of the secret is not up to date 

so add the secret to a freshly retrieved copy of the service account 
so remove the secret from a freshly retrieved copy of the service account Ignore NotFound errors when attempting to remove a reference 

so add the secret to a freshly retrieved copy of the service account 
so add the token to a freshly retrieved copy of the secret 

We dont want to update the caches copy of the service account 
We dont want to update the caches copy of the secret 

If any existing token secrets are referenced by the service account return Otherwise create a new token secret createSecret creates a secret of type ServiceAccountToken for the given ServiceAccount We dont want to update the caches copy of the service account so add the secret to a freshly retrieved copy of the service account our view of the service account is not up to date 
listTokenSecrets returns a list of all of the ServiceAccountToken secrets that reference the given service accounts name and uid serviceAccountQueueKey holds information we need to sync a service account It contains enough information to look up the cached service account or delete owned tokens if the service account no longer exists 

If any existing token secrets are referenced by the service account return Otherwise create a new token secret createSecret creates a secret of type ServiceAccountToken for the given ServiceAccount We dont want to update the caches copy of the service account so add the secret to a freshly retrieved copy of the service account our view of the service account is not up to date 
Ensure UID matches if given listTokenSecrets returns a list of all of the ServiceAccountToken secrets that reference the given service accounts name and uid serviceAccountQueueKey holds information we need to sync a service account It contains enough information to look up the cached service account 

createSecretIfNeeded makes sure at least one ServiceAccountToken secret exists and is included in the serviceAccounts Secrets list 
ensureReferencedToken makes sure at least one ServiceAccountToken secret exists and is included in the serviceAccounts Secrets list 

secretDeleted reacts to a Secret being deleted by removing a reference from the corresponding ServiceAccount if needed Unknown type If we missed a Secret deletion the corresponding ServiceAccount if it exists will get a secret recreated if needed during the ServiceAccount relist createSecretIfNeeded makes sure at least one ServiceAccountToken secret exists and is included in the serviceAccounts Secrets list If the service account references no secrets shortcircuit and create a new one We shouldnt try to validate secret references until the secrets store is synced 
if we got a Conflict error the service account was updated by someone else and well get an update notification later if we got a NotFound error the service account no longer exists and we dont need to create a token for it retry in all other cases success hasReferencedToken returns true if the serviceAccount references a service account token secret generateTokenIfNeeded populates the token data for the given Secret if not already set 

secretUpdated reacts to a Secret update or relist by deleting the secret if the referenced ServiceAccount does not exist secretDeleted reacts to a Secret being deleted by removing a reference from the corresponding ServiceAccount if needed Unknown type If we missed a Secret deletion the corresponding ServiceAccount if it exists will get a secret recreated if needed during the ServiceAccount relist createSecretIfNeeded makes sure at least one ServiceAccountToken secret exists and is included in the serviceAccounts Secrets list If the service account references no secrets shortcircuit and create a new one 
so remove the secret from a freshly retrieved copy of the service account Ignore NotFound errors when attempting to remove a reference Shortcircuit if the UID doesnt match Shortcircuit if the secret is no longer referenced Remove the secret Ignore NotFound errors when attempting to remove a reference 

This CA will be added in the secretes of service accounts 
This CA will be added in the secrets of service accounts 

SecretResync is the timeDuration at which to fully relist secrets 
ServiceAccountResync is the timeDuration at which to fully relist service accounts 

ServiceAccountResync is the timeDuration at which to fully relist service accounts 
SecretResync is the timeDuration at which to fully relist secrets 

AccessorFunc implements the Accessor interface 
MergeFunc implements the Merger interface 

to use the same channel Different source names however will be treated as a union Accessor is an interface for retrieving the current merge state MergedState returns a representation of the current merge state Must be reentrant when more than one source is defined 
Invoked when a change from a source is received  May also function as an incremental merger if you wish to consume changes incrementally  Must be reentrant when more than one source is defined MergeFunc implements the Merger interface Mux is a class for merging configuration from multiple sources  Changes are 

to use the same channel Different source names however will be treated as a union Accessor is an interface for retrieving the current merge state MergedState returns a representation of the current merge state Must be reentrant when more than one source is defined 
See the License for the specific language governing permissions and limitations under the License Invoked when a change from a source is received  May also function as an incremental merger if you wish to consume changes incrementally  Must be reentrant when more than one source is defined 

source will return the same channel This allows change and state based sources to use the same channel Different source names however will be treated as a union Accessor is an interface for retrieving the current merge state 
See the License for the specific language governing permissions and limitations under the License Invoked when a change from a source is received  May also function as an incremental merger if you wish to consume changes incrementally  Must be reentrant when more than one source is defined MergeFunc implements the Merger interface 

Channel returns a channel where a configuration source 
ChannelWithContext returns a channel where a configuration source 

MergeFunc implements the Merger interface 
AccessorFunc implements the Accessor interface 

Invoked when a change from a source is received  May also function as an incremental merger if you wish to consume changes incrementally  Must be reentrant when more than one source is defined MergeFunc implements the Merger interface Mux is a class for merging configuration from multiple sources  Changes are 
to use the same channel Different source names however will be treated as a union Accessor is an interface for retrieving the current merge state MergedState returns a representation of the current merge state Must be reentrant when more than one source is defined 

See the License for the specific language governing permissions and limitations under the License Invoked when a change from a source is received  May also function as an incremental merger if you wish to consume changes incrementally  Must be reentrant when more than one source is defined MergeFunc implements the Merger interface 
source will return the same channel This allows change and state based sources to use the same channel Different source names however will be treated as a union Accessor is an interface for retrieving the current merge state 

See the License for the specific language governing permissions and limitations under the License Invoked when a change from a source is received  May also function as an incremental merger if you wish to consume changes incrementally  Must be reentrant when more than one source is defined 
to use the same channel Different source names however will be treated as a union Accessor is an interface for retrieving the current merge state MergedState returns a representation of the current merge state Must be reentrant when more than one source is defined 

TODO make it a flag or HPA spec element A store of HPA objects populated by the controller Watches changes to all HPA objects We are not interested in deletions TODO what to do on partial errors like metrics obtained for 75 of pods Computes the desired number of replicas based on the CustomMetrics passed in cmAnnotation as jsonserialized 
If all metrics are invalid or some are invalid and we would scale down return an error and set the condition of the hpa based on the first invalid metric Otherwise set the condition as scaling active as were going to scale Computes the desired number of replicas for a specific hpa and metric specification returning the metric status and a proposed condition to be set on the HPA object computeStatusForObjectMetric computes the desired number of replicas for the specified metric of type ObjectMetricSourceType 

getExec handles requests to run a command inside a container 
getRun handles requests to run a command inside a container 

getRun handles requests to run a command inside a container 
getExec handles requests to run a command inside a container 

encodePods creates an apiPodList object from pods and returns the encoded 
encodePods creates an v1PodList object from pods and returns the encoded 

Setup pporf handlers 
Setup pprof handlers 

InstallDeguggingHandlers registers the HTTP request patterns that serve logs or run commandscontainers 
InstallDebuggingHandlers registers the HTTP request patterns that serve logs or run commandscontainers 

InstallAuthFilter installs authentication filters with the restful Container 
InstallTracingFilter installs OpenTelemetry tracing filter with the restful Container 

HostInterface contains all the kubelet methods required by the server 
AuthInterface contains all methods required by the auth filters 

AuthInterface contains all methods required by the auth filters 
HostInterface contains all the kubelet methods required by the server 

ListenAndServeKubeletReadOnlyServer initializes a server to respond to HTTP network requests on the Kubelet AuthInterface contains all methods required by the auth filters 
ServeHTTP responds to HTTP requests on the Kubelet monitor http requests prometheusHostAdapter adapts the HostInterface to the interface expected by the cAdvisor prometheus collector 

ListenAndServeKubeletReadOnlyServer initializes a server to respond to HTTP network requests on the Kubelet 
ListenAndServeKubeletServer initializes a server to respond to HTTP network requests on the Kubelet 

ListenAndServeKubeletServer initializes a server to respond to HTTP network requests on the Kubelet ListenAndServeKubeletReadOnlyServer initializes a server to respond to HTTP network requests on the Kubelet AuthInterface contains all methods required by the auth filters HostInterface contains all the kubelet methods required by the server 
ServeHTTP responds to HTTP requests on the Kubelet monitor http requests prometheusHostAdapter adapts the HostInterface to the interface expected by the 

ListenAndServeKubeletServer initializes a server to respond to HTTP network requests on the Kubelet 
ListenAndServeKubeletReadOnlyServer initializes a server to respond to HTTP network requests on the Kubelet 

Default port  used if no information about Kubelet port can be found in NodeNodeStatusDaemonEndpoints 
Port specifies the default port  used if no information about Kubelet port can be found in NodeNodeStatusDaemonEndpoints 

Namespace is the name of a namespace APIGroup Resource and Namespace are required to match resource requests 
Resource is the name of a resource APIGroup Resource and Namespace are required to match resource requests 

Resource is the name of a resource APIGroup Resource and Namespace are required to match resource requests 
Namespace is the name of a namespace APIGroup Resource and Namespace are required to match resource requests 

identfier for a loadbalanced service 
identifier for a loadbalanced service 

Sync immediately synchronizes the ProxyProviders current state to iptables 
Sync immediately synchronizes the Providers current state to proxy rules 

TestSelectableFieldLabelConversions verifies that given resource have field 
TestSelectableFieldLabelConversionsOfKind verifies that given resource have field 

which lazily draws from the set of registered credential providers 
which draws from the set of registered credential providers 

This is the primary entrypoint for volume plugins 
ProbeVolumePlugins is the primary entrypoint for volume plugins 

TODO If many pods changed during the same relist period inspecting the pod and getting the PodStatus to update the cache serially may take a while We should be aware of this and parallelize if needed Update the internal storage and send out the events 
Compare the old and the current pods and generate events Get all containers in the old and the new pod If there are events associated with a pod we should update the podCache updateCache will inspect the pod and update the cache If an error occurs during the inspection we want PLEG to retry again 

in the next relist To achieve this we do not update the associated podRecord of the pod so that the change will be detect again in the next relist TODO If many pods changed during the same relist period inspecting the pod and getting the PodStatus to update the cache 
reinspect any pods that failed inspection during the previous relist Rely on updateCache calling GetPodStatus to log the actual error Update the cache timestamp  This needs to happen after all pods have been properly updated in the cache make sure we retain the list of pods that need reinspecting the next time relist is called 

updateCache will inspect the pod and update the cache If an error occurs during the inspection we want PLEG to retry again in the next relist To achieve this we do not update the associated podRecord of the pod so that the change will be 
inspecting the pod and getting the PodStatus to update the cache serially may take a while We should be aware of this and parallelize if needed Rely on updateCache calling GetPodStatus to log the actual error make sure we try to reinspect the pod during the next relisting 

Get all containers in the old and the new pod If there are events associated with a pod we should update the podCache updateCache will inspect the pod and update the cache If an error occurs during the inspection we want PLEG to retry again in the next relist To achieve this we do not update the 
TODO If many pods changed during the same relist period inspecting the pod and getting the PodStatus to update the cache serially may take a while We should be aware of this and parallelize if needed Rely on updateCache calling GetPodStatus to log the actual error make sure we try to reinspect the pod during the next relisting 

with the internal podscontainers and generats events accordingly 
with the internal podscontainers and generates events accordingly 

threshold so that we dont cause kubelet to be restarted unnecessarily We report ContainerDied when container was stopped OR removed We may want to distinguish the two cases in the future We already reported that the container died before TODO We may want to generate a ContainerRemoved event as well Its ok now because no one relies on the ContainerRemoved event 
serially may take a while We should be aware of this and parallelize if needed Rely on updateCache calling GetPodStatus to log the actual error make sure we try to reinspect the pod during the next relisting this pod was in the list to reinspect and we did so because it had events so remove it 

Returns a channel from which the subscriber can receive PodLifecycleEvent 
Watch returns a channel from which the subscriber can receive PodLifecycleEvent 

kubecontainerContainerState except for the nonexistent state This state 
kubecontainerState except for the nonexistent state This state 

garbage collector is implemented to work with such situtations However to 
garbage collector is implemented to work with such situations However to 

Note that this assumption is not unique  many kubelet internal components rely on terminated containers as tombstones for bookkeeping purposes The garbage collector is implemented to work with such situtations However to guarantee that kubelet can handle missing container events it is recommended to set the relist period short and have an auxiliary longer 
limitations under the License GenericPLEG is an extremely simple generic PLEG that relies solely on periodic listing to discover container changes It should be used as temporary replacement for container runtimes do not support a proper 

periodic listing to discover container changes It should be be used 
periodic listing to discover container changes It should be used 

package capbabilities manages system level capabilities 
Package capabilities manages system level capabilities 

Simulate a matchin with 1 core and 375GB of memory 
Simulate a machine with 1 core and 375GB of memory 

Fake cAdvisor implementation 
Fake cadvisorInterface implementation 

This is the primary entrypoint for volume plugins 
ProbeVolumePlugins is the primary endpoint for volume plugins 

NewManager creates ane returns an empty results manager 
NewManager creates and returns an empty results manager 

Remove clears the cached result for the container with the given ID 
Set sets the cached result for the container with the given ID 

Remove clears the cached result for the container with the given ID 
Get returns the cached result for the container with the given ID 

Set sets the cached result for the container with the given ID 
Remove clears the cached result for the container with the given ID 

Get returns the cached result for the container with the given ID 
Remove clears the cached result for the container with the given ID 

if no disk matches input wwn and lun exit 
if no disk matches input wwid exit 

ecrProvider is a DockerConfigProvider that gets and refreshes 12hour tokens 
ecrProvider is a DockerConfigProvider that gets and refreshes tokens 

Copied from cloudproviderawslog_handlergo 
Copied from pkgcloudproviderprovidersawslog_handlergo 

Namespace is the name of a namespace APIGroup Resource and Namespace are required to match resource requests 
Resource is the name of a resource APIGroup Resource and Namespace are required to match resource requests 

Resource is the name of a resource APIGroup Resource and Namespace are required to match resource requests 
Namespace is the name of a namespace APIGroup Resource and Namespace are required to match resource requests 

If all containers are known and succeeded just return PodCompleted 
If all init containers are known and succeeded just return PodCompleted 

returns an unready condition 
returns an uninitialized condition 

GeneratePodReadyCondition returns ready condition if all containers in a pod are ready else it 
GeneratePodInitializedCondition returns initialized condition if all init containers in a pod are ready else it 

obj could be an apiNamespace or a DeletionFinalStateUnknown item 
obj could be an v1Namespace or a DeletionFinalStateUnknown item 

The container fails a liveness check it will need to be restared 
The container fails a livenessstartup check it will need to be restarted 

Either the container has not been created yet or it was deleted 
Either the pod has not been created yet or it was already deleted 

Either the pod has not been created yet or it was already deleted 
Either the container has not been created yet or it was deleted 

readOnly bool is supplied by persistentclaim volume source when its builder creates other volumes 
readOnly bool is supplied by persistentclaim volume source when its mounter creates other volumes 

Eg If the devices cgroup for the container is stored in sysfscgroupdevicesdockernginx 
Eg if the devices cgroup for the container is stored in sysfscgroupdevicesdockernginx 

GetLoadBalancerSourceRanges verifies and parses the AnnotationLoadBalancerSourceRangesKey annotation from a service extracting the source ranges to allow and if not present returns a default allowall value 
IsAllowAll checks whether the utilnetIPNet allows traffic from 00000 GetLoadBalancerSourceRanges first try to parse and verify LoadBalancerSourceRanges field from a service If the field is not specified turn to parse and verify the AnnotationLoadBalancerSourceRangesKey annotation from a service 

IsAllowAll checks whether the netsetsIPNet allows traffic from 00000 
IsAllowAll checks whether the utilnetIPNet allows traffic from 00000 

make a directory like varlibkubeletpluginskubernetesiopodfctargetlun0 
make a directory like varlibkubeletpluginskubernetesiofcvolumeDevicestargetlun0 

make a directory like varlibkubeletpluginskubernetesiopodfctargetlun0 
make a directory like varlibkubeletpluginskubernetesiofctarget1target2lun0 

given a wwn and lun find the device and associated devicemapper parent 
given a wwid find the device and associated devicemapper parent 

Reconcile a CIDR managed by this shaper with the state on the ground 
Reconcile the interface managed by this shaper with the state on the ground 

Reconcile the interface managed by this shaper with the state on the ground 
Reconcile a CIDR managed by this shaper with the state on the ground 

Remove a bandwidth limit for a particular CIDR on a particular network interface 
Limit the bandwidth for a particular CIDR on a particular interface 

ingress bandwidth limit applies to all packets on the interface whose destination matches cidr 
egress bandwidth limit applies to all packets on the interface whose source matches cidr 

egress bandwidth limit applies to all packets on the interface whose source matches cidr 
ingress bandwidth limit applies to all packets on the interface whose destination matches cidr 

Limit the bandwidth for a particular CIDR on a particular interface 
Remove a bandwidth limit for a particular CIDR on a particular network interface 

Handles kubernetes podcontainer stats requests to 
Handles stats summary requests to statssummary 

Host methods required by stats handlers 
Provider hosts methods required by stats handlers 

Unmounts the device and detaches the disk from the kubelets host machine 
DetachDisk unmounts the device and detaches the disk from the kubelets host machine 

Attaches a disk specified by a volumeCinderPersistenDisk to the current kubelet 
AttachDisk attaches a disk specified by a volumeCinderPersistenDisk to the current kubelet 

over write older snapshots of endpoints from the producer goroutine 
over write older snapshots of services from the producer goroutine 

We might get 1 or more updates for N endpoint updates because we 
We might get 1 or more updates for N service updates because we 

over write older snapshots of services from the producer goroutine 
over write older snapshots of endpoints from the producer goroutine 

We might get 1 or more updates for N service updates because we 
We might get 1 or more updates for N endpoint updates because we 

namespaceUpdated reacts to a Namespace update or relist by creating a default ServiceAccount in the namespace if needed 
serviceAccountDeleted reacts to a ServiceAccount deletion by recreating a default ServiceAccount in the namespace if needed 

serviceAccountDeleted reacts to a ServiceAccount deletion by recreating a default ServiceAccount in the namespace if needed 
namespaceUpdated reacts to a Namespace update or relist by creating a default ServiceAccount in the namespace if needed 

If nonzero all namespaces will be relisted this often 
If nonzero all service accounts will be relisted this often 

NamespaceResync is the interval between full resyncs of Namespaces 
ServiceAccountResync is the interval between full resyncs of ServiceAccounts 

If nonzero all service accounts will be relisted this often 
If nonzero all namespaces will be relisted this often 

ServiceAccountResync is the interval between full resyncs of ServiceAccounts 
NamespaceResync is the interval between full resyncs of Namespaces 

of the selector of the replicaset so the possible matching pods must be part of the filteredPods Always updates status as pods come up or die Multiple things could lead to this update failing Requeuing the replica set ensures we retry with some fairness 
retry the slow start process Decrement the expected number of creates because the informer wont observe this pod Choose which Pods to delete preferring those in earlier phases of startup Snapshot the UIDs nsname of the pods were expecting to see deleted so we know to record their expectations exactly once either when we see it as an update of the deletion timestamp or as a delete 

meaning it did not expect to see any more of its pods created or deleted This function is not meant to be invoked concurrently with the same key Sleep so we give the pod reflector goroutine a chance to run Check the expectations of the ReplicaSet before counting active pods otherwise a new pod can sneak in and update the expectations after weve retrieved active pods from the store If a new pod enters 
Snapshot the UIDs nsname of the pods were expecting to see deleted so we know to record their expectations exactly once either when we see it as an update of the deletion timestamp or as a delete Note that if the labels on a podrs change in a way that the pod gets orphaned the rs will only wake up after the expectations have 

meaning it did not expect to see any more of its pods created or deleted This function is not meant to be invoked concurrently with the same key Sleep so we give the pod reflector goroutine a chance to run Check the expectations of the ReplicaSet before counting active pods otherwise a new pod can sneak in and update the expectations after weve retrieved active pods from the store If a new pod enters 
Choose which Pods to delete preferring those in earlier phases of startup Snapshot the UIDs nsname of the pods were expecting to see deleted so we know to record their expectations exactly once either when we see it as an update of the deletion timestamp or as a delete Note that if the labels on a podrs change in a way that the pod gets 

meaning it did not expect to see any more of its pods created or deleted This function is not meant to be invoked concurrently with the same key Sleep so we give the pod reflector goroutine a chance to run Check the expectations of the ReplicaSet before counting active pods otherwise a new pod can sneak in and update the expectations after weve retrieved active pods from the store If a new pod enters 
we dont need to check the oldPodDeletionTimestamp because DeletionTimestamp cannot be unset The ControllerRef was changed Sync the old controller if any If it has a ControllerRef thats all that matters TODO MinReadySeconds in the Pod will generate an Available condition to be added in the Pod status which in turn will trigger a requeue of the owning replica set thus having its status updated with the newly available replica For now we can fake the 

meaning it did not expect to see any more of its pods created or deleted This function is not meant to be invoked concurrently with the same key Sleep so we give the pod reflector goroutine a chance to run Check the expectations of the ReplicaSet before counting active pods otherwise a new pod can sneak in and update the expectations after weve retrieved active pods from the store If a new pod enters 
up If the labels of the pod have changed we need to awaken both the old and new replica set old and cur must be v1Pod types Periodic resync will send update events for all known pods Two different versions of the same pod will always have different RVs when a pod is deleted gracefully its deletion timestamp is first modified to reflect a grace period and after such time has passed the kubelet actually deletes it from the store We receive an update 

meaning it did not expect to see any more of its pods created or deleted This function is not meant to be invoked concurrently with the same key Sleep so we give the pod reflector goroutine a chance to run Check the expectations of the ReplicaSet before counting active pods otherwise a new pod can sneak in and update the expectations after weve retrieved active pods from the store If a new pod enters 
When a pod is updated figure out what replica sets manage it and wake them up If the labels of the pod have changed we need to awaken both the old and new replica set old and cur must be v1Pod types Periodic resync will send update events for all known pods Two different versions of the same pod will always have different RVs when a pod is deleted gracefully its deletion timestamp is first modified to reflect a grace period 

meaning it did not expect to see any more of its pods created or deleted This function is not meant to be invoked concurrently with the same key Sleep so we give the pod reflector goroutine a chance to run Check the expectations of the ReplicaSet before counting active pods otherwise a new pod can sneak 
Decrement the expected number of creates because the informer wont observe this pod Choose which Pods to delete preferring those in earlier phases of startup Snapshot the UIDs nsname of the pods were expecting to see deleted so we know to record their expectations exactly once either when we see it as an update of the deletion timestamp or as a delete Note that if the labels on a podrs change in a way that the pod gets 

meaning it did not expect to see any more of its pods created or deleted This function is not meant to be invoked concurrently with the same key Sleep so we give the pod reflector goroutine a chance to run Check the expectations of the ReplicaSet before counting active pods otherwise a new pod can sneak 
low quota that attempts to create a large number of pods will be prevented from spamming the API service with the pod create requests after one of its pods fails  Conveniently this also prevents the event spam that those failures would generate if the namespace is being terminated we dont have to do 

meaning it did not expect to see any more of its pods created or deleted This function is not meant to be invoked concurrently with the same key Sleep so we give the pod reflector goroutine a chance to run Check the expectations of the ReplicaSet before counting active pods otherwise a new pod can sneak 
until the kubelet actually deletes the pod This is different from the Phase of a pod changing because an rs never initiates a phase change and so is never asleep waiting for the same we dont need to check the oldPodDeletionTimestamp because DeletionTimestamp cannot be unset 

syncReplicaSet will sync the ReplicaSet with the given key if it has had its expectations fulfilled meaning it did not expect to see any more of its pods created or deleted This function is not meant to be invoked concurrently with the same key Sleep so we give the pod reflector goroutine a chance to run Check the expectations of the ReplicaSet before counting active pods otherwise a new pod can sneak in and update the expectations after weve retrieved active pods from the store If a new pod enters 
when we see it as an update of the deletion timestamp or as a delete Note that if the labels on a podrs change in a way that the pod gets orphaned the rs will only wake up after the expectations have expired even if other pods are deleted Decrement the expected number of deletes because the informer wont observe this deletion all errors have been reported before and theyre likely to be the same so well only return the first one we hit 

syncReplicaSet will sync the ReplicaSet with the given key if it has had its expectations fulfilled meaning it did not expect to see any more of its pods created or deleted This function is not meant to be invoked concurrently with the same key Sleep so we give the pod reflector goroutine a chance to run Check the expectations of the ReplicaSet before counting active pods otherwise a new pod can sneak in and update the expectations after weve retrieved active pods from the store If a new pod enters 
deleted so we know to record their expectations exactly once either when we see it as an update of the deletion timestamp or as a delete Note that if the labels on a podrs change in a way that the pod gets orphaned the rs will only wake up after the expectations have expired even if other pods are deleted Decrement the expected number of deletes because the informer wont observe this deletion 

syncReplicaSet will sync the ReplicaSet with the given key if it has had its expectations fulfilled meaning it did not expect to see any more of its pods created or deleted This function is not meant to be invoked concurrently with the same key Sleep so we give the pod reflector goroutine a chance to run Check the expectations of the ReplicaSet before counting active pods otherwise a new pod can sneak in and update the expectations after weve retrieved active pods from the store If a new pod enters 
we dont need to check the oldPodDeletionTimestamp because DeletionTimestamp cannot be unset The ControllerRef was changed Sync the old controller if any If it has a ControllerRef thats all that matters TODO MinReadySeconds in the Pod will generate an Available condition to be added in the Pod status which in turn will trigger a requeue of the owning replica set thus 

syncReplicaSet will sync the ReplicaSet with the given key if it has had its expectations fulfilled meaning it did not expect to see any more of its pods created or deleted This function is not meant to be invoked concurrently with the same key Sleep so we give the pod reflector goroutine a chance to run Check the expectations of the ReplicaSet before counting active pods otherwise a new pod can sneak 
we dont need to check the oldPodDeletionTimestamp because DeletionTimestamp cannot be unset The ControllerRef was changed Sync the old controller if any If it has a ControllerRef thats all that matters TODO MinReadySeconds in the Pod will generate an Available condition to be added in the Pod status which in turn will trigger a requeue of the owning replica set thus having its status updated with the newly available replica For now we can fake the 

syncReplicaSet will sync the ReplicaSet with the given key if it has had its expectations fulfilled meaning it did not expect to see any more of its pods created or deleted This function is not meant to be invoked concurrently with the same key Sleep so we give the pod reflector goroutine a chance to run 
low quota that attempts to create a large number of pods will be prevented from spamming the API service with the pod create requests after one of its pods fails  Conveniently this also prevents the event spam that those failures would generate if the namespace is being terminated we dont have to do anything because any creation will fail 

Decrement the expected number of deletes because the informer wont observe this deletion syncReplicaSet will sync the ReplicaSet with the given key if it has had its expectations fulfilled meaning it did not expect to see any more of its pods created or deleted This function is not meant to be invoked concurrently with the same key Sleep so we give the pod reflector goroutine a chance to run 
prevented from spamming the API service with the pod create requests after one of its pods fails  Conveniently this also prevents the event spam that those failures would generate if the namespace is being terminated we dont have to do 

Decrement the expected number of deletes because the informer wont observe this deletion syncReplicaSet will sync the ReplicaSet with the given key if it has had its expectations fulfilled meaning it did not expect to see any more of its pods created or deleted This function is not meant to be invoked concurrently with the same key Sleep so we give the pod reflector goroutine a chance to run 
we dont need to check the oldPodDeletionTimestamp because DeletionTimestamp cannot be unset The ControllerRef was changed Sync the old controller if any If it has a ControllerRef thats all that matters TODO MinReadySeconds in the Pod will generate an Available condition to be added in the Pod status which in turn will trigger a requeue of the owning replica set thus having its status updated with the newly available replica For now we can fake the 

Decrement the expected number of deletes because the informer wont observe this deletion syncReplicaSet will sync the ReplicaSet with the given key if it has had its expectations fulfilled meaning it did not expect to see any more of its pods created or deleted This function is not meant to be invoked concurrently with the same key Sleep so we give the pod reflector goroutine a chance to run 
for modification of the deletion timestamp and expect an rs to create more replicas asap not wait until the kubelet actually deletes the pod This is different from the Phase of a pod changing because an rs never initiates a phase change and so is never asleep waiting for the same we dont need to check the oldPodDeletionTimestamp because DeletionTimestamp cannot be unset The ControllerRef was changed Sync the old controller if any 

Decrement the expected number of deletes because the informer wont observe this deletion syncReplicaSet will sync the ReplicaSet with the given key if it has had its expectations fulfilled meaning it did not expect to see any more of its pods created or deleted This function is not meant to be invoked concurrently with the same key 
low quota that attempts to create a large number of pods will be prevented from spamming the API service with the pod create requests after one of its pods fails  Conveniently this also prevents the event spam that those failures would generate if the namespace is being terminated we dont have to do 

Decrement the expected number of deletes because the informer wont observe this deletion syncReplicaSet will sync the ReplicaSet with the given key if it has had its expectations fulfilled meaning it did not expect to see any more of its pods created or deleted This function is not meant to be 
prevented from spamming the API service with the pod create requests after one of its pods fails  Conveniently this also prevents the event spam that those failures would generate if the namespace is being terminated we dont have to do anything because any creation will fail 

Decrement the expected number of deletes because the informer wont observe this deletion 
Decrement the expected number of creates because the informer wont observe this pod 

expired even if other pods are deleted Decrement the expected number of deletes because the informer wont observe this deletion syncReplicaSet will sync the ReplicaSet with the given key if it has had its expectations fulfilled meaning it did not expect to see any more of its pods created or deleted This function is not meant to be invoked concurrently with the same key Sleep so we give the pod reflector goroutine a chance to run 
after one of its pods fails  Conveniently this also prevents the event spam that those failures would generate if the namespace is being terminated we dont have to do 

expired even if other pods are deleted Decrement the expected number of deletes because the informer wont observe this deletion syncReplicaSet will sync the ReplicaSet with the given key if it has had its expectations fulfilled meaning it did not expect to see any more of its pods created or deleted This function is not meant to be invoked concurrently with the same key 
prevented from spamming the API service with the pod create requests after one of its pods fails  Conveniently this also prevents the event spam that those failures would generate if the namespace is being terminated we dont have to do 

expired even if other pods are deleted Decrement the expected number of deletes because the informer wont observe this deletion syncReplicaSet will sync the ReplicaSet with the given key if it has had its expectations fulfilled meaning it did not expect to see any more of its pods created or deleted This function is not meant to be 
after one of its pods fails  Conveniently this also prevents the event spam that those failures would generate if the namespace is being terminated we dont have to do anything because any creation will fail 

orphaned the rs will only wake up after the expectations have expired even if other pods are deleted Decrement the expected number of deletes because the informer wont observe this deletion syncReplicaSet will sync the ReplicaSet with the given key if it has had its expectations fulfilled meaning it did not expect to see any more of its pods created or deleted This function is not meant to be 
prevented from spamming the API service with the pod create requests after one of its pods fails  Conveniently this also prevents the event spam that those failures would generate if the namespace is being terminated we dont have to do 

Note that if the labels on a podrs change in a way that the pod gets orphaned the rs will only wake up after the expectations have expired even if other pods are deleted Decrement the expected number of deletes because the informer wont observe this deletion syncReplicaSet will sync the ReplicaSet with the given key if it has had its expectations fulfilled meaning it did not expect to see any more of its pods created or deleted This function is not meant to be 
we dont need to check the oldPodDeletionTimestamp because DeletionTimestamp cannot be unset The ControllerRef was changed Sync the old controller if any If it has a ControllerRef thats all that matters TODO MinReadySeconds in the Pod will generate an Available condition to be added in the Pod status which in turn will trigger a requeue of the owning replica set thus 

Note that if the labels on a podrs change in a way that the pod gets orphaned the rs will only wake up after the expectations have expired even if other pods are deleted Decrement the expected number of deletes because the informer wont observe this deletion syncReplicaSet will sync the ReplicaSet with the given key if it has had its expectations fulfilled meaning it did not expect to see any more of its pods created or deleted This function is not meant to be 
until the kubelet actually deletes the pod This is different from the Phase of a pod changing because an rs never initiates a phase change and so is never asleep waiting for the same we dont need to check the oldPodDeletionTimestamp because DeletionTimestamp cannot be unset The ControllerRef was changed Sync the old controller if any If it has a ControllerRef thats all that matters TODO MinReadySeconds in the Pod will generate an Available condition to be added in 

Note that if the labels on a podrs change in a way that the pod gets orphaned the rs will only wake up after the expectations have expired even if other pods are deleted Decrement the expected number of deletes because the informer wont observe this deletion syncReplicaSet will sync the ReplicaSet with the given key if it has had its expectations fulfilled meaning it did not expect to see any more of its pods created or deleted This function is not meant to be 
for modification of the deletion timestamp and expect an rs to create more replicas asap not wait until the kubelet actually deletes the pod This is different from the Phase of a pod changing because an rs never initiates a phase change and so is never asleep waiting for the same we dont need to check the oldPodDeletionTimestamp because DeletionTimestamp cannot be unset The ControllerRef was changed Sync the old controller if any If it has a ControllerRef thats all that matters 

Note that if the labels on a podrs change in a way that the pod gets orphaned the rs will only wake up after the expectations have expired even if other pods are deleted Decrement the expected number of deletes because the informer wont observe this deletion syncReplicaSet will sync the ReplicaSet with the given key if it has had its expectations fulfilled meaning it did not expect to see any more of its pods created or deleted This function is not meant to be 
getPodReplicaSets returns a list of ReplicaSets matching the given pod ControllerRef will ensure we dont do anything crazy but more than one item in this list nevertheless constitutes user error resolveControllerRef returns the controller referenced by a ControllerRef or nil if the ControllerRef could not be resolved to a matching controller of the correct Kind 

Decrement the expected number of creates because the informer wont observe this pod 
Decrement the expected number of deletes because the informer wont observe this deletion 

into a performance bottleneck We should generate a UID for the pod beforehand and store it via ExpectCreations Decrement the expected number of creates because the informer wont observe this pod No need to sort pods if we are about to delete all of them Sort the pods in the order such that notready  ready unscheduled 
low quota that attempts to create a large number of pods will be prevented from spamming the API service with the pod create requests after one of its pods fails  Conveniently this also prevents the event spam that those failures would generate if the namespace is being terminated we dont have to do 

UID which would require locking across the create which will turn into a performance bottleneck We should generate a UID for the pod beforehand and store it via ExpectCreations Decrement the expected number of creates because the informer wont observe this pod No need to sort pods if we are about to delete all of them 
low quota that attempts to create a large number of pods will be prevented from spamming the API service with the pod create requests after one of its pods fails  Conveniently this also prevents the event spam that those failures would generate if the namespace is being terminated we dont have to do 

is wed need to wait on the result of a create to record the pods UID which would require locking across the create which will turn into a performance bottleneck We should generate a UID for the pod beforehand and store it via ExpectCreations Decrement the expected number of creates because the informer wont observe this pod 
likely all fail with the same error For example a project with a low quota that attempts to create a large number of pods will be prevented from spamming the API service with the pod create requests after one of its pods fails  Conveniently this also prevents the event spam that those failures would generate if the namespace is being terminated we dont have to do 

It enforces that the syncHandler is never invoked concurrently with the same key manageReplicas checks and updates replicas for the given ReplicaSet TODO Track UIDs of creates just like deletes The problem currently is wed need to wait on the result of a create to record the pods UID which would require locking across the create which will turn into a performance bottleneck We should generate a UID for the pod 
likely all fail with the same error For example a project with a low quota that attempts to create a large number of pods will be prevented from spamming the API service with the pod create requests after one of its pods fails  Conveniently this also prevents the event spam that those failures would generate 

It enforces that the syncHandler is never invoked concurrently with the same key manageReplicas checks and updates replicas for the given ReplicaSet TODO Track UIDs of creates just like deletes The problem currently is wed need to wait on the result of a create to record the pods UID which would require locking across the create which will turn into a performance bottleneck We should generate a UID for the pod 
This handles attempts to start large numbers of pods that would likely all fail with the same error For example a project with a low quota that attempts to create a large number of pods will be prevented from spamming the API service with the pod create requests 

It enforces that the syncHandler is never invoked concurrently with the same key manageReplicas checks and updates replicas for the given ReplicaSet TODO Track UIDs of creates just like deletes The problem currently is wed need to wait on the result of a create to record the pods UID which would require locking across the create which will turn into a performance bottleneck We should generate a UID for the pod 
the Pod status which in turn will trigger a requeue of the owning replica set thus having its status updated with the newly available replica For now we can fake the update by resyncing the controller MinReadySeconds after the it is requeued because a Pod transitioned to Ready Note that this still suffers from 29229 we are just moving the problem one level 

It enforces that the syncHandler is never invoked concurrently with the same key manageReplicas checks and updates replicas for the given ReplicaSet TODO Track UIDs of creates just like deletes The problem currently is wed need to wait on the result of a create to record the pods UID which would require locking across the create which will turn into a performance bottleneck We should generate a UID for the pod 
Periodic resync will send update events for all known pods Two different versions of the same pod will always have different RVs when a pod is deleted gracefully its deletion timestamp is first modified to reflect a grace period and after such time has passed the kubelet actually deletes it from the store We receive an update for modification of the deletion timestamp and expect an rs to create more replicas asap not wait 

It enforces that the syncHandler is never invoked concurrently with the same key manageReplicas checks and updates replicas for the given ReplicaSet TODO Track UIDs of creates just like deletes The problem currently is wed need to wait on the result of a create to record the pods UID which would require locking across the create which will turn 
This handles attempts to start large numbers of pods that would likely all fail with the same error For example a project with a low quota that attempts to create a large number of pods will be prevented from spamming the API service with the pod create requests after one of its pods fails  Conveniently this also prevents the event spam that those failures would generate 

It enforces that the syncHandler is never invoked concurrently with the same key manageReplicas checks and updates replicas for the given ReplicaSet TODO Track UIDs of creates just like deletes The problem currently is wed need to wait on the result of a create to record the pods UID which would require locking across the create which will turn 
NewReplicaSetController configures a replica set controller with the specified event recorder NewBaseController is the implementation of NewReplicaSetController with additional injected parameters so that it can also serve as the implementation of NewReplicationController This invokes the ReplicaSet for every pod change eg host assignment Though this might seem like overkill the most frequent pod update is status and the associated ReplicaSet will only list from 

It enforces that the syncHandler is never invoked concurrently with the same key manageReplicas checks and updates replicas for the given ReplicaSet TODO Track UIDs of creates just like deletes The problem currently is wed need to wait on the result of a create to record the pods UID which would require locking across the create which will turn 
Controllers that need to be synced NewReplicaSetController configures a replica set controller with the specified event recorder NewBaseController is the implementation of NewReplicaSetController with additional injected parameters so that it can also serve as the implementation of NewReplicationController This invokes the ReplicaSet for every pod change eg host assignment Though this might seem like 

It enforces that the syncHandler is never invoked concurrently with the same key manageReplicas checks and updates replicas for the given ReplicaSet TODO Track UIDs of creates just like deletes The problem currently is wed need to wait on the result of a create to record the pods 
Two different versions of the same pod will always have different RVs when a pod is deleted gracefully its deletion timestamp is first modified to reflect a grace period and after such time has passed the kubelet actually deletes it from the store We receive an update for modification of the deletion timestamp and expect an rs to create more replicas asap not wait until the kubelet actually deletes the pod This is different from the Phase of a pod changing because 

It enforces that the syncHandler is never invoked concurrently with the same key manageReplicas checks and updates replicas for the given ReplicaSet TODO Track UIDs of creates just like deletes The problem currently is wed need to wait on the result of a create to record the pods 
Controllers that need to be synced NewReplicaSetController configures a replica set controller with the specified event recorder NewBaseController is the implementation of NewReplicaSetController with additional injected parameters so that it can also serve as the implementation of NewReplicationController This invokes the ReplicaSet for every pod change eg host assignment Though this might seem like overkill the most frequent pod update is status and the associated ReplicaSet will only list from 

worker runs a worker thread that just dequeues items processes them and marks them done It enforces that the syncHandler is never invoked concurrently with the same key manageReplicas checks and updates replicas for the given ReplicaSet TODO Track UIDs of creates just like deletes The problem currently is wed need to wait on the result of a create to record the pods UID which would require locking across the create which will turn 
likely all fail with the same error For example a project with a low quota that attempts to create a large number of pods will be prevented from spamming the API service with the pod create requests after one of its pods fails  Conveniently this also prevents the event spam that those failures would generate 

worker runs a worker thread that just dequeues items processes them and marks them done It enforces that the syncHandler is never invoked concurrently with the same key manageReplicas checks and updates replicas for the given ReplicaSet TODO Track UIDs of creates just like deletes The problem currently is wed need to wait on the result of a create to record the pods UID which would require locking across the create which will turn 
This handles attempts to start large numbers of pods that would likely all fail with the same error For example a project with a low quota that attempts to create a large number of pods will be prevented from spamming the API service with the pod create requests after one of its pods fails  Conveniently this also prevents the 

obj could be an extensionsReplicaSet or a DeletionFinalStateUnknown marker item 
obj could be an v1Pod or a DeletionFinalStateUnknown marker item 

changed labels the new ReplicaSet will not be woken up till the periodic resync obj could be an extensionsReplicaSet or a DeletionFinalStateUnknown marker item TODO Handle overlapping replica sets better Either disallow them at admission time or deterministically avoid syncing replica sets that fight over pods Currently we only ensure that the same replica set is synced for a given pod When we periodically relist all replica sets there will still be some replica instability One way to handle this is 
DO NOT observe creation because no controller should be waiting for an orphan When a pod is updated figure out what replica sets manage it and wake them up If the labels of the pod have changed we need to awaken both the old and new replica set old and cur must be v1Pod types Periodic resync will send update events for all known pods 

When a delete is dropped the relist will notice a pod in the store not in the list leading to the insertion of a tombstone object which contains the deleted keyvalue Note that this value might be stale If the pod changed labels the new ReplicaSet will not be woken up till the periodic resync obj could be an extensionsReplicaSet or a DeletionFinalStateUnknown marker item TODO Handle overlapping replica sets better Either disallow them at admission time or 
item in this list nevertheless constitutes user error resolveControllerRef returns the controller referenced by a ControllerRef or nil if the ControllerRef could not be resolved to a matching controller of the correct Kind 

When a delete is dropped the relist will notice a pod in the store not in the list leading to the insertion of a tombstone object which contains the deleted keyvalue Note that this value might be stale If the pod changed labels the new ReplicaSet will not be woken up till the periodic resync 
If it has a ControllerRef thats all that matters TODO MinReadySeconds in the Pod will generate an Available condition to be added in the Pod status which in turn will trigger a requeue of the owning replica set thus having its status updated with the newly available replica For now we can fake the update by resyncing the controller MinReadySeconds after the it is requeued because a Pod transitioned to Ready 

When a delete is dropped the relist will notice a pod in the store not in the list leading to the insertion of a tombstone object which contains the deleted keyvalue Note that this value might be stale If the pod 
The ControllerRef was changed Sync the old controller if any If it has a ControllerRef thats all that matters TODO MinReadySeconds in the Pod will generate an Available condition to be added in the Pod status which in turn will trigger a requeue of the owning replica set thus 

obj could be an apiPod or a DeletionFinalStateUnknown marker item When a delete is dropped the relist will notice a pod in the store not in the list leading to the insertion of a tombstone object which contains the deleted keyvalue Note that this value might be stale If the pod 
item in this list nevertheless constitutes user error resolveControllerRef returns the controller referenced by a ControllerRef or nil if the ControllerRef could not be resolved to a matching controller of the correct Kind 

obj could be an apiPod or a DeletionFinalStateUnknown marker item 
obj could be an v1Pod or a DeletionFinalStateUnknown marker item 

When a pod is deleted enqueue the replica set that manages the pod and update its expectations 
When a pod is created enqueue the replica set that manages it and update its expectations 

will set expectations preventing any damage from the second When a pod is deleted enqueue the replica set that manages the pod and update its expectations obj could be an apiPod or a DeletionFinalStateUnknown marker item When a delete is dropped the relist will notice a pod in the store not in the list leading to the insertion of a tombstone object which contains the deleted keyvalue Note that this value might be stale If the pod 
If it has a ControllerRef thats all that matters TODO MinReadySeconds in the Pod will generate an Available condition to be added in the Pod status which in turn will trigger a requeue of the owning replica set thus having its status updated with the newly available replica For now we can fake the update by resyncing the controller MinReadySeconds after the it is requeued because a Pod transitioned to Ready 

will set expectations preventing any damage from the second When a pod is deleted enqueue the replica set that manages the pod and update its expectations obj could be an apiPod or a DeletionFinalStateUnknown marker item When a delete is dropped the relist will notice a pod in the store not in the list leading to the insertion of a tombstone object which contains 
when a pod is deleted gracefully its deletion timestamp is first modified to reflect a grace period and after such time has passed the kubelet actually deletes it from the store We receive an update for modification of the deletion timestamp and expect an rs to create more replicas asap not wait until the kubelet actually deletes the pod This is different from the Phase of a pod changing because 

If the old and new ReplicaSet are the same the first one that syncs will set expectations preventing any damage from the second When a pod is deleted enqueue the replica set that manages the pod and update its expectations obj could be an apiPod or a DeletionFinalStateUnknown marker item When a delete is dropped the relist will notice a pod in the store not in the list leading to the insertion of a tombstone object which contains 
If it has a ControllerRef thats all that matters TODO MinReadySeconds in the Pod will generate an Available condition to be added in the Pod status which in turn will trigger a requeue of the owning replica set thus having its status updated with the newly available replica For now we can fake the update by resyncing the controller MinReadySeconds after the it is requeued because a Pod transitioned to Ready 

If the old and new ReplicaSet are the same the first one that syncs will set expectations preventing any damage from the second When a pod is deleted enqueue the replica set that manages the pod and update its expectations obj could be an apiPod or a DeletionFinalStateUnknown marker item When a delete is dropped the relist will notice a pod in the store not in the list leading to the insertion of a tombstone object which contains 
The ControllerRef was changed Sync the old controller if any If it has a ControllerRef thats all that matters TODO MinReadySeconds in the Pod will generate an Available condition to be added in the Pod status which in turn will trigger a requeue of the owning replica set thus 

an rs never initiates a phase change and so is never asleep waiting for the same If the old and new ReplicaSet are the same the first one that syncs will set expectations preventing any damage from the second When a pod is deleted enqueue the replica set that manages the pod and update its expectations obj could be an apiPod or a DeletionFinalStateUnknown marker item 
when a pod is deleted gracefully its deletion timestamp is first modified to reflect a grace period and after such time has passed the kubelet actually deletes it from the store We receive an update for modification of the deletion timestamp and expect an rs to create more replicas asap not wait until the kubelet actually deletes the pod This is different from the Phase of a pod changing because 

an rs never initiates a phase change and so is never asleep waiting for the same If the old and new ReplicaSet are the same the first one that syncs will set expectations preventing any damage from the second When a pod is deleted enqueue the replica set that manages the pod and update its expectations 
The ControllerRef was changed Sync the old controller if any If it has a ControllerRef thats all that matters TODO MinReadySeconds in the Pod will generate an Available condition to be added in the Pod status which in turn will trigger a requeue of the owning replica set thus having its status updated with the newly available replica For now we can fake the update by resyncing the controller MinReadySeconds after the it is requeued because 

an rs never initiates a phase change and so is never asleep waiting for the same If the old and new ReplicaSet are the same the first one that syncs will set expectations preventing any damage from the second When a pod is deleted enqueue the replica set that manages the pod and update its expectations 
Two different versions of the same pod will always have different RVs when a pod is deleted gracefully its deletion timestamp is first modified to reflect a grace period and after such time has passed the kubelet actually deletes it from the store We receive an update for modification of the deletion timestamp and expect an rs to create more replicas asap not wait until the kubelet actually deletes the pod This is different from the Phase of a pod changing because 

until the kubelet actually deletes the pod This is different from the Phase of a pod changing because an rs never initiates a phase change and so is never asleep waiting for the same If the old and new ReplicaSet are the same the first one that syncs will set expectations preventing any damage from the second When a pod is deleted enqueue the replica set that manages the pod and update its expectations 
the Pod status which in turn will trigger a requeue of the owning replica set thus having its status updated with the newly available replica For now we can fake the update by resyncing the controller MinReadySeconds after the it is requeued because a Pod transitioned to Ready Note that this still suffers from 29229 we are just moving the problem one level 

until the kubelet actually deletes the pod This is different from the Phase of a pod changing because an rs never initiates a phase change and so is never asleep waiting for the same If the old and new ReplicaSet are the same the first one that syncs will set expectations preventing any damage from the second When a pod is deleted enqueue the replica set that manages the pod and update its expectations 
Two different versions of the same pod will always have different RVs when a pod is deleted gracefully its deletion timestamp is first modified to reflect a grace period and after such time has passed the kubelet actually deletes it from the store We receive an update 

for modification of the deletion timestamp and expect an rs to create more replicas asap not wait until the kubelet actually deletes the pod This is different from the Phase of a pod changing because an rs never initiates a phase change and so is never asleep waiting for the same If the old and new ReplicaSet are the same the first one that syncs will set expectations preventing any damage from the second When a pod is deleted enqueue the replica set that manages the pod and update its expectations 
If it has a ControllerRef thats all that matters TODO MinReadySeconds in the Pod will generate an Available condition to be added in the Pod status which in turn will trigger a requeue of the owning replica set thus having its status updated with the newly available replica For now we can fake the update by resyncing the controller MinReadySeconds after the it is requeued because 

for modification of the deletion timestamp and expect an rs to create more replicas asap not wait until the kubelet actually deletes the pod This is different from the Phase of a pod changing because an rs never initiates a phase change and so is never asleep waiting for the same If the old and new ReplicaSet are the same the first one that syncs will set expectations preventing any damage from the second 
TODO MinReadySeconds in the Pod will generate an Available condition to be added in the Pod status which in turn will trigger a requeue of the owning replica set thus having its status updated with the newly available replica For now we can fake the update by resyncing the controller MinReadySeconds after the it is requeued because a Pod transitioned to Ready Note that this still suffers from 29229 we are just moving the problem one level 

for modification of the deletion timestamp and expect an rs to create more replicas asap not wait until the kubelet actually deletes the pod This is different from the Phase of a pod changing because an rs never initiates a phase change and so is never asleep waiting for the same If the old and new ReplicaSet are the same the first one that syncs will set expectations preventing any damage from the second 
The ControllerRef was changed Sync the old controller if any If it has a ControllerRef thats all that matters TODO MinReadySeconds in the Pod will generate an Available condition to be added in the Pod status which in turn will trigger a requeue of the owning replica set thus having its status updated with the newly available replica For now we can fake the 

for modification of the deletion timestamp and expect an rs to create more replicas asap not wait until the kubelet actually deletes the pod This is different from the Phase of a pod changing because an rs never initiates a phase change and so is never asleep waiting for the same If the old and new ReplicaSet are the same the first one that syncs 
Two different versions of the same pod will always have different RVs when a pod is deleted gracefully its deletion timestamp is first modified to reflect a grace period and after such time has passed the kubelet actually deletes it from the store We receive an update 

for modification of the deletion timestamp and expect an rs to create more replicas asap not wait until the kubelet actually deletes the pod This is different from the Phase of a pod changing because 
It will requeue the replica set in case of an error while creatingdeleting pods TODO Track UIDs of creates just like deletes The problem currently is wed need to wait on the result of a create to record the pods 

and after such time has passed the kubelet actually deletes it from the store We receive an update for modification of the deletion timestamp and expect an rs to create more replicas asap not wait until the kubelet actually deletes the pod This is different from the Phase of a pod changing because an rs never initiates a phase change and so is never asleep waiting for the same If the old and new ReplicaSet are the same the first one that syncs will set expectations preventing any damage from the second 
the Pod status which in turn will trigger a requeue of the owning replica set thus having its status updated with the newly available replica For now we can fake the update by resyncing the controller MinReadySeconds after the it is requeued because a Pod transitioned to Ready Note that this still suffers from 29229 we are just moving the problem one level 

and after such time has passed the kubelet actually deletes it from the store We receive an update for modification of the deletion timestamp and expect an rs to create more replicas asap not wait until the kubelet actually deletes the pod This is different from the Phase of a pod changing because an rs never initiates a phase change and so is never asleep waiting for the same If the old and new ReplicaSet are the same the first one that syncs will set expectations preventing any damage from the second 
If it has a ControllerRef thats all that matters TODO MinReadySeconds in the Pod will generate an Available condition to be added in the Pod status which in turn will trigger a requeue of the owning replica set thus having its status updated with the newly available replica For now we can fake the update by resyncing the controller MinReadySeconds after the it is requeued because 

and after such time has passed the kubelet actually deletes it from the store We receive an update for modification of the deletion timestamp and expect an rs to create more replicas asap not wait until the kubelet actually deletes the pod This is different from the Phase of a pod changing because an rs never initiates a phase change and so is never asleep waiting for the same If the old and new ReplicaSet are the same the first one that syncs 
The ControllerRef was changed Sync the old controller if any If it has a ControllerRef thats all that matters TODO MinReadySeconds in the Pod will generate an Available condition to be added in the Pod status which in turn will trigger a requeue of the owning replica set thus having its status updated with the newly available replica For now we can fake the update by resyncing the controller MinReadySeconds after the it is requeued because 

when a pod is deleted gracefully its deletion timestamp is first modified to reflect a grace period and after such time has passed the kubelet actually deletes it from the store We receive an update for modification of the deletion timestamp and expect an rs to create more replicas asap not wait until the kubelet actually deletes the pod This is different from the Phase of a pod changing because an rs never initiates a phase change and so is never asleep waiting for the same If the old and new ReplicaSet are the same the first one that syncs 
When a pod is deleted enqueue the replica set that manages the pod and update its expectations obj could be an v1Pod or a DeletionFinalStateUnknown marker item When a delete is dropped the relist will notice a pod in the store not in the list leading to the insertion of a tombstone object which contains the deleted keyvalue Note that this value might be stale If the pod changed labels the new ReplicaSet will not be woken up till the periodic resync 

when a pod is deleted gracefully its deletion timestamp is first modified to reflect a grace period and after such time has passed the kubelet actually deletes it from the store We receive an update for modification of the deletion timestamp and expect an rs to create more replicas asap not wait until the kubelet actually deletes the pod This is different from the Phase of a pod changing because an rs never initiates a phase change and so is never asleep waiting for the same If the old and new ReplicaSet are the same the first one that syncs 
If it has a ControllerRef thats all that matters TODO MinReadySeconds in the Pod will generate an Available condition to be added in the Pod status which in turn will trigger a requeue of the owning replica set thus having its status updated with the newly available replica For now we can fake the update by resyncing the controller MinReadySeconds after the it is requeued because 

when a pod is deleted gracefully its deletion timestamp is first modified to reflect a grace period and after such time has passed the kubelet actually deletes it from the store We receive an update for modification of the deletion timestamp and expect an rs to create more replicas asap not wait until the kubelet actually deletes the pod This is different from the Phase of a pod changing because an rs never initiates a phase change and so is never asleep waiting for the same If the old and new ReplicaSet are the same the first one that syncs 
on a restart of the controller manager its possible a new pod shows up in a state that is already pending deletion Prevent the pod from being a creation observation If it has a ControllerRef thats all that matters Otherwise its an orphan Get a list of all matching ReplicaSets and sync them to see if anyone wants to adopt it DO NOT observe creation because no controller should be waiting for an 

when a pod is deleted gracefully its deletion timestamp is first modified to reflect a grace period and after such time has passed the kubelet actually deletes it from the store We receive an update for modification of the deletion timestamp and expect an rs to create more replicas asap not wait until the kubelet actually deletes the pod This is different from the Phase of a pod changing because 
If it has a ControllerRef thats all that matters TODO MinReadySeconds in the Pod will generate an Available condition to be added in the Pod status which in turn will trigger a requeue of the owning replica set thus having its status updated with the newly available replica For now we can fake the update by resyncing the controller MinReadySeconds after the it is requeued because a Pod transitioned to Ready 

and new replica set old and cur must be apiPod types A periodic relist will send update events for all known pods when a pod is deleted gracefully its deletion timestamp is first modified to reflect a grace period and after such time has passed the kubelet actually deletes it from the store We receive an update for modification of the deletion timestamp and expect an rs to create more replicas asap not wait until the kubelet actually deletes the pod This is different from the Phase of a pod changing because 
TODO MinReadySeconds in the Pod will generate an Available condition to be added in the Pod status which in turn will trigger a requeue of the owning replica set thus having its status updated with the newly available replica For now we can fake the update by resyncing the controller MinReadySeconds after the it is requeued because a Pod transitioned to Ready Note that this still suffers from 29229 we are just moving the problem one level 

and new replica set old and cur must be apiPod types A periodic relist will send update events for all known pods when a pod is deleted gracefully its deletion timestamp is first modified to reflect a grace period and after such time has passed the kubelet actually deletes it from the store We receive an update for modification of the deletion timestamp and expect an rs to create more replicas asap not wait until the kubelet actually deletes the pod This is different from the Phase of a pod changing because 
Controllers that need to be synced NewReplicaSetController configures a replica set controller with the specified event recorder NewBaseController is the implementation of NewReplicaSetController with additional injected parameters so that it can also serve as the implementation of NewReplicationController This invokes the ReplicaSet for every pod change eg host assignment Though this might seem like overkill the most frequent pod update is status and the associated ReplicaSet will only list from 

and new replica set old and cur must be apiPod types 
and new replica set old and cur must be v1Pod types 

up If the labels of the pod have changed we need to awaken both the old and new replica set old and cur must be apiPod types A periodic relist will send update events for all known pods when a pod is deleted gracefully its deletion timestamp is first modified to reflect a grace period and after such time has passed the kubelet actually deletes it from the store We receive an update for modification of the deletion timestamp and expect an rs to create more replicas asap not wait 
It will requeue the replica set in case of an error while creatingdeleting pods TODO Track UIDs of creates just like deletes The problem currently is wed need to wait on the result of a create to record the pods UID which would require locking across the create which will turn into a performance bottleneck We should generate a UID for the pod beforehand and store it via ExpectCreations 

on a restart of the controller manager its possible a new pod shows up in a state that is already pending deletion Prevent the pod from being a creation observation When a pod is updated figure out what replica sets manage it and wake them up If the labels of the pod have changed we need to awaken both the old and new replica set old and cur must be apiPod types A periodic relist will send update events for all known pods 
is wed need to wait on the result of a create to record the pods UID which would require locking across the create which will turn into a performance bottleneck We should generate a UID for the pod beforehand and store it via ExpectCreations Batch the pod creates Batch sizes start at SlowStartInitialBatchSize and double with each successful iteration in a kind of slow start 

When a pod is created enqueue the replica set that manages it and update its expectations 
When a pod is deleted enqueue the replica set that manages the pod and update its expectations 

If a ReplicaSet with a nil or empty selector creeps in it should match nothing not everything When a pod is created enqueue the replica set that manages it and update its expectations on a restart of the controller manager its possible a new pod shows up in a state that is already pending deletion Prevent the pod from being a creation observation When a pod is updated figure out what replica sets manage it and wake them up If the labels of the pod have changed we need to awaken both the old 
When a delete is dropped the relist will notice a pod in the store not in the list leading to the insertion of a tombstone object which contains the deleted keyvalue Note that this value might be stale If the pod changed labels the new ReplicaSet will not be woken up till the periodic resync No controller should care about orphans being deleted worker runs a worker thread that just dequeues items processes them and marks them done 

Note that deleting a replica set immediately after scaling it to 0 will not work The recommended way of achieving this is by performing a stop operation on the replica set This invokes the ReplicaSet for every pod change eg host assignment Though this might seem like overkill the most frequent pod update is status and the associated ReplicaSet will only list from 
Periodic resync will send update events for all known pods Two different versions of the same pod will always have different RVs when a pod is deleted gracefully its deletion timestamp is first modified to reflect a grace period and after such time has passed the kubelet actually deletes it from the store We receive an update for modification of the deletion timestamp and expect an rs to create more replicas asap not wait until the kubelet actually deletes the pod This is different from the Phase of a pod changing because 

This will enter the sync loop and noop because the replica set has been deleted from the store Note that deleting a replica set immediately after scaling it to 0 will not work The recommended way of achieving this is by performing a stop operation on the replica set This invokes the ReplicaSet for every pod change eg host assignment Though this might seem like overkill the most frequent pod update is status and the associated ReplicaSet will only list from local storage so it should be ok 
and new replica set old and cur must be v1Pod types Periodic resync will send update events for all known pods Two different versions of the same pod will always have different RVs when a pod is deleted gracefully its deletion timestamp is first modified to reflect a grace period and after such time has passed the kubelet actually deletes it from the store We receive an update 

This will enter the sync loop and noop because the replica set has been deleted from the store Note that deleting a replica set immediately after scaling it to 0 will not work The recommended way of achieving this is by performing a stop operation on the replica set This invokes the ReplicaSet for every pod change eg host assignment Though this might seem like overkill the most frequent pod update is status and the associated ReplicaSet will only list from 
the Pod status which in turn will trigger a requeue of the owning replica set thus having its status updated with the newly available replica For now we can fake the update by resyncing the controller MinReadySeconds after the it is requeued because a Pod transitioned to Ready Note that this still suffers from 29229 we are just moving the problem one level 

in such case we must invalidate the whole cache so that pod could be adopted by RS1 This makes the lookup cache less helpful but selector update does not happen often so its not a big problem You might imagine that we only really need to enqueue the replica set when Spec changes but it is safer to sync any time this function is triggered That way a full informer 
an rs never initiates a phase change and so is never asleep waiting for the same we dont need to check the oldPodDeletionTimestamp because DeletionTimestamp cannot be unset The ControllerRef was changed Sync the old controller if any If it has a ControllerRef thats all that matters 

podStoreSynced returns true if the pod store has been synced at least once 
podListerSynced returns true if the pod store has been synced at least once 

podStoreSynced returns true if the pod store has been synced at least once 
rsListerSynced returns true if the pod store has been synced at least once 

otherwise the controller will not be able to sync this service again until it is restarted Sleep so we give the pod reflector goroutine a chance to run Delete the corresponding endpoint as the service has been deleted TODO Please note that this will delete an endpoint when a service is deleted However if were down at the time when 
if the family was incorrectly identified then this will be corrected once the the upgrade is completed controller connects to apiserver that correctly defaults services find an ip that matches the family When a pod is updated figure out what services it used to be a member of 

panic wed still like this to get marked done otherwise the controller will not be able to sync this service again until it is restarted Sleep so we give the pod reflector goroutine a chance to run Delete the corresponding endpoint as the service has been deleted 
Since were getting stuff from a local cache it is basically impossible to get this error We call ComputeEndpointLastChangeTriggerTime here to make sure that the state of the trigger time tracker gets updated even if the sync turns out to be noop and we dont update the endpoints object 

obj could be an apiService or a DeletionFinalStateUnknown marker item 
obj could be an v1Pod or a DeletionFinalStateUnknown marker item 

Enqueue all the services that the pod used to be a member of This happens to be exactly the same thing we do when a pod is added TODO keep a map of pods to services to handle this condition obj could be an apiService or a DeletionFinalStateUnknown marker item worker runs a worker thread that just dequeues items processes them and 
the upgrade is completed controller connects to apiserver that correctly defaults services find an ip that matches the family When a pod is updated figure out what services it used to be a member of 

Enqueue all the services that the pod used to be a member of This happens to be exactly the same thing we do when a pod is added TODO keep a map of pods to services to handle this condition obj could be an apiService or a DeletionFinalStateUnknown marker item 
if the family was incorrectly identified then this will be corrected once the the upgrade is completed controller connects to apiserver that correctly defaults services find an ip that matches the family When a pod is updated figure out what services it used to be a member of 

obj could be an apiPod or a DeletionFinalStateUnknown marker item Enqueue all the services that the pod used to be a member of This happens to be exactly the same thing we do when a pod is added 
if the family was incorrectly identified then this will be corrected once the the upgrade is completed controller connects to apiserver that correctly defaults services find an ip that matches the family When a pod is updated figure out what services it used to be a member of 

obj could be an apiPod or a DeletionFinalStateUnknown marker item 
obj could be an v1Pod or a DeletionFinalStateUnknown marker item 

When a pod is deleted enqueue the services the pod used to be a member of obj could be an apiPod or a DeletionFinalStateUnknown marker item Enqueue all the services that the pod used to be a member of This happens to be exactly the same thing we do when a pod is added 
find an ip that matches the family When a pod is updated figure out what services it used to be a member of 

When a pod is deleted enqueue the services the pod used to be a member of obj could be an apiPod or a DeletionFinalStateUnknown marker item Enqueue all the services that the pod used to be a member of This happens to be exactly the same thing we do when a 
find an ip that matches the family When a pod is updated figure out what services it used to be a member of and what services it will be a member of and enqueue the union of these old and cur must be v1Pod types 

When a pod is deleted enqueue the services the pod used to be a member of obj could be an apiPod or a DeletionFinalStateUnknown marker item Enqueue all the services that the pod used to be a member of This happens to be exactly the same thing we do when a 
if the family was incorrectly identified then this will be corrected once the the upgrade is completed controller connects to apiserver that correctly defaults services find an ip that matches the family When a pod is updated figure out what services it used to be a member of 

Only need to get the old services if the labels changed When a pod is deleted enqueue the services the pod used to be a member of obj could be an apiPod or a DeletionFinalStateUnknown marker item Enqueue all the services that the pod used to be a member 
the upgrade is completed controller connects to apiserver that correctly defaults services find an ip that matches the family When a pod is updated figure out what services it used to be a member of and what services it will be a member of and enqueue the union of these old and cur must be v1Pod types 

Only need to get the old services if the labels changed When a pod is deleted enqueue the services the pod used to be a member of obj could be an apiPod or a DeletionFinalStateUnknown marker item Enqueue all the services that the pod used to be a member 
if the family was incorrectly identified then this will be corrected once the the upgrade is completed controller connects to apiserver that correctly defaults services find an ip that matches the family When a pod is updated figure out what services it used to be a member of 

old and cur must be apiPod types 
old and cur must be v1Pod types 

When a pod is updated figure out what services it used to be a member of 
When a pod is added figure out what services it will be a member of and 

enqueue them obj must have apiPod type 
enqueue them obj must have v1Pod type 

When a pod is added figure out what services it will be a member of and 
When a pod is updated figure out what services it used to be a member of 

podStoreSynced returns true if the pod store has been synced at least once 
podsSynced returns true if the pod shared informer has been synced at least once 

NewEndpointController returns a new EndpointController 
NewEndpointController returns a new Controller 

v1 matches 
v0 matches v1 mismatches 

v0 matches v1 mismatches 
v1 matches 

Chuck cant access things with no resource or namespace 
Chunk cant access things with no kind or namespace 

Chunk cant access things with no kind or namespace 
Chuck cant access things with no resource or namespace 

authenticator identifies the user for requests to the Kubelet API 
GetRequestAttributes populates authorizer attributes for the requests to the kubelet API 

Scheme is the default instance of runtimeScheme to which types in the abac API group are registered 
Scheme is the default instance of runtimeScheme to which types in the abac API group are apiRegistry 

Group is the API group for abac 
GroupName is the API group for abac 

In v0 unspecified user and group matches all subjects 
In v0 unspecified user and group matches all authenticated subjects 

Delete pod1 
Delete pod2 

Delete pod2 
Delete pod1 

Delete pod2 
Delete pod1 

Delete pod1 
Delete pod2 

Put 2 ReplicaSets and one pod into the controllers stores 
Put 2 ReplicaSets and one pod into the informers 

Start only the pod watcher and the workqueue send a watch event 
Start only the ReplicaSet watcher and the workqueue send a watch event 

and make sure it hits the sync method Put one ReplicaSet and one pod into the controllers stores The pod update sent through the fakeWatcher should figure out the managing ReplicaSet and 
send it into the syncHandler Start only the pod watcher and the workqueue send a watch event and make sure it hits the sync method for the right ReplicaSet Put 2 ReplicaSets and one pod into the informers 

Start only the ReplicaSet watcher and the workqueue send a watch event 
Start only the pod watcher and the workqueue send a watch event 

Matching ns and labels returns the key to the ReplicaSet not the ReplicaSet name The update sent through the fakeWatcher should make its way into the workqueue and eventually into the syncHandler The handler validates the received controller 
The DeletedFinalStateUnknown object should cause the ReplicaSet manager to insert the controller matching the selectors of the deleted pod into the work queue 

1 PUT for the ReplicaSet status during dormancy window 
2 PUT for the ReplicaSet status during dormancy window 

The DeletedFinalStateUnknown object should cause the ReplicaSet manager to insert the controller matching the selectors of the deleted pod into the work queue 
Matching ns and labels returns the key to the ReplicaSet not the ReplicaSet name The update sent through the fakeWatcher should make its way into the workqueue and eventually into the syncHandler The handler validates the received controller 

Otherwise it returns nil The caller should acquire the lock Status is not cached but the global timestamp is newer than minTime return the default status Status is cached return status if either of the following is true status was modified after minTime the global timestamp of the cache is newer than minTime 
cache entries GetNewerThan is a blocking call that only returns the status when it is newer than the given time Status of the pod 

getIfNewerThan returns the data it is newer than the given time Otherwise it returns nil The caller should acquire the lock Status is not cached but the global timestamp is newer than 
cache entries GetNewerThan is a blocking call that only returns the status when it is newer than the given time 

PodStatus to reflect this getIfNewerThan returns the data it is newer than the given time Otherwise it returns nil The caller should acquire the lock Status is not cached but the global timestamp is newer than 
cache content is at the least newer than this timestamp Note that the timestamp is nil after initialization and will only become nonnil when it is ready to serve the cached statuses 

it is ready to serve the cached statuses Map that stores the subscriber records NewCache creates a pod cache Get returns the PodStatus for the pod callers are expected not to modify the objects returned 
Cache provides two methods to retrieve the PodStatus the nonblocking Get and the blocking GetNewerThan method The component responsible for populating the cache is expected to call Delete to explicitly free the cache entries GetNewerThan is a blocking call that only returns the status 

it is ready to serve the cached statuses Map that stores the subscriber records NewCache creates a pod cache Get returns the PodStatus for the pod callers are expected not to 
populating the cache is expected to call Delete to explicitly free the cache entries GetNewerThan is a blocking call that only returns the status when it is newer than the given time Status of the pod Error got when trying to inspect the pod 

it is ready to serve the cached statuses Map that stores the subscriber records NewCache creates a pod cache Get returns the PodStatus for the pod callers are expected not to 
Cache provides two methods to retrieve the PodStatus the nonblocking Get and the blocking GetNewerThan method The component responsible for populating the cache is expected to call Delete to explicitly free the cache entries GetNewerThan is a blocking call that only returns the status when it is newer than the given time 

timestamp is nil after initialization and will only become nonnil when it is ready to serve the cached statuses Map that stores the subscriber records NewCache creates a pod cache Get returns the PodStatus for the pod callers are expected not to modify the objects returned 
and the blocking GetNewerThan method The component responsible for populating the cache is expected to call Delete to explicitly free the cache entries GetNewerThan is a blocking call that only returns the status 

timestamp is nil after initialization and will only become nonnil when it is ready to serve the cached statuses Map that stores the subscriber records NewCache creates a pod cache 
populating the cache is expected to call Delete to explicitly free the cache entries GetNewerThan is a blocking call that only returns the status when it is newer than the given time 

cache content is at the least newer than this timestamp Note that the timestamp is nil after initialization and will only become nonnil when it is ready to serve the cached statuses 
PodStatus to reflect this getIfNewerThan returns the data it is newer than the given time Otherwise it returns nil The caller should acquire the lock Status is not cached but the global timestamp is newer than 

cache content is at the least newer than this timestamp Note that the timestamp is nil after initialization and will only become nonnil when it is ready to serve the cached statuses 
populating the cache is expected to call Delete to explicitly free the cache entries GetNewerThan is a blocking call that only returns the status when it is newer than the given time 

Time when the data was last modfied 
Time when the data was last modified 

cache entries GetNewerThan is a blocking call that only returns the status when it is newer than the given time Status of the pod 
Otherwise it returns nil The caller should acquire the lock Status is not cached but the global timestamp is newer than minTime return the default status Status is cached return status if either of the following is true status was modified after minTime the global timestamp of the cache is newer than minTime 

cache entries GetNewerThan is a blocking call that only returns the status when it is newer than the given time 
getIfNewerThan returns the data it is newer than the given time Otherwise it returns nil The caller should acquire the lock Status is not cached but the global timestamp is newer than 

populating the cache is expected to call Delete to explicitly free the cache entries GetNewerThan is a blocking call that only returns the status when it is newer than the given time Status of the pod Error got when trying to inspect the pod 
it is ready to serve the cached statuses Map that stores the subscriber records NewCache creates a pod cache Get returns the PodStatus for the pod callers are expected not to 

populating the cache is expected to call Delete to explicitly free the cache entries GetNewerThan is a blocking call that only returns the status when it is newer than the given time 
timestamp is nil after initialization and will only become nonnil when it is ready to serve the cached statuses Map that stores the subscriber records NewCache creates a pod cache 

populating the cache is expected to call Delete to explicitly free the cache entries GetNewerThan is a blocking call that only returns the status when it is newer than the given time 
cache content is at the least newer than this timestamp Note that the timestamp is nil after initialization and will only become nonnil when it is ready to serve the cached statuses 

and the blocking GetNewerThan method The component responsible for populating the cache is expected to call Delete to explicitly free the cache entries GetNewerThan is a blocking call that only returns the status 
timestamp is nil after initialization and will only become nonnil when it is ready to serve the cached statuses Map that stores the subscriber records NewCache creates a pod cache Get returns the PodStatus for the pod callers are expected not to modify the objects returned 

Cache provides two methods to retrive the PodStatus the nonblocking Get and the blocking GetNewerThan method The component responsible for populating the cache is expected to call Delete to explicitly free the cache entries GetNewerThan is a blocking call that only returns the status when it is newer than the given time 
it is ready to serve the cached statuses Map that stores the subscriber records NewCache creates a pod cache Get returns the PodStatus for the pod callers are expected not to 

Cache provides two methods to retrive the PodStatus the nonblocking Get and the blocking GetNewerThan method The component responsible for populating the cache is expected to call Delete to explicitly free the cache entries GetNewerThan is a blocking call that only returns the status 
it is ready to serve the cached statuses Map that stores the subscriber records NewCache creates a pod cache Get returns the PodStatus for the pod callers are expected not to modify the objects returned 

Cache provides two methods to retrive the PodStatus the nonblocking Get 
Cache provides two methods to retrieve the PodStatus the nonblocking Get 

this func blocks until either there are no remaining free ports or else the stopCh chan is closed nextFreePort finds a free port first picking a random port if that port is already in use 
then a random port allocator is returned otherwise a new rangebased allocator is returned fillPorts loops always searching for the next free port and if found fills the ports buffer with it this func blocks unless there are no remaining free ports 

then a random port allocator is returned otherwise a new rangebased allocator is returned fillPorts loops always searching for the next free port and if found fills the ports buffer with it this func blocks until either there are no remaining free ports or else the stopCh chan is closed 
this func blocks unless there are no remaining free ports nextFreePort finds a free port first picking a random port if that port is already in use then the port range is scanned sequentially until either a port is found or the scan completes 

3  podSpecActiveDeadlineSeconds gives the recycler pod a maximum timeout before failing  Recommended  Default is 60 seconds 
3 podSpecActiveDeadlineSeconds gives the recycler pod a maximum timeout before failing  Recommended  Default is 60 seconds 

2  podGenerateName helps distinguish recycler pods by name  Recommended Default is pvrecycler 
2 podGenerateName helps distinguish recycler pods by name  Recommended Default is pvrecycler 

1  podSpecVolumes0VolumeSource must be overridden  Recycler implementations without a valid VolumeSource will fail 
1 podSpecVolumes0VolumeSource must be overridden  Recycler implementations without a valid VolumeSource will fail 

does not return error if no plugin is found  All volumes require a builder and cleaner but not every volume will have an attacherdetacher NewPersistentVolumeRecyclerPodTemplate creates a template for a recycler pod  By default a recycler pod simply runs rm rf on a volume and tests for emptiness  Most attributes of the template will be correct for most 
FindPersistentPluginBySpec looks for a persistent volume plugin that can support a given volume specification  If no plugin is found return an error FindVolumePluginWithLimitsBySpec returns volume plugin that has a limit on how many of them can be attached to a node 

does not return error if no plugin is found  All volumes require a builder and cleaner but not every volume will 
Unlike the other FindPlugin methods this does not return error if no plugin is found  All volumes require a mounter and unmounter but not 

does not return error if no plugin is found  All volumes require a builder and cleaner but not every volume will 
Unlike the other FindPlugin methods this does not return error if no plugin is found  All volumes require a mounter and unmounter but not 

FindAttachablePluginByName fetches an attachable volume plugin by name Unlike the other FindPlugin methods this does not return error if no plugin is found  All volumes require a builder and cleaner but not every volume will have an attacherdetacher 
every volume will have an attacherdetacher FindAttachablePluginByName fetches an attachable volume plugin by name Unlike the other FindPlugin methods this does not return error if no 

FindAttachablePluginByName fetches an attachable volume plugin by name Unlike the other FindPlugin methods this does not return error if no plugin is found  All volumes require a builder and cleaner but not every volume will 
plugin is found  All volumes require a mounter and unmounter but not every volume will have an attacherdetacher FindAttachablePluginByName fetches an attachable volume plugin by name Unlike the other FindPlugin methods this does not return error if no 

have an attacherdetacher FindAttachablePluginByName fetches an attachable volume plugin by name Unlike the other FindPlugin methods this does not return error if no plugin is found  All volumes require a builder and cleaner but not every volume will 
every volume will have an attacherdetacher FindAttachablePluginByName fetches an attachable volume plugin by name Unlike the other FindPlugin methods this does not return error if no 

does not return error if no plugin is found  All volumes require a builder and cleaner but not every volume will have an attacherdetacher FindAttachablePluginByName fetches an attachable volume plugin by name Unlike the other FindPlugin methods this 
every volume will have an attacherdetacher FindAttachablePluginByName fetches an attachable volume plugin by name Unlike the other FindPlugin methods this does not return error if no 

does not return error if no plugin is found  All volumes require a builder and cleaner but not every volume will 
Unlike the other FindPlugin methods this does not return error if no plugin is found  All volumes require a mounter and unmounter but not 

does not return error if no plugin is found  All volumes require a builder and cleaner but not every volume will 
Unlike the other FindPlugin methods this does not return error if no plugin is found  All volumes require a mounter and unmounter but not 

FindAttachablePluginBySpec fetches a persistent volume plugin by name  Unlike the other FindPlugin methods this does not return error if no plugin is found  All volumes require a builder and cleaner but not every volume will have an attacherdetacher 
plugin is found  All volumes require a mounter and unmounter but not every volume will have an attacherdetacher FindAttachablePluginByName fetches an attachable volume plugin by name Unlike the other FindPlugin methods this does not return error if no 

FindAttachablePluginBySpec fetches a persistent volume plugin by name  Unlike the other FindPlugin methods this does not return error if no plugin is found  All volumes require a builder and cleaner but not every volume will 
FindAttachablePluginBySpec fetches a persistent volume plugin by spec Unlike the other FindPlugin methods this does not return error if no 

FindCreatablePluginBySpec fetches a persistent volume plugin by name  If no plugin is found returns error 
no plugin is found returns error FindCreatablePluginBySpec fetches a persistent volume plugin by name  If 

FindCreatablePluginBySpec fetches a persistent volume plugin by name  If no plugin is found returns error 
FindDeletablePluginByName fetches a persistent volume plugin by name  If no plugin is found returns error 

FindCreatablePluginBySpec fetches a persistent volume plugin by name  If no plugin is found returns error 
no plugin is found returns error FindDeletablePluginByName fetches a persistent volume plugin by name  If 

FindCreatablePluginBySpec fetches a persistent volume plugin by name  If no plugin is found returns error 
FindDeletablePluginBySpec fetches a persistent volume plugin by spec  If no plugin is found returns error 

FindCreatablePluginBySpec fetches a persistent volume plugin by name  If no plugin is found returns error 
no plugin is found returns error FindDeletablePluginBySpec fetches a persistent volume plugin by spec  If 

FindCreatablePluginBySpec fetches a persistent volume plugin by name  If no plugin is found returns error 
FindProvisionablePluginByName fetches  a persistent volume plugin by name  If no plugin is found returns error 

FindCreatablePluginBySpec fetches a persistent volume plugin by name  If no plugin is found returns error 
no plugin is found returns error FindProvisionablePluginByName fetches  a persistent volume plugin by name  If 

FindCreatablePluginBySpec fetches a persistent volume plugin by name  If no plugin is found returns error 
FindRecyclablePluginByName fetches a persistent volume plugin by name  If no plugin is found returns error 

FindCreatablePluginBySpec fetches a persistent volume plugin by name  If no plugin is found returns error 
no plugin is found returns error FindRecyclablePluginByName fetches a persistent volume plugin by name  If 

FindCreatablePluginBySpec fetches a persistent volume plugin by name  If no plugin is found returns error 
FindPersistentPluginByName fetches a persistent volume plugin by name  If no plugin is found returns error 

is found returns error FindCreatablePluginBySpec fetches a persistent volume plugin by name  If no plugin 
FindCreatablePluginBySpec fetches a persistent volume plugin by name  If no plugin is found returns error 

is found returns error FindCreatablePluginBySpec fetches a persistent volume plugin by name  If no plugin 
no plugin is found returns error FindCreatablePluginBySpec fetches a persistent volume plugin by name  If 

is found returns error FindCreatablePluginBySpec fetches a persistent volume plugin by name  If no plugin 
FindDeletablePluginByName fetches a persistent volume plugin by name  If no plugin is found returns error 

is found returns error FindCreatablePluginBySpec fetches a persistent volume plugin by name  If no plugin 
no plugin is found returns error FindDeletablePluginByName fetches a persistent volume plugin by name  If 

is found returns error FindCreatablePluginBySpec fetches a persistent volume plugin by name  If no plugin 
FindDeletablePluginBySpec fetches a persistent volume plugin by spec  If no plugin is found returns error 

is found returns error FindCreatablePluginBySpec fetches a persistent volume plugin by name  If no plugin 
no plugin is found returns error FindDeletablePluginBySpec fetches a persistent volume plugin by spec  If 

is found returns error FindCreatablePluginBySpec fetches a persistent volume plugin by name  If no plugin 
FindProvisionablePluginByName fetches  a persistent volume plugin by name  If no plugin is found returns error 

is found returns error FindCreatablePluginBySpec fetches a persistent volume plugin by name  If no plugin 
no plugin is found returns error FindProvisionablePluginByName fetches  a persistent volume plugin by name  If 

is found returns error FindCreatablePluginBySpec fetches a persistent volume plugin by name  If no plugin 
FindRecyclablePluginByName fetches a persistent volume plugin by name  If no plugin is found returns error 

is found returns error FindCreatablePluginBySpec fetches a persistent volume plugin by name  If no plugin 
no plugin is found returns error FindRecyclablePluginByName fetches a persistent volume plugin by name  If 

is found returns error FindCreatablePluginBySpec fetches a persistent volume plugin by name  If no plugin 
FindPersistentPluginByName fetches a persistent volume plugin by name  If no plugin is found returns error 

FindDeletablePluginByName fetches a persistent volume plugin by name  If no plugin is found returns error 
FindCreatablePluginBySpec fetches a persistent volume plugin by name  If no plugin is found returns error 

FindDeletablePluginByName fetches a persistent volume plugin by name  If no plugin is found returns error 
no plugin is found returns error FindCreatablePluginBySpec fetches a persistent volume plugin by name  If 

FindDeletablePluginByName fetches a persistent volume plugin by name  If no plugin is found returns error 
no plugin is found returns error FindDeletablePluginByName fetches a persistent volume plugin by name  If 

FindDeletablePluginByName fetches a persistent volume plugin by name  If no plugin is found returns error 
FindDeletablePluginBySpec fetches a persistent volume plugin by spec  If no plugin is found returns error 

FindDeletablePluginByName fetches a persistent volume plugin by name  If no plugin is found returns error 
no plugin is found returns error FindDeletablePluginBySpec fetches a persistent volume plugin by spec  If 

FindDeletablePluginByName fetches a persistent volume plugin by name  If no plugin is found returns error 
FindProvisionablePluginByName fetches  a persistent volume plugin by name  If no plugin is found returns error 

FindDeletablePluginByName fetches a persistent volume plugin by name  If no plugin is found returns error 
no plugin is found returns error FindProvisionablePluginByName fetches  a persistent volume plugin by name  If 

FindDeletablePluginByName fetches a persistent volume plugin by name  If no plugin is found returns error 
FindRecyclablePluginByName fetches a persistent volume plugin by name  If no plugin is found returns error 

FindDeletablePluginByName fetches a persistent volume plugin by name  If no plugin is found returns error 
no plugin is found returns error FindRecyclablePluginByName fetches a persistent volume plugin by name  If 

FindDeletablePluginByName fetches a persistent volume plugin by name  If no plugin is found returns error 
FindPersistentPluginByName fetches a persistent volume plugin by name  If no plugin is found returns error 

is found returns error FindDeletablePluginByName fetches a persistent volume plugin by name  If no plugin 
FindCreatablePluginBySpec fetches a persistent volume plugin by name  If no plugin is found returns error 

is found returns error FindDeletablePluginByName fetches a persistent volume plugin by name  If no plugin 
no plugin is found returns error FindCreatablePluginBySpec fetches a persistent volume plugin by name  If 

is found returns error FindDeletablePluginByName fetches a persistent volume plugin by name  If no plugin 
FindDeletablePluginByName fetches a persistent volume plugin by name  If no plugin is found returns error 

is found returns error FindDeletablePluginByName fetches a persistent volume plugin by name  If no plugin 
no plugin is found returns error FindDeletablePluginByName fetches a persistent volume plugin by name  If 

is found returns error FindDeletablePluginByName fetches a persistent volume plugin by name  If no plugin 
FindDeletablePluginBySpec fetches a persistent volume plugin by spec  If no plugin is found returns error 

is found returns error FindDeletablePluginByName fetches a persistent volume plugin by name  If no plugin 
no plugin is found returns error FindDeletablePluginBySpec fetches a persistent volume plugin by spec  If 

is found returns error FindDeletablePluginByName fetches a persistent volume plugin by name  If no plugin 
FindProvisionablePluginByName fetches  a persistent volume plugin by name  If no plugin is found returns error 

is found returns error FindDeletablePluginByName fetches a persistent volume plugin by name  If no plugin 
no plugin is found returns error FindProvisionablePluginByName fetches  a persistent volume plugin by name  If 

is found returns error FindDeletablePluginByName fetches a persistent volume plugin by name  If no plugin 
FindRecyclablePluginByName fetches a persistent volume plugin by name  If no plugin is found returns error 

is found returns error FindDeletablePluginByName fetches a persistent volume plugin by name  If no plugin 
no plugin is found returns error FindRecyclablePluginByName fetches a persistent volume plugin by name  If 

is found returns error FindDeletablePluginByName fetches a persistent volume plugin by name  If no plugin 
FindPersistentPluginByName fetches a persistent volume plugin by name  If no plugin is found returns error 

FindRecyclablePluginByName fetches a persistent volume plugin by name  If no plugin is found returns error 
FindCreatablePluginBySpec fetches a persistent volume plugin by name  If no plugin is found returns error 

FindRecyclablePluginByName fetches a persistent volume plugin by name  If no plugin is found returns error 
no plugin is found returns error FindCreatablePluginBySpec fetches a persistent volume plugin by name  If 

FindRecyclablePluginByName fetches a persistent volume plugin by name  If no plugin is found returns error 
FindDeletablePluginByName fetches a persistent volume plugin by name  If no plugin is found returns error 

FindRecyclablePluginByName fetches a persistent volume plugin by name  If no plugin is found returns error 
no plugin is found returns error FindDeletablePluginByName fetches a persistent volume plugin by name  If 

FindRecyclablePluginByName fetches a persistent volume plugin by name  If no plugin is found returns error 
FindDeletablePluginBySpec fetches a persistent volume plugin by spec  If no plugin is found returns error 

FindRecyclablePluginByName fetches a persistent volume plugin by name  If no plugin is found returns error 
no plugin is found returns error FindDeletablePluginBySpec fetches a persistent volume plugin by spec  If 

FindRecyclablePluginByName fetches a persistent volume plugin by name  If no plugin is found returns error 
FindProvisionablePluginByName fetches  a persistent volume plugin by name  If no plugin is found returns error 

FindRecyclablePluginByName fetches a persistent volume plugin by name  If no plugin is found returns error 
no plugin is found returns error FindProvisionablePluginByName fetches  a persistent volume plugin by name  If 

FindRecyclablePluginByName fetches a persistent volume plugin by name  If no plugin is found returns error 
no plugin is found returns error FindRecyclablePluginByName fetches a persistent volume plugin by name  If 

FindRecyclablePluginByName fetches a persistent volume plugin by name  If no plugin is found returns error 
FindPersistentPluginByName fetches a persistent volume plugin by name  If no plugin is found returns error 

is found returns error FindRecyclablePluginByName fetches a persistent volume plugin by name  If no plugin 
FindCreatablePluginBySpec fetches a persistent volume plugin by name  If no plugin is found returns error 

is found returns error FindRecyclablePluginByName fetches a persistent volume plugin by name  If no plugin 
no plugin is found returns error FindCreatablePluginBySpec fetches a persistent volume plugin by name  If 

is found returns error FindRecyclablePluginByName fetches a persistent volume plugin by name  If no plugin 
FindDeletablePluginByName fetches a persistent volume plugin by name  If no plugin is found returns error 

is found returns error FindRecyclablePluginByName fetches a persistent volume plugin by name  If no plugin 
no plugin is found returns error FindDeletablePluginByName fetches a persistent volume plugin by name  If 

is found returns error FindRecyclablePluginByName fetches a persistent volume plugin by name  If no plugin 
FindDeletablePluginBySpec fetches a persistent volume plugin by spec  If no plugin is found returns error 

is found returns error FindRecyclablePluginByName fetches a persistent volume plugin by name  If no plugin 
no plugin is found returns error FindDeletablePluginBySpec fetches a persistent volume plugin by spec  If 

is found returns error FindRecyclablePluginByName fetches a persistent volume plugin by name  If no plugin 
FindProvisionablePluginByName fetches  a persistent volume plugin by name  If no plugin is found returns error 

is found returns error FindRecyclablePluginByName fetches a persistent volume plugin by name  If no plugin 
no plugin is found returns error FindProvisionablePluginByName fetches  a persistent volume plugin by name  If 

is found returns error FindRecyclablePluginByName fetches a persistent volume plugin by name  If no plugin 
FindRecyclablePluginByName fetches a persistent volume plugin by name  If no plugin is found returns error 

is found returns error FindRecyclablePluginByName fetches a persistent volume plugin by name  If no plugin 
no plugin is found returns error FindRecyclablePluginByName fetches a persistent volume plugin by name  If 

is found returns error FindRecyclablePluginByName fetches a persistent volume plugin by name  If no plugin 
FindPersistentPluginByName fetches a persistent volume plugin by name  If no plugin is found returns error 

FindPersistentPluginByName fetches a persistent volume plugin by name  If no plugin is found returns error 
FindCreatablePluginBySpec fetches a persistent volume plugin by name  If no plugin is found returns error 

FindPersistentPluginByName fetches a persistent volume plugin by name  If no plugin is found returns error 
no plugin is found returns error FindCreatablePluginBySpec fetches a persistent volume plugin by name  If 

FindPersistentPluginByName fetches a persistent volume plugin by name  If no plugin is found returns error 
FindDeletablePluginByName fetches a persistent volume plugin by name  If no plugin is found returns error 

FindPersistentPluginByName fetches a persistent volume plugin by name  If no plugin is found returns error 
no plugin is found returns error FindDeletablePluginByName fetches a persistent volume plugin by name  If 

FindPersistentPluginByName fetches a persistent volume plugin by name  If no plugin is found returns error 
FindDeletablePluginBySpec fetches a persistent volume plugin by spec  If no plugin is found returns error 

FindPersistentPluginByName fetches a persistent volume plugin by name  If no plugin is found returns error 
no plugin is found returns error FindDeletablePluginBySpec fetches a persistent volume plugin by spec  If 

FindPersistentPluginByName fetches a persistent volume plugin by name  If no plugin is found returns error 
FindProvisionablePluginByName fetches  a persistent volume plugin by name  If no plugin is found returns error 

FindPersistentPluginByName fetches a persistent volume plugin by name  If no plugin is found returns error 
no plugin is found returns error FindProvisionablePluginByName fetches  a persistent volume plugin by name  If 

FindPersistentPluginByName fetches a persistent volume plugin by name  If no plugin is found returns error 
FindRecyclablePluginByName fetches a persistent volume plugin by name  If no plugin is found returns error 

FindPersistentPluginByName fetches a persistent volume plugin by name  If no plugin is found returns error 
no plugin is found returns error FindRecyclablePluginByName fetches a persistent volume plugin by name  If 

FindPersistentPluginBySpec looks for a persistent volume plugin that can support a given volume 
FindPluginBySpec looks for a plugin that can support a given volume 

FindPluginByName fetches a plugin by name or by legacy name  If no plugin is found returns error Once we can get rid of legacy names we can reduce this to a map lookup FindPersistentPluginBySpec looks for a persistent volume plugin that can support a given volume specification  If no plugin is found return an error FindPersistentPluginByName fetches a persistent volume plugin by name  If no plugin 
of them can be attached to a node FindPersistentPluginByName fetches a persistent volume plugin by name  If no plugin is found returns error 

plugins FindPluginBySpec looks for a plugin that can support a given volume 
FindPersistentPluginBySpec looks for a persistent volume plugin that can 

NewSpecFromPersistentVolume creates an Spec from an apiPersistentVolume 
NewSpecFromPersistentVolume creates an Spec from an v1PersistentVolume 

NewSpecFromPersistentVolume creates an Spec from an apiPersistentVolume 
NewSpecFromVolume creates an Spec from an v1Volume 

NewSpecFromVolume creates an Spec from an apiVolume 
NewSpecFromPersistentVolume creates an Spec from an v1PersistentVolume 

NewSpecFromVolume creates an Spec from an apiVolume 
NewSpecFromVolume creates an Spec from an v1Volume 

Gi of capacity in the persistent volume Example 5Gi volume x 30s increment  150s  30s minimum  180s ActiveDeadlineSeconds for recycler pod 
pods ActiveDeadlineSeconds for each Gi of capacity in the persistent volume Example 5Gi volume x 30s increment  150s  30s minimum  180s 

RecyclerTimeoutIncrement is the number of seconds added to the recycler pods ActiveDeadlineSeconds for each Gi of capacity in the persistent volume 
RecyclerMinimumTimeout is the minimum amount of time in seconds for the recycler pods ActiveDeadlineSeconds attribute Added to the minimum timeout is the increment per Gi of capacity 

Added to the minimum timeout is the increment per Gi of capacity 
timeout is the increment per Gi of capacity RecyclerTimeoutIncrement is the number of seconds added to the recycler 

The template is used by plugins which override specific properties of the pod in accordance with that plugin 
a persistent volume after its release The template is used by plugins which override specific properties of the pod in accordance with that 

RecyclerPodTemplate is pod template that understands how to scrub clean a persistent volume after its release The template is used by plugins which override specific properties of the pod in accordance with that plugin See NewPersistentVolumeRecyclerPodTemplate for the properties that are expected to be overridden RecyclerMinimumTimeout is the minimum amount of time in seconds for the recycler pods ActiveDeadlineSeconds attribute 
Name returns the name of either Volume or PersistentVolume one of which must not be nil IsKubeletExpandable returns true for volume types that can be expanded only by the node and not the controller Currently Flex volume is the only one in this category since it is typically not installed on the controller KubeletExpandablePluginName creates and returns a name for the plugin this is used in context on the controller where the plugin lookup fails 

RecyclerPodTemplate is pod template that understands how to scrub clean a persistent volume after its release The template is used by plugins which override specific properties of the pod in accordance with that plugin See NewPersistentVolumeRecyclerPodTemplate for the properties that are expected to be overridden RecyclerMinimumTimeout is the minimum amount of time in seconds for the recycler pods ActiveDeadlineSeconds attribute 
Spec is an internal representation of a volume  All API volume types translate to Spec Name returns the name of either Volume or PersistentVolume one of which must not be nil IsKubeletExpandable returns true for volume types that can be expanded only by the node and not the controller Currently Flex volume is the only one in this category since it is typically not installed on the controller KubeletExpandablePluginName creates and returns a name for the plugin 

RecyclerPodTemplate is pod template that understands how to scrub clean a persistent volume after its release The template is used by plugins which override specific properties of the pod in accordance with that plugin See NewPersistentVolumeRecyclerPodTemplate for the properties that are expected to be overridden 
It is used to generate unique recycler pod name OtherAttributes stores config as strings  These strings are opaque to the system and only understood by the binary hosting the plugin and the plugin itself ProvisioningEnabled configures whether provisioning of this plugin is 

RecyclerPodTemplate is pod template that understands how to scrub clean a persistent volume after its release The template is used by plugins which override specific properties of the pod in accordance with that plugin See NewPersistentVolumeRecyclerPodTemplate for the properties that are expected to be overridden 
OK  An instance of config will be given directly to the plugin so config names specific to plugins are unneeded and wrongly expose plugins in this VolumeConfig struct OtherAttributes is a map of string values intended for oneoff configuration of a plugin or config that is only relevant to a single plugin  All values are passed by string and require interpretation by the 

RecyclerPodTemplate is pod template that understands how to scrub clean a persistent volume after its release The template is used by plugins which override specific properties of the pod in accordance with that plugin See NewPersistentVolumeRecyclerPodTemplate for the properties that are expected to be overridden 
DeletableVolumePlugin is an extended interface of VolumePlugin and is used by persistent volumes that want to be deleted from the cluster after their release from a PersistentVolumeClaim NewDeleter creates a new volumeDeleter which knows how to delete this resource in accordance with the underlying storage provider after the 

RecyclerPodTemplate is pod template that understands how to scrub clean a persistent volume after its release The template is used by plugins which override specific properties of the pod in accordance with that plugin 
strings in OtherAttributes RecyclerPodTemplate is pod template that understands how to scrub clean a persistent volume after its release The template is used by plugins 

RecyclerPodTemplate is pod template that understands how to scrub clean a persistent volume after its release The template is used by plugins which override specific properties of the pod in accordance with that plugin 
the PV being recycled DeletableVolumePlugin is an extended interface of VolumePlugin and is used by persistent volumes that want to be deleted from the cluster after their release from a PersistentVolumeClaim NewDeleter creates a new volumeDeleter which knows how to delete this resource in accordance with the underlying storage provider after the 

The binary should still use strong typing for this value when binding CLI values before they are passed as strings in OtherAttributes RecyclerPodTemplate is pod template that understands how to scrub clean a persistent volume after its release The template is used by plugins which override specific properties of the pod in accordance with that plugin See NewPersistentVolumeRecyclerPodTemplate for the properties that are expected to be overridden 
Host however the end result should be that specific Volume Hosts are passed to the specific functions they are needed in instead of using a catchall VolumeHost interface KubeletVolumeHost is a Kubelet specific interface that plugins can use to access the kubelet 

The binary should still use strong typing for this value when binding CLI values before they are passed as strings in OtherAttributes RecyclerPodTemplate is pod template that understands how to scrub clean a persistent volume after its release The template is used by plugins which override specific properties of the pod in accordance with that plugin 
It is used to generate unique recycler pod name OtherAttributes stores config as strings  These strings are opaque to the system and only understood by the binary hosting the plugin and the plugin itself ProvisioningEnabled configures whether provisioning of this plugin is enabled or not Currently used only in host_path plugin 

Passing config as strings is the least desirable option but can be used for truly oneoff configuration The binary should still use strong typing for this value when binding CLI values before they are passed as strings in OtherAttributes RecyclerPodTemplate is pod template that understands how to scrub clean a persistent volume after its release The template is used by plugins which override specific properties of the pod in accordance with that plugin See NewPersistentVolumeRecyclerPodTemplate for the properties that are expected to be overridden 
NodeExpand expands volume on given deviceMountPath and returns true if resize is successful VolumePluginWithAttachLimits is an extended interface of VolumePlugin that restricts number of volumes that can be attached to a node Return maximum number of volumes that can be attached to a node for this plugin The key must be same as string returned by VolumeLimitKey function The returned map may look like 

Passing config as strings is the least desirable option but can be used for truly oneoff configuration The binary should still use strong typing for this value when binding CLI values before they are passed as strings in OtherAttributes RecyclerPodTemplate is pod template that understands how to scrub clean a persistent volume after its release The template is used by plugins which override specific properties of the pod in accordance with that plugin 
It is used to generate unique recycler pod name OtherAttributes stores config as strings  These strings are opaque to the system and only understood by the binary hosting the plugin and the plugin itself ProvisioningEnabled configures whether provisioning of this plugin is 

Passing config as strings is the least desirable option but can be used for truly oneoff configuration 
plugin Passing config as strings is the least desirable option but can be 

relevant to a single plugin  All values are passed by string and require interpretation by the plugin Passing config as strings is the least desirable option but can be used for truly oneoff configuration The binary should still use strong typing for this value when binding CLI values before they are passed as strings in OtherAttributes RecyclerPodTemplate is pod template that understands how to scrub clean a persistent volume after its release The template is used by plugins which override specific properties of the pod in accordance with that plugin 
OtherAttributes stores config as strings  These strings are opaque to the system and only understood by the binary hosting the plugin and the plugin itself ProvisioningEnabled configures whether provisioning of this plugin is 

relevant to a single plugin  All values are passed by string and require interpretation by the plugin Passing config as strings is the least desirable option but can be used for truly oneoff configuration The binary should still use strong typing for this value when binding CLI values before they are passed as strings in OtherAttributes RecyclerPodTemplate is pod template that understands how to scrub clean a persistent volume after its release The template is used by plugins which override specific properties of the pod in accordance with that plugin 
ActiveDeadlineSeconds for recycler pod PVName is name of the PersistentVolume instance that is being recycled It is used to generate unique recycler pod name OtherAttributes stores config as strings  These strings are opaque to the system and only understood by the binary hosting the plugin and the 

relevant to a single plugin  All values are passed by string and require interpretation by the plugin Passing config as strings is the least desirable option but can be used for truly oneoff configuration The binary should still use strong typing for this value when binding CLI values before they are passed as strings in OtherAttributes RecyclerPodTemplate is pod template that understands how to scrub clean a persistent volume after its release The template is used by plugins which override specific properties of the pod in accordance with that plugin 
config names specific to plugins are unneeded and wrongly expose plugins in this VolumeConfig struct OtherAttributes is a map of string values intended for oneoff configuration of a plugin or config that is only relevant to a single plugin  All values are passed by string and require interpretation by the 

relevant to a single plugin  All values are passed by string and require interpretation by the plugin Passing config as strings is the least desirable option but can be used for truly oneoff configuration The binary should still use strong typing for this value when binding CLI values before they are passed as strings in OtherAttributes RecyclerPodTemplate is pod template that understands how to scrub clean a persistent volume after its release 
RecyclerPodTemplate is pod template that understands how to scrub clean a persistent volume after its release The template is used by plugins which override specific properties of the pod in accordance with that plugin See NewPersistentVolumeRecyclerPodTemplate for the properties that are expected to be overridden 

OtherAttributes is a map of string values intended for oneoff configuration of a plugin or config that is only relevant to a single plugin  All values are passed by string and require interpretation by the plugin Passing config as strings is the least desirable option but can be used for truly oneoff configuration The binary should still use strong typing for this value when binding CLI values before they are passed as strings in OtherAttributes RecyclerPodTemplate is pod template that understands how to scrub clean a persistent volume after its release 
Spec is an internal representation of a volume  All API volume types translate to Spec Name returns the name of either Volume or PersistentVolume one of which must not be nil IsKubeletExpandable returns true for volume types that can be expanded only by the node and not the controller Currently Flex volume is the only one in this category since it is typically not installed on the controller KubeletExpandablePluginName creates and returns a name for the plugin 

OtherAttributes is a map of string values intended for oneoff configuration of a plugin or config that is only relevant to a single plugin  All values are passed by string and require interpretation by the plugin Passing config as strings is the least desirable option but can be used for truly oneoff configuration The binary should still use strong typing for this value when binding CLI values before they are passed as strings in OtherAttributes RecyclerPodTemplate is pod template that understands how to scrub clean a persistent volume after its release 
NodeExpand expands volume on given deviceMountPath and returns true if resize is successful VolumePluginWithAttachLimits is an extended interface of VolumePlugin that restricts number of volumes that can be attached to a node Return maximum number of volumes that can be attached to a node for this plugin The key must be same as string returned by VolumeLimitKey function The returned 

OtherAttributes is a map of string values intended for oneoff configuration of a plugin or config that is only relevant to a single plugin  All values are passed by string and require interpretation by the plugin Passing config as strings is the least desirable option but can be used for truly oneoff configuration The binary should still use strong typing for this value when binding CLI values before they are passed as strings in OtherAttributes 
It is used to generate unique recycler pod name OtherAttributes stores config as strings  These strings are opaque to the system and only understood by the binary hosting the plugin and the plugin itself ProvisioningEnabled configures whether provisioning of this plugin is enabled or not Currently used only in host_path plugin 

in this VolumeConfig struct OtherAttributes is a map of string values intended for oneoff configuration of a plugin or config that is only relevant to a single plugin  All values are passed by string and require interpretation by the plugin Passing config as strings is the least desirable option but can be used for truly oneoff configuration The binary should still use strong typing for this value when binding CLI values before they are passed as strings in OtherAttributes 
OtherAttributes stores config as strings  These strings are opaque to the system and only understood by the binary hosting the plugin and the plugin itself ProvisioningEnabled configures whether provisioning of this plugin is enabled or not Currently used only in host_path plugin 

in this VolumeConfig struct OtherAttributes is a map of string values intended for oneoff configuration of a plugin or config that is only relevant to a single plugin  All values are passed by string and require interpretation by the plugin Passing config as strings is the least desirable option but can be used for truly oneoff configuration The binary should still use strong typing for this value when binding CLI values before they are passed as strings 
It is used to generate unique recycler pod name OtherAttributes stores config as strings  These strings are opaque to the system and only understood by the binary hosting the plugin and the plugin itself ProvisioningEnabled configures whether provisioning of this plugin is enabled or not Currently used only in host_path plugin 

in this VolumeConfig struct OtherAttributes is a map of string values intended for oneoff configuration of a plugin or config that is only relevant to a single plugin  All values are passed by string and require interpretation by the plugin Passing config as strings is the least desirable option but can be used for truly oneoff configuration 
Spec is an internal representation of a volume  All API volume types translate to Spec Name returns the name of either Volume or PersistentVolume one of which must not be nil IsKubeletExpandable returns true for volume types that can be expanded only by the node and not the controller Currently Flex volume is the only one in this category since it is typically not installed on the controller KubeletExpandablePluginName creates and returns a name for the plugin 

given directly to the plugin so config names specific to plugins are unneeded and wrongly expose plugins in this VolumeConfig struct OtherAttributes is a map of string values intended for oneoff configuration of a plugin or config that is only relevant to a single plugin  All values are passed by string and require interpretation by the plugin Passing config as strings is the least desirable option but can be used for truly oneoff configuration The binary should still use strong typing for this value when binding CLI values before they are passed as strings 
NodeExpand expands volume on given deviceMountPath and returns true if resize is successful VolumePluginWithAttachLimits is an extended interface of VolumePlugin that restricts number of volumes that can be attached to a node Return maximum number of volumes that can be attached to a node for this plugin The key must be same as string returned by VolumeLimitKey function The returned map may look like 

given directly to the plugin so config names specific to plugins are unneeded and wrongly expose plugins in this VolumeConfig struct OtherAttributes is a map of string values intended for oneoff configuration of a plugin or config that is only relevant to a single plugin  All values are passed by string and require interpretation by the plugin Passing config as strings is the least desirable option but can be used for truly oneoff configuration 
It is used to generate unique recycler pod name OtherAttributes stores config as strings  These strings are opaque to the system and only understood by the binary hosting the plugin and the plugin itself ProvisioningEnabled configures whether provisioning of this plugin is 

given directly to the plugin so config names specific to plugins are unneeded and wrongly expose plugins in this VolumeConfig struct OtherAttributes is a map of string values intended for oneoff configuration of a plugin or config that is only relevant to a single plugin  All values are passed by string and require interpretation by the plugin Passing config as strings is the least desirable option but can be used for truly oneoff configuration 
RecyclerPodTemplate is pod template that understands how to scrub clean a persistent volume after its release The template is used by plugins which override specific properties of the pod in accordance with that plugin See NewPersistentVolumeRecyclerPodTemplate for the properties that are expected to be overridden 

given directly to the plugin so config names specific to plugins are unneeded and wrongly expose plugins in this VolumeConfig struct OtherAttributes is a map of string values intended for oneoff configuration of a plugin or config that is only relevant to a single plugin  All values are passed by string and require interpretation by the plugin Passing config as strings is the least desirable option but can be used for truly oneoff configuration 
as volume expansion on controller isnt supported but a plugin name is required VolumeConfig is how volume plugins receive configuration  An instance specific to the plugin will be passed to the plugins ProbeVolumePluginsconfig func  Reasonable defaults will be provided by the binary hosting the plugins while allowing override of those default 

given directly to the plugin so config names specific to plugins are unneeded and wrongly expose plugins in this VolumeConfig struct OtherAttributes is a map of string values intended for oneoff configuration of a plugin or config that is only relevant to a single plugin  All values are passed by string and require interpretation by the plugin Passing config as strings is the least desirable option but can be used for truly oneoff configuration 
Host however the end result should be that specific Volume Hosts are passed to the specific functions they are needed in instead of using a catchall VolumeHost interface KubeletVolumeHost is a Kubelet specific interface that plugins can use to access the kubelet 

given directly to the plugin so config names specific to plugins are unneeded and wrongly expose plugins in this VolumeConfig struct OtherAttributes is a map of string values intended for oneoff configuration of a plugin or config that is only relevant to a single plugin  All values are passed by string and require interpretation by the plugin Passing config as strings is the least desirable option but can be used for truly oneoff configuration 
DeviceStagingPath stores location where the volume is staged aggregates events for successful drivers and errors for failed drivers VolumePlugin is an interface to volume plugins that can be used on a kubernetes node eg by kubelet to instantiate and manage volumes Init initializes the plugin  This will be called exactly once before any New calls are made  implementations of plugins may 

given directly to the plugin so config names specific to plugins are unneeded and wrongly expose plugins in this VolumeConfig struct OtherAttributes is a map of string values intended for oneoff configuration of a plugin or config that is only relevant to a single plugin  All values are passed by string and require interpretation by the plugin 
OtherAttributes stores config as strings  These strings are opaque to the system and only understood by the binary hosting the plugin and the plugin itself ProvisioningEnabled configures whether provisioning of this plugin is enabled or not Currently used only in host_path plugin 

name ie RecyclerMinimumTimeout is OK but RecyclerMinimumTimeoutForNFS is OK  An instance of config will be given directly to the plugin so config names specific to plugins are unneeded and wrongly expose plugins in this VolumeConfig struct OtherAttributes is a map of string values intended for oneoff configuration of a plugin or config that is only relevant to a single plugin  All values are passed by string and require interpretation by the plugin Passing config as strings is the least desirable option but can be used for truly oneoff configuration 
NodeExpand expands volume on given deviceMountPath and returns true if resize is successful VolumePluginWithAttachLimits is an extended interface of VolumePlugin that restricts number of volumes that can be attached to a node Return maximum number of volumes that can be attached to a node for this plugin The key must be same as string returned by VolumeLimitKey function The returned map may look like 

name ie RecyclerMinimumTimeout is OK but RecyclerMinimumTimeoutForNFS is OK  An instance of config will be given directly to the plugin so config names specific to plugins are unneeded and wrongly expose plugins in this VolumeConfig struct OtherAttributes is a map of string values intended for oneoff configuration of a plugin or config that is only relevant to a single plugin  All values are passed by string and require interpretation by the plugin 
as volume expansion on controller isnt supported but a plugin name is required VolumeConfig is how volume plugins receive configuration  An instance specific to the plugin will be passed to the plugins ProbeVolumePluginsconfig func  Reasonable defaults will be provided by the binary hosting the plugins while allowing override of those default 

name ie RecyclerMinimumTimeout is OK but RecyclerMinimumTimeoutForNFS is OK  An instance of config will be given directly to the plugin so config names specific to plugins are unneeded and wrongly expose plugins in this VolumeConfig struct 
values  Those config values are then set to an instance of VolumeConfig and passed to the plugin Values in VolumeConfig are intended to be relevant to several plugins but not necessarily all plugins  The preference is to leverage strong typing in this struct  All config items must have a descriptive but nonspecific name ie RecyclerMinimumTimeout is OK but RecyclerMinimumTimeoutForNFS is 

preference is to leverage strong typing in this struct  All config items must have a descriptive but nonspecific name ie RecyclerMinimumTimeout is OK but RecyclerMinimumTimeoutForNFS is OK  An instance of config will be given directly to the plugin so config names specific to plugins are unneeded and wrongly expose plugins in this VolumeConfig struct OtherAttributes is a map of string values intended for oneoff configuration of a plugin or config that is only relevant to a single plugin  All values are passed by string and require interpretation by the plugin 
DeviceStagingPath stores location where the volume is staged aggregates events for successful drivers and errors for failed drivers VolumePlugin is an interface to volume plugins that can be used on a kubernetes node eg by kubelet to instantiate and manage volumes Init initializes the plugin  This will be called exactly once before any New calls are made  implementations of plugins may 

preference is to leverage strong typing in this struct  All config items must have a descriptive but nonspecific name ie RecyclerMinimumTimeout is OK but RecyclerMinimumTimeoutForNFS is OK  An instance of config will be 
name ie RecyclerMinimumTimeout is OK but RecyclerMinimumTimeoutForNFS is OK  An instance of config will be given directly to the plugin so config names specific to plugins are unneeded and wrongly expose plugins in this VolumeConfig struct OtherAttributes is a map of string values intended for oneoff 

preference is to leverage strong typing in this struct  All config items must have a descriptive but nonspecific name ie RecyclerMinimumTimeout is OK but RecyclerMinimumTimeoutForNFS is OK  An instance of config will be 
in this struct  All config items must have a descriptive but nonspecific name ie RecyclerMinimumTimeout is OK but RecyclerMinimumTimeoutForNFS is OK  An instance of config will be given directly to the plugin so 

Values in VolumeConfig are intended to be relevant to several plugins but not necessarily all plugins  The preference is to leverage strong typing in this struct  All config items must have a descriptive but nonspecific name ie RecyclerMinimumTimeout is OK but RecyclerMinimumTimeoutForNFS is OK  An instance of config will be given directly to the plugin so config names specific to plugins are unneeded and wrongly expose plugins 
OK  An instance of config will be given directly to the plugin so config names specific to plugins are unneeded and wrongly expose plugins in 

Values in VolumeConfig are intended to be relevant to several plugins but not necessarily all plugins  The preference is to leverage strong typing in this struct  All config items must have a descriptive but nonspecific name ie RecyclerMinimumTimeout is OK but RecyclerMinimumTimeoutForNFS is OK  An instance of config will be 
name ie RecyclerMinimumTimeout is OK but RecyclerMinimumTimeoutForNFS is OK  An instance of config will be given directly to the plugin so config names specific to plugins are unneeded and wrongly expose plugins in 

the plugins while allowing override of those default values  Those config values are then set to an instance of VolumeConfig and passed to the plugin Values in VolumeConfig are intended to be relevant to several plugins but not necessarily all plugins  The preference is to leverage strong typing in this struct  All config items must have a descriptive but nonspecific name ie RecyclerMinimumTimeout is OK but RecyclerMinimumTimeoutForNFS is OK  An instance of config will be 
OK  An instance of config will be given directly to the plugin so config names specific to plugins are unneeded and wrongly expose plugins in 

the plugins while allowing override of those default values  Those config values are then set to an instance of VolumeConfig and passed to the plugin Values in VolumeConfig are intended to be relevant to several plugins but not necessarily all plugins  The preference is to leverage strong typing in this struct  All config items must have a descriptive but nonspecific name ie RecyclerMinimumTimeout is OK but RecyclerMinimumTimeoutForNFS is OK  An instance of config will be 
Recycle will use the provided recorder to write any events that might be interesting to user Its expected that caller will pass these events to the PV being recycled DeletableVolumePlugin is an extended interface of VolumePlugin and is used by persistent volumes that want to be deleted from the cluster after their 

the plugins while allowing override of those default values  Those config values are then set to an instance of VolumeConfig and passed to the plugin Values in VolumeConfig are intended to be relevant to several plugins but not necessarily all plugins  The preference is to leverage strong typing in this struct  All config items must have a descriptive but nonspecific 
Host however the end result should be that specific Volume Hosts are passed to the specific functions they are needed in instead of using a catchall VolumeHost interface KubeletVolumeHost is a Kubelet specific interface that plugins can use to access the kubelet SetKubeletError lets plugins set an error on the Kubelet runtime status that will cause the Kubelet to post NotReady status with the error message provided 

the plugins while allowing override of those default values  Those config values are then set to an instance of 
values  Those config values are then set to an instance of VolumeConfig 

the plugins ProbeVolumePluginsconfig func  Reasonable defaults will be provided by the binary hosting the plugins while allowing override of those default values  Those config values are then set to an instance of VolumeConfig and passed to the plugin Values in VolumeConfig are intended to be relevant to several plugins but not necessarily all plugins  The preference is to leverage strong typing in this struct  All config items must have a descriptive but nonspecific name ie RecyclerMinimumTimeout is OK but RecyclerMinimumTimeoutForNFS is OK  An instance of config will be 
Returns an interface that should be used to execute subpath operations Returns options to pass for proxyutil filtered dialers VolumePluginMgr tracks registered plugins Spec is an internal representation of a volume  All API volume types translate to Spec Name returns the name of either Volume or PersistentVolume one of which must not be nil IsKubeletExpandable returns true for volume types that can be expanded only by the node 

the plugins ProbeVolumePluginsconfig func  Reasonable defaults will be provided by the binary hosting the plugins while allowing override of those default values  Those config values are then set to an instance of VolumeConfig and passed to the plugin Values in VolumeConfig are intended to be relevant to several plugins but not necessarily all plugins  The preference is to leverage strong typing in this struct  All config items must have a descriptive but nonspecific name ie RecyclerMinimumTimeout is OK but RecyclerMinimumTimeoutForNFS is OK  An instance of config will be 
VolumePlugin is an interface to volume plugins that can be used on a kubernetes node eg by kubelet to instantiate and manage volumes Init initializes the plugin  This will be called exactly once before any New calls are made  implementations of plugins may 

the plugins ProbeVolumePluginsconfig func  Reasonable defaults will be provided by the binary hosting the plugins while allowing override of those default values  Those config values are then set to an instance of VolumeConfig and passed to the plugin Values in VolumeConfig are intended to be relevant to several plugins but not necessarily all plugins  The preference is to leverage strong typing in this struct  All config items must have a descriptive but nonspecific 
OK  An instance of config will be given directly to the plugin so config names specific to plugins are unneeded and wrongly expose plugins in this VolumeConfig struct 

the plugins ProbeVolumePluginsconfig func  Reasonable defaults will be provided by the binary hosting the plugins while allowing override of those default values  Those config values are then set to an instance of VolumeConfig and passed to the plugin Values in VolumeConfig are intended to be relevant to several plugins but not necessarily all plugins  The preference is to leverage strong typing in this struct  All config items must have a descriptive but nonspecific 
Host however the end result should be that specific Volume Hosts are passed to the specific functions they are needed in instead of using a catchall VolumeHost interface KubeletVolumeHost is a Kubelet specific interface that plugins can use to access the kubelet 

the plugins ProbeVolumePluginsconfig func  Reasonable defaults will be provided by the binary hosting the plugins while allowing override of those default values  Those config values are then set to an instance of VolumeConfig and passed to the plugin Values in VolumeConfig are intended to be relevant to several plugins but not necessarily all plugins  The preference is to leverage strong typing in this struct  All config items must have a descriptive but nonspecific 
interesting to user Its expected that caller will pass these events to the PV being recycled DeletableVolumePlugin is an extended interface of VolumePlugin and is used by persistent volumes that want to be deleted from the cluster after their 

the plugins ProbeVolumePluginsconfig func  Reasonable defaults will be provided by the binary hosting the plugins while allowing override of those default values  Those config values are then set to an instance of VolumeConfig and passed to the plugin Values in VolumeConfig are intended to be relevant to several plugins but not necessarily all plugins  The preference is to leverage strong typing in this struct  All config items must have a descriptive but nonspecific 
The kubernetesio namespace is reserved for plugins which are bundled with kubernetes GetVolumeName returns the nameID to uniquely identifying the actual backing device directory path etc referenced by the specified volume spec For Attachable volumes this value must be able to be passed back to 

VolumeConfig is how volume plugins receive configuration  An instance specific to the plugin will be passed to the plugins ProbeVolumePluginsconfig func  Reasonable defaults will be provided by the binary hosting the plugins while allowing override of those default values  Those config values are then set to an instance of VolumeConfig and passed to the plugin Values in VolumeConfig are intended to be relevant to several plugins but not necessarily all plugins  The preference is to leverage strong typing in this struct  All config items must have a descriptive but nonspecific 
Recycle will use the provided recorder to write any events that might be interesting to user Its expected that caller will pass these events to the PV being recycled DeletableVolumePlugin is an extended interface of VolumePlugin and is used 

VolumeConfig is how volume plugins receive configuration  An instance specific to the plugin will be passed to the plugins ProbeVolumePluginsconfig func  Reasonable defaults will be provided by the binary hosting the plugins while allowing override of those default values  Those config values are then set to an instance of VolumeConfig and passed to the plugin Values in VolumeConfig are intended to be relevant to several plugins but not necessarily all plugins  The preference is to leverage strong typing in this struct  All config items must have a descriptive but nonspecific 
RecyclableVolumePlugin is an extended interface of VolumePlugin and is used by persistent volumes that want to be recycled before being made available again to new claims Recycle knows how to reclaim this resource after the volumes release from a PersistentVolumeClaim Recycle will use the provided recorder to write any events that might be 

VolumeConfig is how volume plugins receive configuration  An instance specific to the plugin will be passed to the plugins ProbeVolumePluginsconfig func  Reasonable defaults will be provided by the binary hosting the plugins while allowing override of those default values  Those config values are then set to an instance of VolumeConfig and passed to the plugin Values in VolumeConfig are intended to be relevant to several plugins but not necessarily all plugins  The preference is to leverage strong typing in this struct  All config items must have a descriptive but nonspecific 
aggregates events for successful drivers and errors for failed drivers VolumePlugin is an interface to volume plugins that can be used on a kubernetes node eg by kubelet to instantiate and manage volumes Init initializes the plugin  This will be called exactly once before any New calls are made  implementations of plugins may 

VolumeConfig is how volume plugins receive configuration  An instance specific to the plugin will be passed to the plugins ProbeVolumePluginsconfig func  Reasonable defaults will be provided by the binary hosting the plugins while allowing override of those default values  Those config values are then set to an instance of VolumeConfig and passed to the plugin Values in VolumeConfig are intended to be relevant to several plugins but not necessarily all plugins  The preference is to leverage strong typing in this struct  All config items must have a descriptive but nonspecific 
The operation to the plugin Common parameter which can be specified in StorageClass to specify the desired FSType Provisioners SHOULD implement support for this if they are block device based 

VolumeConfig is how volume plugins receive configuration  An instance specific to the plugin will be passed to the plugins ProbeVolumePluginsconfig func  Reasonable defaults will be provided by the binary hosting the plugins while allowing override of those default values  Those config values are then set to an instance of VolumeConfig and passed to the plugin Values in VolumeConfig are intended to be relevant to several plugins but not necessarily all plugins  The 
OK  An instance of config will be given directly to the plugin so config names specific to plugins are unneeded and wrongly expose plugins in 

VolumeConfig is how volume plugins receive configuration  An instance specific to the plugin will be passed to the plugins ProbeVolumePluginsconfig func  Reasonable defaults will be provided by the binary hosting the plugins while allowing override of those default values  Those config values are then set to an instance of VolumeConfig and passed to the plugin Values in VolumeConfig are intended to be relevant to several plugins but not necessarily all plugins  The 
The kubernetesio namespace is reserved for plugins which are bundled with kubernetes GetVolumeName returns the nameID to uniquely identifying the actual backing device directory path etc referenced by the specified volume spec For Attachable volumes this value must be able to be passed back to 

VolumeConfig is how volume plugins receive configuration  An instance specific to the plugin will be passed to the plugins ProbeVolumePluginsconfig func  Reasonable defaults will be provided by the binary hosting the plugins while allowing override of those default values  Those config values are then set to an instance of 
Returns an interface that should be used to execute subpath operations Returns options to pass for proxyutil filtered dialers VolumePluginMgr tracks registered plugins Spec is an internal representation of a volume  All API volume types translate to Spec Name returns the name of either Volume or PersistentVolume one of which must not be nil IsKubeletExpandable returns true for volume types that can be expanded only by the node 

VolumeConfig is how volume plugins receive configuration  An instance specific to the plugin will be passed to the plugins ProbeVolumePluginsconfig func  Reasonable defaults will be provided by the binary hosting the plugins while allowing override of those default values  Those config values are then set to an instance of 
interesting to user Its expected that caller will pass these events to the PV being recycled DeletableVolumePlugin is an extended interface of VolumePlugin and is used by persistent volumes that want to be deleted from the cluster after their 

Name returns the name of either Volume or PersistentVolume one of which must not be nil VolumeConfig is how volume plugins receive configuration  An instance specific to the plugin will be passed to the plugins ProbeVolumePluginsconfig func  Reasonable defaults will be provided by the binary hosting the plugins while allowing override of those default values  Those config values are then set to an instance of VolumeConfig and passed to the plugin 
Returns an interface that should be used to execute any utilities in volume plugins Returns the labels on the node Returns the name of the node Returns the event recorder of kubelet Returns an interface that should be used to execute subpath operations Returns options to pass for proxyutil filtered dialers 

Name returns the name of either Volume or PersistentVolume one of which must not be nil VolumeConfig is how volume plugins receive configuration  An instance specific to the plugin will be passed to the plugins ProbeVolumePluginsconfig func  Reasonable defaults will be provided by the binary hosting the plugins while allowing override of those default values  Those config values are then set to an instance of 
OK  An instance of config will be given directly to the plugin so config names specific to plugins are unneeded and wrongly expose plugins in this VolumeConfig struct OtherAttributes is a map of string values intended for oneoff 

Spec is an internal representation of a volume  All API volume types translate to Spec Name returns the name of either Volume or PersistentVolume one of which must not be nil VolumeConfig is how volume plugins receive configuration  An instance specific to the plugin will be passed to the plugins ProbeVolumePluginsconfig func  Reasonable defaults will be provided by the binary hosting the plugins while allowing override of those default values  Those config values are then set to an instance of VolumeConfig and passed to the plugin 
Returns the name of the node Returns the event recorder of kubelet Returns an interface that should be used to execute subpath operations Returns options to pass for proxyutil filtered dialers 

Returns the hostname of the host kubelet is running on VolumePluginMgr tracks registered plugins Spec is an internal representation of a volume  All API volume types translate to Spec Name returns the name of either Volume or PersistentVolume one of which must not be nil VolumeConfig is how volume plugins receive configuration  An instance specific to the plugin will be passed to the plugins ProbeVolumePluginsconfig func  Reasonable defaults will be provided by the binary hosting 
OK  An instance of config will be given directly to the plugin so config names specific to plugins are unneeded and wrongly expose plugins in this VolumeConfig struct OtherAttributes is a map of string values intended for oneoff configuration of a plugin or config that is only relevant to a single plugin  All values are passed by string and require interpretation by the 

the provided spec  See comments on NewWrapperBuilder for more 
the provided spec  See comments on NewWrapperMounter for more 

NewWrapperCleaner finds an appropriate plugin with which to handle 
NewWrapperUnmounter finds an appropriate plugin with which to handle 

NewWrapperCleaner finds an appropriate plugin with which to handle 
NewWrapperMounter finds an appropriate plugin with which to handle 

the provided spec  This is used to implement volume plugins which wrap other plugins  For example the secret volume is implemented in terms of the emptyDir volume NewWrapperCleaner finds an appropriate plugin with which to handle 
create PersistentVolumes in accordance with the plugins underlying storage provider AttachableVolumePlugin is an extended interface of VolumePlugin and is used for volumes that require attachment to a node before mounting 

the provided spec  This is used to implement volume plugins which wrap other plugins  For example the secret volume is implemented in terms of the emptyDir volume NewWrapperCleaner finds an appropriate plugin with which to handle 
NewProvisioner creates a new volumeProvisioner which knows how to create PersistentVolumes in accordance with the plugins underlying storage provider AttachableVolumePlugin is an extended interface of VolumePlugin and is used for volumes that require attachment 

the provided spec  This is used to implement volume plugins which wrap other plugins  For example the secret volume is implemented in terms of the emptyDir volume NewWrapperCleaner finds an appropriate plugin with which to handle 
ProvisionableVolumePlugin is an extended interface of VolumePlugin and is used to create volumes for the cluster NewProvisioner creates a new volumeProvisioner which knows how to create PersistentVolumes in accordance with the plugins underlying 

the provided spec  This is used to implement volume plugins which wrap other plugins  For example the secret volume is implemented in terms of the emptyDir volume NewWrapperCleaner finds an appropriate plugin with which to handle 
NewDeleter creates a new volumeDeleter which knows how to delete this resource in accordance with the underlying storage provider after the volumes release from a claim ProvisionableVolumePlugin is an extended interface of VolumePlugin and is used to create volumes for the cluster 

NewWrapperBuilder finds an appropriate plugin with which to handle the provided spec  This is used to implement volume plugins which wrap other plugins  For example the secret volume is implemented in terms of the emptyDir volume 
create PersistentVolumes in accordance with the plugins underlying storage provider AttachableVolumePlugin is an extended interface of VolumePlugin and is used for volumes that require attachment to a node before mounting 

NewWrapperBuilder finds an appropriate plugin with which to handle the provided spec  This is used to implement volume plugins which wrap other plugins  For example the secret volume is implemented in terms of the emptyDir volume 
NewProvisioner creates a new volumeProvisioner which knows how to create PersistentVolumes in accordance with the plugins underlying storage provider AttachableVolumePlugin is an extended interface of VolumePlugin and is used for volumes that require attachment 

NewWrapperBuilder finds an appropriate plugin with which to handle the provided spec  This is used to implement volume plugins which wrap other plugins  For example the secret volume is implemented in terms of the emptyDir volume 
ProvisionableVolumePlugin is an extended interface of VolumePlugin and is used to create volumes for the cluster NewProvisioner creates a new volumeProvisioner which knows how to create PersistentVolumes in accordance with the plugins underlying 

NewWrapperBuilder finds an appropriate plugin with which to handle the provided spec  This is used to implement volume plugins which wrap other plugins  For example the secret volume is implemented in terms of the emptyDir volume 
NewDeleter creates a new volumeDeleter which knows how to delete this resource in accordance with the underlying storage provider after the volumes release from a claim ProvisionableVolumePlugin is an extended interface of VolumePlugin and is used to create volumes for the cluster 

NewWrapperBuilder finds an appropriate plugin with which to handle 
NewWrapperUnmounter finds an appropriate plugin with which to handle 

NewWrapperBuilder finds an appropriate plugin with which to handle 
NewWrapperMounter finds an appropriate plugin with which to handle 

GetKubeClient returns a client interface NewWrapperBuilder finds an appropriate plugin with which to handle the provided spec  This is used to implement volume plugins which wrap other plugins  For example the secret volume is implemented in terms of the emptyDir volume NewWrapperCleaner finds an appropriate plugin with which to handle 
volumes release from a claim ProvisionableVolumePlugin is an extended interface of VolumePlugin and is used to create volumes for the cluster NewProvisioner creates a new volumeProvisioner which knows how to 

GetKubeClient returns a client interface NewWrapperBuilder finds an appropriate plugin with which to handle the provided spec  This is used to implement volume plugins which wrap other plugins  For example the secret volume is implemented in terms of the emptyDir volume 
resource in accordance with the underlying storage provider after the volumes release from a claim ProvisionableVolumePlugin is an extended interface of VolumePlugin and is used to create volumes for the cluster NewProvisioner creates a new volumeProvisioner which knows how to 

GetKubeClient returns a client interface NewWrapperBuilder finds an appropriate plugin with which to handle the provided spec  This is used to implement volume plugins which wrap other plugins  For example the secret volume is implemented in terms of the emptyDir volume 
NewDeleter creates a new volumeDeleter which knows how to delete this resource in accordance with the underlying storage provider after the volumes release from a claim ProvisionableVolumePlugin is an extended interface of VolumePlugin and is 

GetKubeClient returns a client interface NewWrapperBuilder finds an appropriate plugin with which to handle the provided spec  This is used to implement volume plugins which wrap other plugins  For example the secret volume is 
NewProvisioner creates a new volumeProvisioner which knows how to create PersistentVolumes in accordance with the plugins underlying storage provider AttachableVolumePlugin is an extended interface of VolumePlugin and is used for volumes that require attachment to a node before mounting CanAttach tests if provided volume spec is attachable 

GetKubeClient returns a client interface NewWrapperBuilder finds an appropriate plugin with which to handle the provided spec  This is used to implement volume plugins which 
VolumeAttachmentLister returns the informer lister for the VolumeAttachment API Object IsAttachDetachController is an interface marker to strictly tie AttachDetachVolumeHost to the attachDetachController VolumeHost is an interface that plugins can use to access the kubelet GetPluginDir returns the absolute path to a directory under which 

directory might not actually exist on disk yet GetKubeClient returns a client interface NewWrapperBuilder finds an appropriate plugin with which to handle the provided spec  This is used to implement volume plugins which wrap other plugins  For example the secret volume is implemented in terms of the emptyDir volume 
Spec is an internal representation of a volume  All API volume types translate to Spec Name returns the name of either Volume or PersistentVolume one of which must not be nil IsKubeletExpandable returns true for volume types that can be expanded only by the node and not the controller Currently Flex volume is the only one in this category since it is typically not installed on the controller KubeletExpandablePluginName creates and returns a name for the plugin 

directory might not actually exist on disk yet GetKubeClient returns a client interface NewWrapperBuilder finds an appropriate plugin with which to handle the provided spec  This is used to implement volume plugins which wrap other plugins  For example the secret volume is implemented in terms of the emptyDir volume 
Host however the end result should be that specific Volume Hosts are passed to the specific functions they are needed in instead of using a catchall VolumeHost interface KubeletVolumeHost is a Kubelet specific interface that plugins can use to access the kubelet SetKubeletError lets plugins set an error on the Kubelet runtime status 

directory might not actually exist on disk yet GetKubeClient returns a client interface NewWrapperBuilder finds an appropriate plugin with which to handle the provided spec  This is used to implement volume plugins which wrap other plugins  For example the secret volume is implemented in terms of the emptyDir volume 
NodeExpandableVolumePlugin is an expanded interface of VolumePlugin and is used for volumes that require expansion on the node via NodeExpand call NodeExpand expands volume on given deviceMountPath and returns true if resize is successful VolumePluginWithAttachLimits is an extended interface of VolumePlugin that restricts number of volumes that can be attached to a node Return maximum number of volumes that can be attached to a node for this plugin 

directory might not actually exist on disk yet GetKubeClient returns a client interface NewWrapperBuilder finds an appropriate plugin with which to handle the provided spec  This is used to implement volume plugins which wrap other plugins  For example the secret volume is 
this is used in context on the controller where the plugin lookup fails as volume expansion on controller isnt supported but a plugin name is required VolumeConfig is how volume plugins receive configuration  An instance specific to the plugin will be passed to the plugins 

directory might not actually exist on disk yet GetKubeClient returns a client interface NewWrapperBuilder finds an appropriate plugin with which to handle the provided spec  This is used to implement volume plugins which 
VolumeHost is an interface that plugins can use to access the kubelet GetPluginDir returns the absolute path to a directory under which a given plugin may store data  This directory might not actually 

a given plugin may store data for a given pod  If the specified pod does not exist the result of this call might not exist  This directory might not actually exist on disk yet GetKubeClient returns a client interface NewWrapperBuilder finds an appropriate plugin with which to handle the provided spec  This is used to implement volume plugins which 
GetPluginDir returns the absolute path to a directory under which a given plugin may store data  This directory might not actually exist on disk yet  For plugin data that is perpod see 

GetPodPluginDir returns the absolute path to a directory under which a given plugin may store data for a given pod  If the specified pod does not exist the result of this call might not exist  This 
GetPodVolumeDeviceDir returns the absolute path a directory which represents the named plugin for the given pod If the specified pod does not exist the result of this call might not exist ex podspodUidDefaultKubeletVolumeDevicesDirNameescapeQualifiedPluginName GetKubeClient returns a client interface 

GetPodPluginDir returns the absolute path to a directory under which 
GetVolumeDevicePluginDir returns the absolute path to a directory 

GetPodPluginDir returns the absolute path to a directory under which 
GetPluginDir returns the absolute path to a directory under which 

represents the named volume under the named plugin for the given pod  If the specified pod does not exist the result of this call might not exist GetPodPluginDir returns the absolute path to a directory under which 
does not exist the result of this call might not exist  This directory might not actually exist on disk yet GetPodVolumeDeviceDir returns the absolute path a directory which represents the named plugin for the given pod 

GetPodVolumeDir returns the absolute path a directory which 
GetPodVolumeDeviceDir returns the absolute path a directory which 

a given plugin may store data  This directory might not actually exist on disk yet  For plugin data that is perpod see GetPodPluginDir GetPodVolumeDir returns the absolute path a directory which represents the named volume under the named plugin for the given pod  If the specified pod does not exist the result of this call 
GetPodPluginDir returns the absolute path to a directory under which a given plugin may store data for a given pod  If the specified pod does not exist the result of this call might not exist  This 

GetPluginDir returns the absolute path to a directory under which a given plugin may store data  This directory might not actually 
under which a given plugin may store data ex pluginskubernetesioPluginNameDefaultKubeletVolumeDevicesDirNamevolumePluginDependentPath GetPodsDir returns the absolute path to a directory where all the pods information is stored 

GetPluginDir returns the absolute path to a directory under which 
GetPodPluginDir returns the absolute path to a directory under which 

GetPluginDir returns the absolute path to a directory under which 
GetVolumeDevicePluginDir returns the absolute path to a directory 

AttachableVolumePlugin is an extended interface of VolumePlugin and is used for volumes that require attachment to a node before mounting VolumeHost is an interface that plugins can use to access the kubelet GetPluginDir returns the absolute path to a directory under which a given plugin may store data  This directory might not actually exist on disk yet  For plugin data that is perpod see 
KubeletExpandablePluginName creates and returns a name for the plugin this is used in context on the controller where the plugin lookup fails as volume expansion on controller isnt supported but a plugin name is required VolumeConfig is how volume plugins receive configuration  An instance specific to the plugin will be passed to the plugins 

AttachableVolumePlugin is an extended interface of VolumePlugin and is used for volumes that require attachment to a node before mounting VolumeHost is an interface that plugins can use to access the kubelet GetPluginDir returns the absolute path to a directory under which a given plugin may store data  This directory might not actually 
to the specific functions they are needed in instead of using a catchall VolumeHost interface KubeletVolumeHost is a Kubelet specific interface that plugins can use to access the kubelet SetKubeletError lets plugins set an error on the Kubelet runtime status 

AttachableVolumePlugin is an extended interface of VolumePlugin and is used for volumes that require attachment to a node before mounting VolumeHost is an interface that plugins can use to access the kubelet GetPluginDir returns the absolute path to a directory under which 
GetKubeClient returns a client interface NewWrapperMounter finds an appropriate plugin with which to handle the provided spec  This is used to implement volume plugins which 

AttachableVolumePlugin is an extended interface of VolumePlugin and is used for volumes that require attachment to a node before mounting VolumeHost is an interface that plugins can use to access the kubelet 
GetKubeClient returns a client interface NewWrapperMounter finds an appropriate plugin with which to handle the provided spec  This is used to implement volume plugins which wrap other plugins  For example the secret volume is 

AttachableVolumePlugin is an extended interface of VolumePlugin and is used for volumes that require attachment to a node before mounting VolumeHost is an interface that plugins can use to access the kubelet 
DeviceMountableVolumePlugin is an extended interface of VolumePlugin and is used for volumes that requires mount device to a node before binding to volume to pod 

AttachableVolumePlugin is an extended interface of VolumePlugin and is used for volumes that require attachment to a node before mounting 
DeviceMountableVolumePlugin is an extended interface of VolumePlugin and is used for volumes that requires mount device to a node before binding to volume to pod CanDeviceMount determines if device in volumeSpec is mountable ExpandableVolumePlugin is an extended interface of VolumePlugin and is used for volumes that can be 

AttachableVolumePlugin is an extended interface of VolumePlugin and is used for volumes that require attachment to a node before mounting 
mount o contextXYZ for a given volume PersistentVolumePlugin is an extended interface of VolumePlugin and is used by volumes that want to provide long term persistence of data 

AttachableVolumePlugin is an extended interface of VolumePlugin and is used for volumes that require attachment 
BlockVolumePlugin is an extend interface of VolumePlugin and is used for block volumes support 

AttachableVolumePlugin is an extended interface of VolumePlugin and is used for volumes that require attachment 
NodeExpandableVolumePlugin is an expanded interface of VolumePlugin and is used for volumes that 

AttachableVolumePlugin is an extended interface of VolumePlugin and is used for volumes that require attachment 
ProvisionableVolumePlugin is an extended interface of VolumePlugin and is used to create volumes for the cluster 

AttachableVolumePlugin is an extended interface of VolumePlugin and is used for volumes that require attachment 
PersistentVolumePlugin is an extended interface of VolumePlugin and is used by volumes that want to provide long term persistence of data GetAccessModes describes the ways a given volume can be accessedmounted RecyclableVolumePlugin is an extended interface of VolumePlugin and is used 

the plugins underlying storage provider AttachableVolumePlugin is an extended interface of VolumePlugin and is used for volumes that require attachment to a node before mounting VolumeHost is an interface that plugins can use to access the kubelet GetPluginDir returns the absolute path to a directory under which a given plugin may store data  This directory might not actually 
VolumeHost interface KubeletVolumeHost is a Kubelet specific interface that plugins can use to access the kubelet SetKubeletError lets plugins set an error on the Kubelet runtime status that will cause the Kubelet to post NotReady status with the error message provided GetInformerFactory returns the informer factory for CSIDriverLister 

the plugins underlying storage provider AttachableVolumePlugin is an extended interface of VolumePlugin and is used for volumes that require attachment to a node before mounting VolumeHost is an interface that plugins can use to access the kubelet GetPluginDir returns the absolute path to a directory under which a given plugin may store data  This directory might not actually 
resource after the volumes release from a PersistentVolumeClaim Recycle will use the provided recorder to write any events that might be interesting to user Its expected that caller will pass these events to the PV being recycled DeletableVolumePlugin is an extended interface of VolumePlugin and is used 

the plugins underlying storage provider AttachableVolumePlugin is an extended interface of VolumePlugin and is used for volumes that require attachment to a node before mounting VolumeHost is an interface that plugins can use to access the kubelet GetPluginDir returns the absolute path to a directory under which 
to the specific functions they are needed in instead of using a catchall VolumeHost interface KubeletVolumeHost is a Kubelet specific interface that plugins can use to access the kubelet SetKubeletError lets plugins set an error on the Kubelet runtime status 

the plugins underlying storage provider AttachableVolumePlugin is an extended interface of VolumePlugin and is used for volumes that require attachment to a node before mounting VolumeHost is an interface that plugins can use to access the kubelet 
NewWrapperMounter finds an appropriate plugin with which to handle the provided spec  This is used to implement volume plugins which wrap other plugins  For example the secret volume is 

the plugins underlying storage provider AttachableVolumePlugin is an extended interface of VolumePlugin and is used for volumes that require attachment to a node before mounting 
GetKubeClient returns a client interface NewWrapperMounter finds an appropriate plugin with which to handle the provided spec  This is used to implement volume plugins which wrap other plugins  For example the secret volume is implemented in terms of the emptyDir volume 

NewProvisioner creates a new volumeProvisioner which knows how to create PersistentVolumes in accordance with the plugins underlying storage provider AttachableVolumePlugin is an extended interface of VolumePlugin and is used for volumes that require attachment to a node before mounting VolumeHost is an interface that plugins can use to access the kubelet GetPluginDir returns the absolute path to a directory under which 
DeletableVolumePlugin is an extended interface of VolumePlugin and is used by persistent volumes that want to be deleted from the cluster after their release from a PersistentVolumeClaim NewDeleter creates a new volumeDeleter which knows how to delete this 

NewProvisioner creates a new volumeProvisioner which knows how to create PersistentVolumes in accordance with the plugins underlying storage provider AttachableVolumePlugin is an extended interface of VolumePlugin and is used for volumes that require attachment to a node before mounting VolumeHost is an interface that plugins can use to access the kubelet 
GetKubeClient returns a client interface NewWrapperMounter finds an appropriate plugin with which to handle the provided spec  This is used to implement volume plugins which 

NewProvisioner creates a new volumeProvisioner which knows how to create PersistentVolumes in accordance with the plugins underlying storage provider AttachableVolumePlugin is an extended interface of VolumePlugin and is used for volumes that require attachment to a node before mounting VolumeHost is an interface that plugins can use to access the kubelet 
to the specific functions they are needed in instead of using a catchall VolumeHost interface KubeletVolumeHost is a Kubelet specific interface that plugins can use to access the kubelet SetKubeletError lets plugins set an error on the Kubelet runtime status 

NewProvisioner creates a new volumeProvisioner which knows how to create PersistentVolumes in accordance with the plugins underlying storage provider AttachableVolumePlugin is an extended interface of VolumePlugin and is used for volumes that require attachment 
the provided spec  This is used to implement volume plugins which wrap other plugins  For example the secret volume is implemented in terms of the emptyDir volume NewWrapperUnmounter finds an appropriate plugin with which to handle 

NewProvisioner creates a new volumeProvisioner which knows how to create PersistentVolumes in accordance with the plugins underlying storage provider AttachableVolumePlugin is an extended interface of VolumePlugin and is used for volumes that require attachment 
NewWrapperMounter finds an appropriate plugin with which to handle the provided spec  This is used to implement volume plugins which wrap other plugins  For example the secret volume is implemented in terms of the emptyDir volume 

NewProvisioner creates a new volumeProvisioner which knows how to create PersistentVolumes in accordance with the plugins underlying storage provider 
NewDeleter creates a new volumeDeleter which knows how to delete this resource in accordance with the underlying storage provider after the 

ProvisionableVolumePlugin is an extended interface of VolumePlugin and is used to create volumes for the cluster NewProvisioner creates a new volumeProvisioner which knows how to create PersistentVolumes in accordance with the plugins underlying storage provider AttachableVolumePlugin is an extended interface of VolumePlugin and is used for volumes that require attachment to a node before mounting VolumeHost is an interface that plugins can use to access the kubelet 
interesting to user Its expected that caller will pass these events to the PV being recycled DeletableVolumePlugin is an extended interface of VolumePlugin and is used 

ProvisionableVolumePlugin is an extended interface of VolumePlugin and is used to create volumes for the cluster NewProvisioner creates a new volumeProvisioner which knows how to create PersistentVolumes in accordance with the plugins underlying storage provider 
the provided spec  This is used to implement volume plugins which wrap other plugins  For example the secret volume is implemented in terms of the emptyDir volume NewWrapperUnmounter finds an appropriate plugin with which to handle 

ProvisionableVolumePlugin is an extended interface of VolumePlugin and is used to create volumes for the cluster NewProvisioner creates a new volumeProvisioner which knows how to create PersistentVolumes in accordance with the plugins underlying storage provider 
NewWrapperMounter finds an appropriate plugin with which to handle the provided spec  This is used to implement volume plugins which wrap other plugins  For example the secret volume is implemented in terms of the emptyDir volume 

ProvisionableVolumePlugin is an extended interface of VolumePlugin and is used to create volumes for the cluster NewProvisioner creates a new volumeProvisioner which knows how to create PersistentVolumes in accordance with 
NewDeleter creates a new volumeDeleter which knows how to delete this resource in accordance with the underlying storage provider after the volumes release from a claim ProvisionableVolumePlugin is an extended interface of VolumePlugin and is 

ProvisionableVolumePlugin is an extended interface of VolumePlugin and is used to create volumes for the cluster 
BlockVolumePlugin is an extend interface of VolumePlugin and is used for block volumes support 

ProvisionableVolumePlugin is an extended interface of VolumePlugin and is used to create volumes for the cluster 
NodeExpandableVolumePlugin is an expanded interface of VolumePlugin and is used for volumes that 

ProvisionableVolumePlugin is an extended interface of VolumePlugin and is used to create volumes for the cluster 
AttachableVolumePlugin is an extended interface of VolumePlugin and is used for volumes that require attachment 

ProvisionableVolumePlugin is an extended interface of VolumePlugin and is used to create volumes for the cluster 
PersistentVolumePlugin is an extended interface of VolumePlugin and is used by volumes that want to provide long term persistence of data GetAccessModes describes the ways a given volume can be accessedmounted RecyclableVolumePlugin is an extended interface of VolumePlugin and is used 

should be ignored by rest of Kubernetes ProvisionableVolumePlugin is an extended interface of VolumePlugin and is used to create volumes for the cluster NewProvisioner creates a new volumeProvisioner which knows how to create PersistentVolumes in accordance with the plugins underlying storage provider AttachableVolumePlugin is an extended interface of VolumePlugin and is used for volumes that require attachment 
by volumes that want to provide long term persistence of data GetAccessModes describes the ways a given volume can be accessedmounted RecyclableVolumePlugin is an extended interface of VolumePlugin and is used 

should be ignored by rest of Kubernetes ProvisionableVolumePlugin is an extended interface of VolumePlugin and is used to create volumes for the cluster NewProvisioner creates a new volumeProvisioner which knows how to create PersistentVolumes in accordance with the plugins underlying storage provider AttachableVolumePlugin is an extended interface of VolumePlugin and is used for volumes that require attachment 
PersistentVolumePlugin is an extended interface of VolumePlugin and is used by volumes that want to provide long term persistence of data 

should be ignored by rest of Kubernetes ProvisionableVolumePlugin is an extended interface of VolumePlugin and is used to create volumes for the cluster NewProvisioner creates a new volumeProvisioner which knows how to create PersistentVolumes in accordance with the plugins underlying storage provider 
RecyclerPodTemplate is pod template that understands how to scrub clean a persistent volume after its release The template is used by plugins which override specific properties of the pod in accordance with that plugin See NewPersistentVolumeRecyclerPodTemplate for the properties that are expected to be overridden RecyclerMinimumTimeout is the minimum amount of time in seconds for the 

should be ignored by rest of Kubernetes ProvisionableVolumePlugin is an extended interface of VolumePlugin and is used to create volumes for the cluster NewProvisioner creates a new volumeProvisioner which knows how to create PersistentVolumes in accordance with the plugins underlying storage provider 
strings in OtherAttributes RecyclerPodTemplate is pod template that understands how to scrub clean a persistent volume after its release The template is used by plugins which override specific properties of the pod in accordance with that plugin See NewPersistentVolumeRecyclerPodTemplate for the properties that are expected to be overridden 

should be ignored by rest of Kubernetes ProvisionableVolumePlugin is an extended interface of VolumePlugin and is used to create volumes for the cluster NewProvisioner creates a new volumeProvisioner which knows how to create PersistentVolumes in accordance with the plugins underlying storage provider 
OK  An instance of config will be given directly to the plugin so config names specific to plugins are unneeded and wrongly expose plugins in this VolumeConfig struct OtherAttributes is a map of string values intended for oneoff configuration of a plugin or config that is only relevant to a single plugin  All values are passed by string and require interpretation by the 

should be ignored by rest of Kubernetes ProvisionableVolumePlugin is an extended interface of VolumePlugin and is used to create volumes for the cluster 
DeletableVolumePlugin is an extended interface of VolumePlugin and is used by persistent volumes that want to be deleted from the cluster after their 

should be ignored by rest of Kubernetes ProvisionableVolumePlugin is an extended interface of VolumePlugin and is used to create volumes for the cluster 
RecyclableVolumePlugin is an extended interface of VolumePlugin and is used by persistent volumes that want to be recycled before being made available 

Name of a volume in external cloud that is being provisioned and thus should be ignored by rest of Kubernetes ProvisionableVolumePlugin is an extended interface of VolumePlugin and is used to create volumes for the cluster NewProvisioner creates a new volumeProvisioner which knows how to create PersistentVolumes in accordance with the plugins underlying storage provider 
a persistent volume after its release The template is used by plugins which override specific properties of the pod in accordance with that plugin See NewPersistentVolumeRecyclerPodTemplate for the properties that are expected to be overridden RecyclerMinimumTimeout is the minimum amount of time in seconds for the 

Name of a volume in external cloud that is being provisioned and thus should be ignored by rest of Kubernetes ProvisionableVolumePlugin is an extended interface of VolumePlugin and is used to create volumes for the cluster NewProvisioner creates a new volumeProvisioner which knows how to create PersistentVolumes in accordance with the plugins underlying storage provider 
RecyclerPodTemplate is pod template that understands how to scrub clean a persistent volume after its release The template is used by plugins which override specific properties of the pod in accordance with that 

Name of a volume in external cloud that is being provisioned and thus should be ignored by rest of Kubernetes ProvisionableVolumePlugin is an extended interface of VolumePlugin and is used to create volumes for the cluster NewProvisioner creates a new volumeProvisioner which knows how to create PersistentVolumes in accordance with the plugins underlying storage provider 
TODO refactor all of this out of volumes when an admin can configure many kinds of provisioners Reclamation policy for a persistent volume Mount options for a persistent volume Suggested PVName of the PersistentVolume to provision This is a generated name guaranteed to be unique in Kubernetes cluster 

Name of a volume in external cloud that is being provisioned and thus should be ignored by rest of Kubernetes ProvisionableVolumePlugin is an extended interface of VolumePlugin and is used to create volumes for the cluster NewProvisioner creates a new volumeProvisioner which knows how to create PersistentVolumes in accordance with 
RecyclerPodTemplate is pod template that understands how to scrub clean a persistent volume after its release The template is used by plugins which override specific properties of the pod in accordance with that plugin See NewPersistentVolumeRecyclerPodTemplate for the properties that are expected to be overridden RecyclerMinimumTimeout is the minimum amount of time in seconds for the 

in accordance with the underlying storage provider after the volumes release from a claim Name of a volume in external cloud that is being provisioned and thus should be ignored by rest of Kubernetes ProvisionableVolumePlugin is an extended interface of VolumePlugin and is used to create volumes for the cluster NewProvisioner creates a new volumeProvisioner which knows how to create PersistentVolumes in accordance with the plugins underlying storage provider 
a persistent volume after its release The template is used by plugins which override specific properties of the pod in accordance with that plugin See NewPersistentVolumeRecyclerPodTemplate for the properties that are expected to be overridden 

in accordance with the underlying storage provider after the volumes release from a claim Name of a volume in external cloud that is being provisioned and thus should be ignored by rest of Kubernetes ProvisionableVolumePlugin is an extended interface of VolumePlugin and is used to create volumes for the cluster NewProvisioner creates a new volumeProvisioner which knows how to create PersistentVolumes in accordance with 
strings in OtherAttributes RecyclerPodTemplate is pod template that understands how to scrub clean a persistent volume after its release The template is used by plugins which override specific properties of the pod in accordance with that 

in accordance with the underlying storage provider after the volumes release from a claim Name of a volume in external cloud that is being provisioned and thus should be ignored by rest of Kubernetes ProvisionableVolumePlugin is an extended interface of VolumePlugin and is used to create volumes for the cluster 
a persistent volume after its release The template is used by plugins which override specific properties of the pod in accordance with that plugin See NewPersistentVolumeRecyclerPodTemplate for the properties that are expected to be overridden RecyclerMinimumTimeout is the minimum amount of time in seconds for the 

NewDeleter creates a new volumeDeleter which knows how to delete this resource in accordance with the underlying storage provider after the volumes release from a claim Name of a volume in external cloud that is being provisioned and thus should be ignored by rest of Kubernetes ProvisionableVolumePlugin is an extended interface of VolumePlugin and is used to create volumes for the cluster 
RecyclerPodTemplate is pod template that understands how to scrub clean a persistent volume after its release The template is used by plugins which override specific properties of the pod in accordance with that 

NewDeleter creates a new volumeDeleter which knows how to delete this resource in accordance with the underlying storage provider after the volumes release from a claim Name of a volume in external cloud that is being provisioned and thus should be ignored by rest of Kubernetes ProvisionableVolumePlugin is an extended interface of VolumePlugin and is used to create volumes for the cluster 
many kinds of provisioners Reclamation policy for a persistent volume Mount options for a persistent volume Suggested PVName of the PersistentVolume to provision This is a generated name guaranteed to be unique in Kubernetes cluster 

NewDeleter creates a new volumeDeleter which knows how to delete this resource in accordance with the underlying storage provider after the volumes release from a claim Name of a volume in external cloud that is being provisioned and thus should be ignored by rest of Kubernetes 
a persistent volume after its release The template is used by plugins which override specific properties of the pod in accordance with that plugin See NewPersistentVolumeRecyclerPodTemplate for the properties that are expected to be overridden 

NewDeleter creates a new volumeDeleter which knows how to delete this resource in accordance with the underlying storage provider after the volumes release from a claim Name of a volume in external cloud that is being provisioned and thus should be ignored by rest of Kubernetes 
strings in OtherAttributes RecyclerPodTemplate is pod template that understands how to scrub clean a persistent volume after its release The template is used by plugins which override specific properties of the pod in accordance with that 

NewDeleter creates a new volumeDeleter which knows how to delete this resource 
NewProvisioner creates a new volumeProvisioner which knows how to 

DeletableVolumePlugin is an extended interface of VolumePlugin and is used by persistent volumes that want to be deleted from the cluster after their release from a PersistentVolumeClaim NewDeleter creates a new volumeDeleter which knows how to delete this resource in accordance with the underlying storage provider after the volumes release from a claim Name of a volume in external cloud that is being provisioned and thus 
RecyclerPodTemplate is pod template that understands how to scrub clean a persistent volume after its release The template is used by plugins which override specific properties of the pod in accordance with that 

DeletableVolumePlugin is an extended interface of VolumePlugin and is used by persistent volumes that want to be deleted from the cluster after their release from a PersistentVolumeClaim 
RecyclableVolumePlugin is an extended interface of VolumePlugin and is used by persistent volumes that want to be recycled before being made available 

after the volumes release from a PersistentVolumeClaim DeletableVolumePlugin is an extended interface of VolumePlugin and is used by persistent volumes that want 
DeletableVolumePlugin is an extended interface of VolumePlugin and is used by persistent volumes that want to be deleted from the cluster after their release from a PersistentVolumeClaim 

NewRecycler creates a new volumeRecycler which knows how to reclaim this resource 
NewProvisioner creates a new volumeProvisioner which knows how to 

NewRecycler creates a new volumeRecycler which knows how to reclaim this resource 
NewDeleter creates a new volumeDeleter which knows how to delete this 

by persistent volumes that want to be recycled before being made available again to new claims NewRecycler creates a new volumeRecycler which knows how to reclaim this resource after the volumes release from a PersistentVolumeClaim DeletableVolumePlugin is an extended interface of VolumePlugin and is used by persistent volumes that want 
interesting to user Its expected that caller will pass these events to the PV being recycled DeletableVolumePlugin is an extended interface of VolumePlugin and is used by persistent volumes that want to be deleted from the cluster after their 

RecyclableVolumePlugin is an extended interface of VolumePlugin and is used 
BlockVolumePlugin is an extend interface of VolumePlugin and is used for block volumes support 

RecyclableVolumePlugin is an extended interface of VolumePlugin and is used 
NodeExpandableVolumePlugin is an expanded interface of VolumePlugin and is used for volumes that 

RecyclableVolumePlugin is an extended interface of VolumePlugin and is used 
DeviceMountableVolumePlugin is an extended interface of VolumePlugin and is used 

RecyclableVolumePlugin is an extended interface of VolumePlugin and is used 
ProvisionableVolumePlugin is an extended interface of VolumePlugin and is 

RecyclableVolumePlugin is an extended interface of VolumePlugin and is used 
DeletableVolumePlugin is an extended interface of VolumePlugin and is used 

RecyclableVolumePlugin is an extended interface of VolumePlugin and is used 
PersistentVolumePlugin is an extended interface of VolumePlugin and is used 

GetAccessModes describes the ways a given volume can be accessedmounted RecyclableVolumePlugin is an extended interface of VolumePlugin and is used by persistent volumes that want to be recycled before being made available again to new claims 
VolumePlugin is an interface to volume plugins that can be used on a kubernetes node eg by kubelet to instantiate and manage volumes Init initializes the plugin  This will be called exactly once before any New calls are made  implementations of plugins may 

GetAccessModes describes the ways a given volume can be accessedmounted RecyclableVolumePlugin is an extended interface of VolumePlugin and is used 
ExpandableVolumePlugin is an extended interface of VolumePlugin and is used for volumes that can be 

by volumes that want to provide long term persistence of data GetAccessModes describes the ways a given volume can be accessedmounted RecyclableVolumePlugin is an extended interface of VolumePlugin and is used by persistent volumes that want to be recycled before being made available again to new claims NewRecycler creates a new volumeRecycler which knows how to reclaim this resource 
Suggested PVName of the PersistentVolume to provision This is a generated name guaranteed to be unique in Kubernetes cluster If you choose not to use it as volume name ensure uniqueness by either combining it with your value or create unique values of your own PVC is reference to the claim that lead to provisioning of a new PV Provisioners must create a PV that would be matched by this PVC 

PersistentVolumePlugin is an extended interface of VolumePlugin and is used by volumes that want to provide long term persistence of data GetAccessModes describes the ways a given volume can be accessedmounted RecyclableVolumePlugin is an extended interface of VolumePlugin and is used 
AttachableVolumePlugin is an extended interface of VolumePlugin and is used for volumes that require attachment 

PersistentVolumePlugin is an extended interface of VolumePlugin and is used 
BlockVolumePlugin is an extend interface of VolumePlugin and is used for block volumes support 

PersistentVolumePlugin is an extended interface of VolumePlugin and is used 
NodeExpandableVolumePlugin is an expanded interface of VolumePlugin and is used for volumes that 

PersistentVolumePlugin is an extended interface of VolumePlugin and is used 
DeviceMountableVolumePlugin is an extended interface of VolumePlugin and is used 

PersistentVolumePlugin is an extended interface of VolumePlugin and is used 
ProvisionableVolumePlugin is an extended interface of VolumePlugin and is 

PersistentVolumePlugin is an extended interface of VolumePlugin and is used 
DeletableVolumePlugin is an extended interface of VolumePlugin and is used 

PersistentVolumePlugin is an extended interface of VolumePlugin and is used 
RecyclableVolumePlugin is an extended interface of VolumePlugin and is used 

 podUID The UID of the enclosing pod PersistentVolumePlugin is an extended interface of VolumePlugin and is used by volumes that want to provide long term persistence of data GetAccessModes describes the ways a given volume can be accessedmounted RecyclableVolumePlugin is an extended interface of VolumePlugin and is used by persistent volumes that want to be recycled before being made available again to new claims 
VolumePluginWithAttachLimits is an extended interface of VolumePlugin that restricts number of volumes that can be attached to a node 

 name The volume name as per the apiVolume spec 
 name The volume name as per the v1Volume spec 

 name The volume name as per the apiVolume spec 
 name The volume name as per the v1Volume spec 

NewCleaner creates a new volumeCleaner from recoverable state 
NewBlockVolumeUnmapper creates a new volumeBlockVolumeUnmapper from recoverable state 

NewCleaner creates a new volumeCleaner from recoverable state 
NewUnmounter creates a new volumeUnmounter from recoverable state 

 spec The apiVolume spec 
 spec The v1Volume spec 

 spec The apiVolume spec 
 spec The v1Volume spec 

NewBuilder creates a new volumeBuilder from an API specification 
NewBlockVolumeMapper creates a new volumeBlockVolumeMapper from an API specification 

NewBuilder creates a new volumeBuilder from an API specification 
NewMounter creates a new volumeMounter from an API specification 

such as examplecomvolume  The kubernetesio namespace is reserved for plugins which are bundled with kubernetes 
such as examplecomvolume and contain exactly one  character The kubernetesio namespace is reserved for plugins which are 

Name returns the plugins name  Plugins should use namespaced names 
Name returns the plugins name  Plugins must use namespaced names 

VolumePlugin is an interface to volume plugins that can be used on a kubernetes node eg by kubelet to instantiate and manage volumes Init initializes the plugin  This will be called exactly once before any New calls are made  implementations of plugins may depend on this 
require expansion on the node via NodeExpand call NodeExpand expands volume on given deviceMountPath and returns true if resize is successful VolumePluginWithAttachLimits is an extended interface of VolumePlugin that restricts number of volumes that can be attached to a node Return maximum number of volumes that can be attached to a node for this plugin 

VolumePlugin is an interface to volume plugins that can be used on a kubernetes node eg by kubelet to instantiate and manage volumes Init initializes the plugin  This will be called exactly once before any New calls are made  implementations of plugins may 
as volume expansion on controller isnt supported but a plugin name is required VolumeConfig is how volume plugins receive configuration  An instance specific to the plugin will be passed to the plugins ProbeVolumePluginsconfig func  Reasonable defaults will be provided by the binary hosting the plugins while allowing override of those default 

VolumePlugin is an interface to volume plugins that can be used on a kubernetes node eg by kubelet to instantiate and manage volumes Init initializes the plugin  This will be called exactly once before any New calls are made  implementations of plugins may 
Recycle will use the provided recorder to write any events that might be interesting to user Its expected that caller will pass these events to the PV being recycled DeletableVolumePlugin is an extended interface of VolumePlugin and is used by persistent volumes that want to be deleted from the cluster after their 

VolumePlugin is an interface to volume plugins that can be used on a kubernetes node eg by kubelet to instantiate and manage volumes Init initializes the plugin  This will be called exactly once before any New calls are made  implementations of plugins may 
GetAccessModes describes the ways a given volume can be accessedmounted RecyclableVolumePlugin is an extended interface of VolumePlugin and is used by persistent volumes that want to be recycled before being made available again to new claims 

Tags to attach to the real volume in the cloud provider  eg AWS EBS VolumePlugin is an interface to volume plugins that can be used on a kubernetes node eg by kubelet to instantiate and manage volumes Init initializes the plugin  This will be called exactly once before any New calls are made  implementations of plugins may 
VolumeConfig is how volume plugins receive configuration  An instance specific to the plugin will be passed to the plugins ProbeVolumePluginsconfig func  Reasonable defaults will be provided by the binary hosting the plugins while allowing override of those default values  Those config values are then set to an instance of VolumeConfig and passed to the plugin 

Tags to attach to the real volume in the cloud provider  eg AWS EBS VolumePlugin is an interface to volume plugins that can be used on a kubernetes node eg by kubelet to instantiate and manage volumes Init initializes the plugin  This will be called exactly once before any New calls are made  implementations of plugins may 
Host however the end result should be that specific Volume Hosts are passed to the specific functions they are needed in instead of using a catchall VolumeHost interface KubeletVolumeHost is a Kubelet specific interface that plugins can use to access the kubelet SetKubeletError lets plugins set an error on the Kubelet runtime status 

Tags to attach to the real volume in the cloud provider  eg AWS EBS VolumePlugin is an interface to volume plugins that can be used on a kubernetes node eg by kubelet to instantiate and manage volumes Init initializes the plugin  This will be called exactly once before any New calls are made  implementations of plugins may 
interesting to user Its expected that caller will pass these events to the PV being recycled DeletableVolumePlugin is an extended interface of VolumePlugin and is used by persistent volumes that want to be deleted from the cluster after their release from a PersistentVolumeClaim NewDeleter creates a new volumeDeleter which knows how to delete this 

Tags to attach to the real volume in the cloud provider  eg AWS EBS VolumePlugin is an interface to volume plugins that can be used on a kubernetes node eg by kubelet to instantiate and manage volumes Init initializes the plugin  This will be called exactly once before any New calls are made  implementations of plugins may 
resource after the volumes release from a PersistentVolumeClaim Recycle will use the provided recorder to write any events that might be interesting to user Its expected that caller will pass these events to the PV being recycled DeletableVolumePlugin is an extended interface of VolumePlugin and is used 

Tags to attach to the real volume in the cloud provider  eg AWS EBS VolumePlugin is an interface to volume plugins that can be used on a kubernetes node eg by kubelet to instantiate and manage volumes Init initializes the plugin  This will be called exactly once before any New calls are made  implementations of plugins may 
RecyclableVolumePlugin is an extended interface of VolumePlugin and is used by persistent volumes that want to be recycled before being made available again to new claims Recycle knows how to reclaim this resource after the volumes release from a PersistentVolumeClaim Recycle will use the provided recorder to write any events that might be 

Unique name of Kubernetes cluster Tags to attach to the real volume in the cloud provider  eg AWS EBS VolumePlugin is an interface to volume plugins that can be used on a kubernetes node eg by kubelet to instantiate and manage volumes Init initializes the plugin  This will be called exactly once before any New calls are made  implementations of plugins may 
OK  An instance of config will be given directly to the plugin so config names specific to plugins are unneeded and wrongly expose plugins in this VolumeConfig struct OtherAttributes is a map of string values intended for oneoff 

Unique name of Kubernetes cluster Tags to attach to the real volume in the cloud provider  eg AWS EBS VolumePlugin is an interface to volume plugins that can be used on a kubernetes node eg by kubelet to instantiate and manage volumes Init initializes the plugin  This will be called exactly once before any New calls are made  implementations of plugins may 
required VolumeConfig is how volume plugins receive configuration  An instance specific to the plugin will be passed to the plugins ProbeVolumePluginsconfig func  Reasonable defaults will be provided by the binary hosting the plugins while allowing override of those default values  Those config values are then set to an instance of VolumeConfig 

Unique name of Kubernetes cluster Tags to attach to the real volume in the cloud provider  eg AWS EBS VolumePlugin is an interface to volume plugins that can be used on a kubernetes node eg by kubelet to instantiate and manage volumes Init initializes the plugin  This will be called exactly once before any New calls are made  implementations of plugins may 
Returns an interface that should be used to execute subpath operations Returns options to pass for proxyutil filtered dialers VolumePluginMgr tracks registered plugins Spec is an internal representation of a volume  All API volume types translate to Spec Name returns the name of either Volume or PersistentVolume one of which must not be nil IsKubeletExpandable returns true for volume types that can be expanded only by the node 

Unique name of Kubernetes cluster Tags to attach to the real volume in the cloud provider  eg AWS EBS VolumePlugin is an interface to volume plugins that can be used on a kubernetes node eg by kubelet to instantiate and manage volumes Init initializes the plugin  This will be called exactly once before any New calls are made  implementations of plugins may 
interesting to user Its expected that caller will pass these events to the PV being recycled DeletableVolumePlugin is an extended interface of VolumePlugin and is used by persistent volumes that want to be deleted from the cluster after their 

Unique name of Kubernetes cluster Tags to attach to the real volume in the cloud provider  eg AWS EBS VolumePlugin is an interface to volume plugins that can be used on a kubernetes node eg by kubelet to instantiate and manage volumes Init initializes the plugin  This will be called exactly once 
interesting to user Its expected that caller will pass these events to the PV being recycled DeletableVolumePlugin is an extended interface of VolumePlugin and is used by persistent volumes that want to be deleted from the cluster after their release from a PersistentVolumeClaim NewDeleter creates a new volumeDeleter which knows how to delete this 

Unique name of Kubernetes cluster Tags to attach to the real volume in the cloud provider  eg AWS EBS VolumePlugin is an interface to volume plugins that can be used on a 
to the specific functions they are needed in instead of using a catchall VolumeHost interface KubeletVolumeHost is a Kubelet specific interface that plugins can use to access the kubelet SetKubeletError lets plugins set an error on the Kubelet runtime status 

Reclamation policy for a persistent volume PVName of the appropriate PersistentVolume Used to generate cloud volume name Unique name of Kubernetes cluster Tags to attach to the real volume in the cloud provider  eg AWS EBS VolumePlugin is an interface to volume plugins that can be used on a 
used to create volumes for the cluster NewProvisioner creates a new volumeProvisioner which knows how to create PersistentVolumes in accordance with the plugins underlying storage provider AttachableVolumePlugin is an extended interface of VolumePlugin and is used for volumes that require attachment to a node before mounting 

AccessModes of a volume Reclamation policy for a persistent volume PVName of the appropriate PersistentVolume Used to generate cloud volume name Unique name of Kubernetes cluster Tags to attach to the real volume in the cloud provider  eg AWS EBS VolumePlugin is an interface to volume plugins that can be used on a 
NewProvisioner creates a new volumeProvisioner which knows how to create PersistentVolumes in accordance with the plugins underlying storage provider AttachableVolumePlugin is an extended interface of VolumePlugin and is used for volumes that require attachment to a node before mounting 

AccessModes of a volume Reclamation policy for a persistent volume PVName of the appropriate PersistentVolume Used to generate cloud volume name Unique name of Kubernetes cluster Tags to attach to the real volume in the cloud provider  eg AWS EBS VolumePlugin is an interface to volume plugins that can be used on a 
NewDeleter creates a new volumeDeleter which knows how to delete this resource in accordance with the underlying storage provider after the volumes release from a claim ProvisionableVolumePlugin is an extended interface of VolumePlugin and is used to create volumes for the cluster 

This is a temporary measure in order to set the rootContext of tmpfs mounts correctly it will be replaced and expanded on by future SecurityContext work The attributes below are required by volumeProvisioner TODO refactor all of this out of volumes when an admin can configure many kinds of provisioners Capacity is the size of a volume 
a persistent volume after its release The template is used by plugins which override specific properties of the pod in accordance with that plugin See NewPersistentVolumeRecyclerPodTemplate for the properties that are expected to be overridden RecyclerMinimumTimeout is the minimum amount of time in seconds for the 

The rootcontext to use when performing mounts for a volume This is a temporary measure in order to set the rootContext of tmpfs mounts correctly it will be replaced and expanded on by future SecurityContext work The attributes below are required by volumeProvisioner TODO refactor all of this out of volumes when an admin can configure many kinds of provisioners 
OK  An instance of config will be given directly to the plugin so config names specific to plugins are unneeded and wrongly expose plugins in this VolumeConfig struct OtherAttributes is a map of string values intended for oneoff configuration of a plugin or config that is only relevant to a single plugin  All values are passed by string and require interpretation by the 

The rootcontext to use when performing mounts for a volume This is a temporary measure in order to set the rootContext of tmpfs mounts correctly it will be replaced and expanded on by future SecurityContext work The attributes below are required by volumeProvisioner TODO refactor all of this out of volumes when an admin can configure many kinds of provisioners 
DeletableVolumePlugin is an extended interface of VolumePlugin and is used by persistent volumes that want to be deleted from the cluster after their release from a PersistentVolumeClaim NewDeleter creates a new volumeDeleter which knows how to delete this resource in accordance with the underlying storage provider after the 

The rootcontext to use when performing mounts for a volume This is a temporary measure in order to set the rootContext of tmpfs mounts correctly it will be replaced and expanded on by future SecurityContext work The attributes below are required by volumeProvisioner 
a persistent volume after its release The template is used by plugins which override specific properties of the pod in accordance with that plugin See NewPersistentVolumeRecyclerPodTemplate for the properties that are expected to be overridden 

VolumeOptions contains option information about a volume The rootcontext to use when performing mounts for a volume This is a temporary measure in order to set the rootContext of tmpfs mounts correctly it will be replaced and expanded on by future SecurityContext work 
by persistent volumes that want to be deleted from the cluster after their release from a PersistentVolumeClaim NewDeleter creates a new volumeDeleter which knows how to delete this resource in accordance with the underlying storage provider after the volumes release from a claim ProvisionableVolumePlugin is an extended interface of VolumePlugin and is 

VolumeOptions contains option information about a volume The rootcontext to use when performing mounts for a volume This is a temporary measure in order to set the rootContext of tmpfs mounts correctly it will be replaced and expanded on by future SecurityContext work 
DeletableVolumePlugin is an extended interface of VolumePlugin and is used by persistent volumes that want to be deleted from the cluster after their release from a PersistentVolumeClaim NewDeleter creates a new volumeDeleter which knows how to delete this resource in accordance with the underlying storage provider after the volumes release from a claim 

VolumeOptions contains option information about a volume The rootcontext to use when performing mounts for a volume This is a temporary measure in order to set the rootContext of tmpfs mounts correctly 
Reclamation policy for a persistent volume Mount options for a persistent volume Suggested PVName of the PersistentVolume to provision This is a generated name guaranteed to be unique in Kubernetes cluster 

limitations under the License VolumeOptions contains option information about a volume The rootcontext to use when performing mounts for a volume This is a temporary measure in order to set the rootContext of tmpfs mounts correctly it will be replaced and expanded on by future SecurityContext work 
DeletableVolumePlugin is an extended interface of VolumePlugin and is used by persistent volumes that want to be deleted from the cluster after their release from a PersistentVolumeClaim NewDeleter creates a new volumeDeleter which knows how to delete this resource in accordance with the underlying storage provider after the 

limitations under the License VolumeOptions contains option information about a volume The rootcontext to use when performing mounts for a volume This is a temporary measure in order to set the rootContext of tmpfs mounts correctly 
release from a PersistentVolumeClaim NewDeleter creates a new volumeDeleter which knows how to delete this resource in accordance with the underlying storage provider after the volumes release from a claim ProvisionableVolumePlugin is an extended interface of VolumePlugin and is used to create volumes for the cluster 

See the License for the specific language governing permissions and limitations under the License VolumeOptions contains option information about a volume The rootcontext to use when performing mounts for a volume This is a temporary measure in order to set the rootContext of tmpfs mounts correctly it will be replaced and expanded on by future SecurityContext work 
a persistent volume after its release The template is used by plugins which override specific properties of the pod in accordance with that plugin See NewPersistentVolumeRecyclerPodTemplate for the properties that are expected to be overridden 

unionDockerKeyring delegates to a set of keyrings 
UnionDockerKeyring delegates to a set of keyrings 

globUrldockerio targetUrlnotrightio    no match 
	globURLdockerio targetURLnotrightio    no match 

globUrldockerio targetUrlblahdockerio  match 
	globURLdockerio targetURLblahdockerio  match 

check whether the given target url matches the glob url which may have 
URLsMatch checks whether the given target url matches the glob url which may have 

split the host name into parts as well as the port 
SplitURL splits the host name into parts as well as the port 

first For example if for the given image quayiocoreosetcd 
first For example if for the given image gcrioetcddevelopmentetcd 

lazyDockerKeyring is an implementation of DockerKeyring that lazily 
providersDockerKeyring is an implementation of DockerKeyring that 

The pod is deleted and also not active on the node The pod directory should be removed 
Pretend the pod is deleted from apiserver but is still active on the node The pod directory should not be removed 

Pretend the pod is deleted from apiserver but is still active on the node The pod directory should not be removed 
The pod is deleted and also not active on the node The pod directory should be removed 

Duplicated should be ignored 
should be removed 

Duplicated should be ignored 
should be removed 

Tests that we handle exceeded resources correctly by setting the failed status in status map 
Tests that we handle not matching labels selector correctly by setting the failed status in status map 

Tests that we handle exceeded resources correctly by setting the failed status in status map 
Tests that we handle not matching labels selector correctly by setting the failed status in status map 

Tests that we handle exceeded resources correctly by setting the failed status in status map 
Tests that we handle host name conflicts correctly by setting the failed status in status map 

Tests that we handle exceeded resources correctly by setting the failed status in status map 
Tests that we handle port conflicts correctly by setting the failed status in status map 

Tests that we handle not matching labels selector correctly by setting the failed status in status map 
Tests that we handle exceeded resources correctly by setting the failed status in status map 

Tests that we handle not matching labels selector correctly by setting the failed status in status map 
Tests that we handle host name conflicts correctly by setting the failed status in status map 

Tests that we handle not matching labels selector correctly by setting the failed status in status map 
Tests that we handle port conflicts correctly by setting the failed status in status map 

Tests that we handle port conflicts correctly by setting the failed status in status map 
Tests that we handle exceeded resources correctly by setting the failed status in status map 

Tests that we handle port conflicts correctly by setting the failed status in status map 
Tests that we handle not matching labels selector correctly by setting the failed status in status map 

Tests that we handle port conflicts correctly by setting the failed status in status map 
Tests that we handle not matching labels selector correctly by setting the failed status in status map 

Tests that we handle port conflicts correctly by setting the failed status in status map 
Tests that we handle host name conflicts correctly by setting the failed status in status map 

Both the global cache timestamp and the modified time are older 
Both the global cache timestamp and the modified time are newer 

time is newer than the given timestamp This means that the 
time is older than the given timestamp This means that the 

Global cache timestamp is older but the pod entry modified 
Global cache timestamp is newer but the pod entry modified 

time is older than the given timestamp This means that the 
time is newer than the given timestamp This means that the 

Global cache timestamp is newer but the pod entry modified 
Global cache timestamp is older but the pod entry modified 

Both the global cache timestamp and the modified time are newer 
Both the global cache timestamp and the modified time are older 

Runs GetMetrics Once and caches the result  Will not cache result if there is an error 
caches the result Will not cache result if there is an error 

NewCachedMetrics creates a new cachedMetrics wrapping another MetricsProvider and caching the results 
caches the result NewCachedMetrics creates a new cachedMetrics wrapping another 

>>comment_elmo_lines_files
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
>>comment_roberta_lines_files
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
If conn has no activity for serviceInfotimeout since last ReadWrite it shoule be closed because of timeout 
If conn has no activity for serviceInfotimeout since last ReadWrite it should be closed because of timeout 

When connecting to a UDP service endpoint there shoule be a Conn for proxy 
When connecting to a UDP service endpoint there should be a Conn for proxy 

Resource takes an unqualified resource and returns back a Group qualified GroupResource 
Resource takes an unqualified resource and returns a Group qualified GroupResource 

Kind takes an unqualified kind and returns back a Group qualified GroupKind 
Kind takes an unqualified kind and returns a Group qualified GroupKind 

udpProxySocket implements proxySocket  Close is implemented by netUDPConn  When Close is called 
udpProxySocket implements ProxySocket  Close is implemented by netUDPConn  When Close is called 

proxyTCP proxies data bidirectionally between in and out 
ProxyTCP proxies data bidirectionally between in and out 

tcpProxySocket implements proxySocket  Close is implemented by netListener  When Close is called 
tcpProxySocket implements ProxySocket  Close is implemented by netListener  When Close is called 

ListenPort returns the host port that the proxySocket is listening on 
ListenPort returns the host port that the ProxySocket is listening on 

Close stops the proxySocket from accepting incoming connections 
Close stops the ProxySocket from accepting incoming connections 

Addr gets the netAddr for a proxySocket 
Addr gets the netAddr for a ProxySocket 

Returns a NodeConfig that is being used by the container manager 
GetNodeConfig returns a NodeConfig that is being used by the container manager 

tcShaper provides an implementation of the BandwidthShaper interface on Linux using the tc tool 
tcShaper provides an implementation of the Shaper interface on Linux using the tc tool 

the originial logic of isPodRunning happens to return true when podstatus is empty so the test can always pass 
the original logic of isPodRunning happens to return true when podstatus is empty so the test can always pass 

remove rbd lock 
Remove rbd lock if found 

remove rbd lock 
Remove a lock if found rbd lock remove 

remove a lock rbd lock remove 
Remove rbd lock if found 

remove a lock rbd lock remove 
Remove a lock if found rbd lock remove 

construct lock id using host name and a magic prefix 
Construct lock id using host name and a magic prefix 

make a directory like varlibkubeletpluginskubernetesiopodrbdpoolimageimage 
Make a directory like varlibkubeletpluginskubernetesiorbdvolumeDevicespoolimageimage 

make a directory like varlibkubeletpluginskubernetesiopodrbdpoolimageimage 
Make a directory like varlibkubeletpluginskubernetesiorbdmountspoolimageimage 

stat a path if not exists retry maxRetries times 
Stat a path if it doesnt exist retry maxRetries times 

found a match check if device exists 
Found a match check if device exists 

first match pool then match name 
First match pool then match name 

pool and name format 
Pool and name format 

search sysbus for rbd device that matches given pool and image 
Search sysbus for rbd device that matches given pool and image 

Build a slice of iptables args for an fromnonlocal publicport rule 
Build a slice of iptables args for a fromhost publicport rule 

Build a slice of iptables args for a fromhost publicport rule 
Build a slice of iptables args for an fromnonlocal publicport rule 

TODO Should we just reuse iptablesContainerPortalArgs TODO Can we REDIRECT with IPv6 
TODO Should we just reuse iptablesHostPortalArgs TODO Can we DNAT with IPv6 

Build a slice of iptables args for a fromcontainer publicport rule See iptablesContainerPortalArgs 
Build a slice of iptables args for a fromcontainer portal rule 

Build a slice of iptables args for a fromcontainer publicport rule 
Build a slice of iptables args for an fromnonlocal publicport rule 

Build a slice of iptables args for a fromcontainer publicport rule 
Build a slice of iptables args for a fromhost publicport rule 

Build a slice of iptables args for a fromhost portal rule 
Build a slice of iptables args for a fromcontainer portal rule 

Build a slice of iptables args for a fromcontainer portal rule 
Build a slice of iptables args for a fromhost portal rule 

TODO check health of the socket  What if ProxyLoop exited 
TODO check health of the socket What if ProxyLoop exited 

addServiceOnPort starts listening for a new service returning the serviceInfo 
addServiceOnPortInternal starts listening for a new service returning the ServiceInfo 

Sync is called to immediately synchronize the proxier state to iptables 
Sync is called to synchronize the proxier state to iptables as soon as possible 

assert Proxier is a ProxyProvider 
assert Proxier is a proxyProvider 

MkDir will will call osMkdir to create a directory 
MkdirAll will call osMkdirAll to create a directory 

RealOS is used to dispatch the real system level operaitons 
RealOS is used to dispatch the real system level operations 

secretVolumeBuilder handles retrieving secrets from the API server 
secretVolumeMounter handles retrieving secrets from the API server 

ProbeVolumePlugin is the entry point for plugin detection in a package 
ProbeVolumePlugins is the entry point for plugin detection in a package 

specifying a namespaceresource removes the  match on nonresource path 
specifying a namespace removes the  match on nonresource path 

specifying a namespace removes the  match on nonresource path 
specifying a namespaceresource removes the  match on nonresource path 

updateReplicaCount attempts to update the StatusReplicas of the given ReplicaSet with a single GETPUT retry 
updateReplicaSetStatus attempts to update the StatusReplicas of the given ReplicaSet with a single GETPUT retry 

Apply default values and validate pods 
Apply default values and validate the pod 

Apply default values and validate the pod 
Apply default values and validate pods 

Returns a readonly copy of the system capabilities 
Get returns a readonly copy of the system capabilities 

SetCapabilitiesForTests  Convenience method for testing  This should only be called from tests 
SetForTests sets capabilities for tests  Convenience method for testing  This should only be called from tests 

Setup the capability set  It wraps Initialize for improving usibility 
Setup the capability set  It wraps Initialize for improving usability 

PrivilegedSources defines the pod sources allowed to make privileged requests for certain types of capabilities like host networking sharing the host IPC namespace and sharing the host PID namespace List of pod sources for which using host network is allowed 
Pod sources from which to allow privileged capabilities like host networking sharing the host IPC namespace and sharing the host PID namespace 

Pod sources from which to allow privileged capabilities like host networking sharing the host IPC namespace and sharing the host PID namespace 
PrivilegedSources defines the pod sources allowed to make privileged requests for certain types of capabilities like host networking sharing the host IPC namespace and sharing the host PID namespace List of pod sources for which using host network is allowed 

Assuemes that the container has Custom Metrics enabled if it has etccustommetrics directory 
Assumes that the container has Custom Metrics enabled if it has etccustommetrics directory 

resolvePort attempts to turn a IntOrString port reference into a concrete port number 
resolvePort attempts to turn an IntOrString port reference into a concrete port number 

localhost93path 
localhost93pathbar 

localhost93path 
localhost93 

localhost93 
localhost93path 

pairs and returns a a populated stringstring httpHeader map 
pairs and returns a populated stringstring httpHeader map 

Prober helps to check the livenessreadiness of a container 
Prober helps to check the livenessreadinessstartup of a container 

the deletion is complete Any error returned indicates the volume has failed to be reclaimed A nil return indicates success This method should block until completion 
to this method should block until the deletion is complete Any error returned indicates the volume has failed to be reclaimed A nil return indicates success 

Deleter removes the resource from the underlying storage provider  Calls to this method should block until the deletion is complete Any error returned indicates the volume has failed to be reclaimed 
to this method should block until the deletion is complete Any error returned indicates the volume has failed to be reclaimed A nil return 

Deleter removes the resource from the underlying storage provider  Calls to this method should block until the deletion is complete Any error returned indicates the volume has failed to be reclaimed 
Deleter removes the resource from the underlying storage provider Calls to this method should block until the deletion is complete Any error 

Any error returned indicates the volume has failed to be reclaimed  A nil return indicates success 
returned indicates the volume has failed to be reclaimed A nil return indicates success 

TearDown unmounts the volume from the specified directory and 
TearDown unmounts the volume from a selfdetermined directory and 

TearDown unmounts the volume from a selfdetermined directory and 
TearDown unmounts the volume from the specified directory and 

The mount point and its content should be owned by fsGroup so that it can be accessed by the pod This may be called more than once so implementations must be idempotent 
content should be owned by fsUser or fsGroup so that it can be accessed by the pod This may be called more than once so implementations must be idempotent 

The mount point and its content should be owned by fsGroup so that it can be accessed by the pod This may be called more than once so implementations must be 
selfdetermined directory path The mount point and its content should be owned by fsUser or fsGroup so that it can be accessed by the pod This may be called more than once so implementations must be idempotent 

SetUpAt prepares and mountsunpacks the volume to the 
SetUp prepares and mountsunpacks the volume to a 

content should be owned by fsGroup so that it can be accessed by the pod This may be called more than once so implementations must be idempotent 
The mount point and its content should be owned by fsUser fsGroup so that it can be accessed by the pod This may be called more than once so implementations must be 

content should be owned by fsGroup so that it can be 
content should be owned by fsUser or fsGroup so that it can be 

selfdetermined directory path The mount point and its content should be owned by fsGroup so that it can be accessed by the pod This may be called more than once so 
The mount point and its content should be owned by fsUser fsGroup so that it can be accessed by the pod This may be called more than once so implementations must be 

SetUp prepares and mountsunpacks the volume to a 
SetUpAt prepares and mountsunpacks the volume to the 

space on the underlying storage and is shared with host processes and other Volumes 
on the underlying storage and is shared with host processes and other volumes 

For Volumes that share a filesystem with the host eg emptydir hostpath this is the available 
For volumes that share a filesystem with the host eg emptydir hostpath 

Available represents the storage space available bytes for the Volume For Volumes that share a filesystem with the host eg emptydir hostpath this is the available 
Capacity represents the total capacity bytes of the volumes underlying storage For Volumes that share a filesystem with the host eg emptydir hostpath this is the size of the underlying storage 

For Volumes that share a filesystem with the host eg emptydir hostpath this is the size 
For volumes that share a filesystem with the host eg emptydir hostpath 

For Volumes that share a filesystem with the host eg emptydir hostpath this is the size 
Volume For Volumes that share a filesystem with the host eg emptydir hostpath this is the available space on the underlying 

GetMetrics returns the Metrics for the Volume  Maybe expensive for some implementations 
GetMetrics returns the Metrics for the Volume Maybe expensive for some implementations 

MetricsProvider embeds methods for exposing metrics eg usedavailable space 
MetricsProvider embeds methods for exposing metrics eg used available space 

MetricsProvider embeds methods for exposing metrics eg usedavailable space 
MetricsProvider embeds methods for exposing metrics eg used available space 

Detaches the disk from the kubelets host machine 
Detaches the block disk from the kubelets host machine 

rbd volume implements diskManager calls diskSetup when creating a volume and calls diskTearDown inside volume cleaner 
rbd volume implements diskManager calls diskSetup when creating a volume and calls diskTearDown inside volume unmounter 

diskManager interface and diskSetupTearDown functions abtract commonly used procedures to setup a block volume 
diskManager interface and diskSetupTearDown functions abstract commonly used procedures to setup a block volume 

uses the specified storage to retrieve service accounts and secrets 
uses the specified client to retrieve service accounts and secrets 

A type to help sort container statuses based on container names 
SortedContainerStatuses is a type to help sort container statuses based on container names 

GetString returns the time in the string format using the RFC3339Nano 
GetString returns the time in the string format using the RFC3339NanoFixed 

and converts it to a Timestamp object Get returns the time as timeTime GetString returns the time in the string format using the RFC3339Nano 
the time using RFC3339Nano NewTimestamp returns a Timestamp object using the current time ConvertToTimestamp takes a string parses it using the RFC3339NanoLenient layout 

ConvertToTimestamp takes a string parses it using the RFC3339Nano layout 
ConvertToTimestamp takes a string parses it using the RFC3339NanoLenient layout 

the nice effect of flushing the chain  Then we can remove the 
for it which has the nice effect of flushing the chain Then we can remove the chain 

assumes proxiermu is held 
Assumes proxiermu is held 

Sync is called to immediately synchronize the proxier state to iptables 
Sync is called to synchronize the proxier state to iptables as soon as possible 

Proxier implements ProxyProvider 
Proxier implements proxyProvider 

Check for the required sysctls  We dont care about the value just 
IsCompatible checks for the required sysctls  We dont care about the value just 

CanUseIptablesProxier returns true if we should use the iptables Proxier 
CanUseIPTablesProxier returns true if we should use the iptables Proxier 

newSourceApiserverFromLW holds creates a config source that watches and pulls from the apiserver 
NewSourceApiserver creates a config source that watches and pulls from the apiserver 

NewSourceApiserver creates a config source that watches and pulls from the apiserver 
newSourceApiserverFromLW holds creates a config source that watches and pulls from the apiserver 

Create and start the cAdvisor container manager 
Create the cAdvisor container manager 

Pods returns a string representating a list of pods in a human readable format 
PodDesc returns a string representing a pod in a consistent human readable format 

Pods returns a string representating a list of pods in a human readable format 
Pod returns a string representing a pod in a consistent human readable format 

If the PodSyncResult is added an error PodSyncResult it should be error 
If the PodSyncResult contains error result it should be error 

If the PodSyncResult is failed it should be error 
If the PodSyncResult contains error result it should be error 

If the PodSyncResult contains error result it should be error 
If the PodSyncResult is added an error PodSyncResult it should be error 

If the PodSyncResult contains error result it should be error 
If the PodSyncResult is failed it should be error 

downwardAPIVolumeCleander handles cleaning up downwardAPI volumes 
downwardAPIVolumeCleaner handles cleaning up downwardAPI volumes 

collectData collects requested downwardAPI in data map 
CollectData collects requested downwardAPI in data map 

downwardAPIVolumeBuilder fetches info from downward API from the pod 
downwardAPIVolumeMounter fetches info from downward API from the pod 

It accepts set add and remove operations of services via channels and invokes registered handlers on change 
It accepts set add and remove operations of node via channels and invokes registered handlers on change 

NewEndpointsConfig creates a new EndpointsConfig 
EndpointSliceConfig tracks a set of endpoints configurations NewEndpointSliceConfig creates a new EndpointSliceConfig 

It accepts set add and remove operations of endpoints via channels and invokes registered handlers on change 
It accepts set add and remove operations of node via channels and invokes registered handlers on change 

EndpointsConfig tracks a set of endpoints configurations 
EndpointSliceConfig tracks a set of endpoints configurations 

OnEndpointsUpdate gets called when endpoints configuration is changed for a given 
OnEndpointsUpdate is called whenever modification of an existing 

Create an event recorder to record objects event except implicitly required containers like infra container 
FilterEventRecorder creates an event recorder to record objects event except implicitly required containers like infra container 

EnvVarsToMap constructs a map of environment name to value from a slice 
envVarsToMap constructs a map of environment name to value from a slice 

Always restart container in unknown state now 
Always restart container in the unknown or in the created state 

Gets all validated sources from the specified sources 
GetValidatedSources gets all validated sources from the specified sources 

Updates from a file Updates from querying a web page Updates from Kubernetes API Server Updates from all sources 
Filesource idenitified updates from a file HTTPSource identifies updates from querying a web page ApiserverSource identifies updates from Kubernetes API Server AllSource identifies updates from all sources 

This is the current pod configuration 
SET is the current pod configuration 

Detaches the disk from the kubelets host machine 
Detaches the block disk from the kubelets host machine 

RestoreAll is part of Interface 
Restore is part of Interface 

Restore is part of Interface 
RestoreAll is part of Interface 

New returns a new Interface which will exec iptables 
newInternal returns a new Interface which will exec iptables and allows the 

data should be formatted like the output of Save 
data should be formatted like the output of SaveInto 

IsIpv6 returns true if this is managing ipv6 tables 
IsIPv6 returns true if this is managing ipv6 tables 

An injectable interface for running iptables commands  Implementations must be goroutinesafe 
Interface is an injectable interface for running iptables commands  Implementations must be goroutinesafe 

changed labels the new ReplicaSet will not be woken up till the periodic resync 
changed labels the new deployment will not be woken up till the periodic resync 

changed labels the new ReplicaSet will not be woken up till the periodic resync 
changed labels the new deployment will not be woken up till the periodic resync 

the deleted keyvalue Note that this value might be stale If the pod changed labels the new ReplicaSet will not be woken up till the periodic 
the deleted keyvalue Note that this value might be stale If the ReplicaSet changed labels the new deployment will not be woken up till the periodic resync 

the deleted keyvalue Note that this value might be stale If the pod 
the deleted keyvalue Note that this value might be stale If the Pod 

getDeploymentForPod returns the deployment managing the ReplicaSet that manages the given Pod 
getDeploymentForPod returns the deployment managing the given Pod 

podStoreSynced returns true if the pod store has been synced at least once 
podListerSynced returns true if the pod store has been synced at least once 

rsStoreSynced returns true if the ReplicaSet store has been synced at least once 
rsListerSynced returns true if the ReplicaSet store has been synced at least once 

dockeriov2 
dockeriov1 

dockeriov2 
dockerio 

dockeriov1 
dockeriov2 

dockeriov1 
dockerio 

prefixdocker8888 
dockerio8888 

dockerio8888 
prefixdocker8888 

dockerio 
dockeriov2 

dockerio 
dockeriov1 

ReplicaSetsByCreationTimestamp sorts a list of ReplicationSets by creation timestamp using their names as a tie breaker 
ReplicaSetsByCreationTimestamp sorts a list of ReplicaSet by creation timestamp using their names as a tie breaker 

ReplicaSetsByCreationTimestamp sorts a list of ReplicationSets by creation timestamp using their names as a tie breaker 
ControllersByCreationTimestamp sorts a list of ReplicationControllers by creation timestamp using their names as a tie breaker 

expcept theres not possibility for error since we know the exact type 
except theres not possibility for error since we know the exact type 

4 Been ready for empty time  less time  more time 
4 Been ready for more time  less time  empty time 

3 Not ready  ready 
3 ready  not ready 

2 PodPending  PodUnknown  PodRunning 
2 PodRunning  PodUnknown  PodPending 

1 Unassigned  assigned 
1 assigned  unassigned 

DeletionTimestamp on an object as a delete To do so consistenly one needs 
DeletionTimestamp on an object as a delete To do so consistently one needs 

ValidSecurityContextWithContainerDefaults creates a valid security context provider based on 
ValidInternalSecurityContextWithContainerDefaults creates a valid security context provider based on 

a specific Google Compute Engine metadata key googledockercfgurl 
Google Compute Engine metadata key googledockercfg 

Google Compute Engine metadata key googledockercfg 
a specific Google Compute Engine metadata key googledockercfgurl 

A DockerConfigProvider that reads its configuration from Google Compute Engine metadata A DockerConfigProvider that reads its configuration from a specific 
DockerConfigKeyProvider is a DockerConfigProvider that reads its configuration from a specific Google Compute Engine metadata key googledockercfg DockerConfigURLKeyProvider is a DockerConfigProvider that reads its configuration from a URL read from 

Package service provides EndpointController implementation 
Package endpoint provides EndpointController implementation 

Resource takes an unqualified resource and returns back a Group qualified GroupResource 
Resource takes an unqualified resource and returns a Group qualified GroupResource 

Kind takes an unqualified kind and returns back a Group qualified GroupKind 
Kind takes an unqualified kind and returns a Group qualified GroupKind 

configMapVolumeBuilder handles retrieving secrets from the API server 
configMapVolumeMounter handles retrieving secrets from the API server 

ProbeVolumePlugin is the entry point for plugin detection in a package 
ProbeVolumePlugins is the entry point for plugin detection in a package 

extract portal and iqn from device path 
Extract the portal and iqn from device path 

extract portal and iqn from device path 
Extract the portal and iqn from device path 

this portaliqn are no longer referenced log out 
This portaliqniface is no longer referenced log out 

this portaliqn are no longer referenced log out 
This portaliqniface is no longer referenced log out 

if device is no longer used see if need to logout the target 
If device is no longer used see if need to logout the target 

if device is no longer used see if need to logout the target 
If we arrive here device is no longer used see if need to logout the target 

if device is no longer used see if need to logout the target 
If device is no longer used see if need to logout the target 

make a directory like varlibkubeletpluginskubernetesioiscsiportalsome_iqnlunlun_id 
make a directory like varlibkubeletpluginskubernetesioiscsivolumeDevicesiface_nameportalsome_iqnlunlun_id 

make a directory like varlibkubeletpluginskubernetesioiscsiportalsome_iqnlunlun_id 
make a directory like varlibkubeletpluginskubernetesioiscsiiface_nameportalsome_iqnlunlun_id 

This function is run to sync the desired stated of pod 
This function is run to sync the desired state of pod 

serviceAccountTokenSecretWithCAData returns an existing ServiceAccountToken secret with the specified ca data serviceAccountTokenSecretWithoutNamespaceData returns an existing ServiceAccountToken secret that lacks namespace data 
serviceAccountTokenSecretWithoutTokenData returns an existing ServiceAccountToken secret that lacks token data serviceAccountTokenSecretWithoutCAData returns an existing ServiceAccountToken secret that lacks ca data 

serviceAccountTokenSecretWithoutTokenData returns an existing ServiceAccountToken secret that lacks token data serviceAccountTokenSecretWithoutCAData returns an existing ServiceAccountToken secret that lacks ca data 
serviceAccountTokenSecretWithCAData returns an existing ServiceAccountToken secret with the specified ca data serviceAccountTokenSecretWithoutNamespaceData returns an existing ServiceAccountToken secret that lacks namespace data 

createdTokenSecret returns the ServiceAccountToken secret posted when creating a new token secret 
namedTokenSecret returns the ServiceAccountToken secret posted when creating a new token secret with the given name 

addTokenSecretReference adds a reference to the ServiceAccountToken that will be created 
addNamedTokenSecretReference adds a reference to the named ServiceAccountToken 

Sort the container statuses by creation time 
SortContainerStatusesByCreationTime sorts the container statuses by creation time 

Build the pod full name from pod name and namespace 
BuildPodFullName builds the pod full name from pod name and namespace 

When there are multiple containers with the same name the first match will be returned 
When there are multiple containers statuses with the same name the first match will be returned 

FindContainerByName returns a container in the pod with the given name 
FindContainerByID returns a container in the pod with the given ContainerID 

FindContainerByName returns a container in the pod with the given name 
FindContainerStatusByName returns container status in the pod status with the given name 

FindPod combines FindPodByID and FindPodByFullName it finds and returns a pod in the pod list either by the full name or the pod ID It will return an empty pod 
FindPodByFullName finds and returns a pod in the pod list by the full name It will return an empty pod if not found 

FindPod combines FindPodByID and FindPodByFullName it finds and returns a pod in the pod list either by the full name or the pod ID It will return an empty pod 
if not found FindPodByFullName finds and returns a pod in the pod list by the full name 

FindPod combines FindPodByID and FindPodByFullName it finds and returns a pod in the pod list either by the full name or the pod ID It will return an empty pod 
FindPodByID finds and returns a pod in the pod list by UID It will return an empty pod 

It will return an empty pod if not found FindPod combines FindPodByID and FindPodByFullName it finds and returns a pod in the 
FindPodByID finds and returns a pod in the pod list by UID It will return an empty pod if not found 

FindPodByFullName finds and returns a pod in the pod list by the full name It will return an empty pod if not found 
FindPod combines FindPodByID and FindPodByFullName it finds and returns a pod in the pod list either by the full name or the pod ID It will return an empty pod 

FindPodByFullName finds and returns a pod in the pod list by the full name It will return an empty pod if not found 
FindPodByID finds and returns a pod in the pod list by UID It will return an empty pod 

if not found FindPodByFullName finds and returns a pod in the pod list by the full name 
FindPod combines FindPodByID and FindPodByFullName it finds and returns a pod in the pod list either by the full name or the pod ID It will return an empty pod 

FindPodByID finds and returns a pod in the pod list by UID It will return an empty pod if not found 
It will return an empty pod if not found FindPod combines FindPodByID and FindPodByFullName it finds and returns a pod in the 

FindPodByID finds and returns a pod in the pod list by UID It will return an empty pod 
FindPod combines FindPodByID and FindPodByFullName it finds and returns a pod in the pod list either by the full name or the pod ID It will return an empty pod 

FindPodByID finds and returns a pod in the pod list by UID It will return an empty pod 
FindPodByFullName finds and returns a pod in the pod list by the full name It will return an empty pod if not found 

Basic information about a container image 
Image contains basic information about a container image 

Get container status of all the running containers in a pod 
GetRunningContainerStatuses returns container status of all the running containers in a pod 

When there are multiple containers statuses with the same name the first match will be returned Get container status of all the running containers in a pod 
FindContainerByName returns a container in the pod with the given name When there are multiple containers with the same name the first match will 

When there are multiple containers statuses with the same name the first match will be returned 
When there are multiple containers with the same name the first match will be returned 

FindContainerStatusByName returns container status in the pod status with the given name 
FindContainerByName returns a container in the pod with the given name 

Name of the pod 
Namespace of the pod 

State is the state of the container 
State represents the state of a container 

RunningPod is the pod defined defined in pkgkubeletcontainerruntimePod 
RunningPod is the pod defined in pkgkubeletcontainerruntimePod 

Removes the specified image 
RemoveImage removes the specified image 

information of all containers in the pod that are visble in Runtime 
information of all containers in the pod that are visible in Runtime 

Syncs the running pod into the desired pod 
SyncPod syncs the running pod into the desired pod 

GetPods returns a list containers group by pods The boolean parameter 
GetPods returns a list of containers grouped by pods The boolean parameter 

Maintains NodeSpecUnschedulable value from previous run of tryUpdateNodeStatus 
maintains NodeSpecUnschedulable value from previous run of tryUpdateNodeStatus 

The resyncTicker wakes up kubelet to checks if there are any pod workers 
The syncTicker wakes up kubelet to checks if there are any pod workers 

cannot pod is new pod while pods include all admitted pods plus the 
cannot pod is new pod while pods are all admitted pods 

getPullSecretsForPod inspects the Pod and retrieves the referenced pull secrets 
Fetch the pull secrets for the pod 

getPullSecretsForPod inspects the Pod and retrieves the referenced pull secrets 
 Fetch the pull secrets for the pod 

Create Mirror Pod for Static Pod if it doesnt already exist 
 Create a mirror pod if the pod is a static pod and does not already have a mirror pod 

Note be cautious when changing the constant it must work with nodeMonitorGracePeriod 
In that case be cautious when changing the constant it must work with nodeMonitorGracePeriod 

New instantiates a new Kubelet object along with all the required internal modules 
NewMainKubelet instantiates a new Kubelet object along with all the required internal modules 

backOffPeriod is the period to back off when pod syncing resulting in an 
backOffPeriod is the period to back off when pod syncing results in an 

Location of container logs max backoff period exported for the e2e test 
MaxContainerBackOff is the max backoff period exported for the e2e test 

validatePayload returns an error if any path in the payload  returns a copy of the payload with the paths cleaned 
validatePayload returns an error if any path in the payload returns a copy of the payload with the paths cleaned 

1  The payload is validated if the payload is invalid the function returns 
1 The payload is validated if the payload is invalid the function returns 

Detaches the disk from the kubelets host machine 
Detaches the block disk from the kubelets host machine 

Register all metrics 
Register registers all metrics 

lock the volume and thus wait for any concurrrent SetUpAt to finish 
lock the volume and thus wait for any concurrent SetUpAt to finish 

getFullContainerName gets the container name given the root process id of the container 
GetFullContainerName gets the container name given the root process id of the container 

package qos contains helper functions for quality of service 
Package qos contains helper functions for quality of service 

Provide implements dockerConfigProvider 
Enabled implements dockerConfigProvider 

Provide implements dockerConfigProvider 
Enabled implements dockerConfigProvider 

Enabled implements dockerConfigProvider 
Provide implements dockerConfigProvider 

Enabled implements dockerConfigProvider 
Provide implements dockerConfigProvider 

Provide implements dockerConfigProvider 
Enabled implements dockerConfigProvider 

Provide implements dockerConfigProvider 
Enabled implements dockerConfigProvider 

Enabled implements dockerConfigProvider 
Provide implements dockerConfigProvider 

Enabled implements dockerConfigProvider 
Provide implements dockerConfigProvider 

Resource takes an unqualified resource and returns back a Group qualified GroupResource 
Resource takes an unqualified resource and returns a Group qualified GroupResource 

Kind takes an unqualified kind and returns back a Group qualified GroupKind 
Kind takes an unqualified kind and returns a Group qualified GroupKind 

Allowed is required  True if the action would be allowed false otherwise 
Allowed is required True if the action would be allowed false otherwise 

Verb is a kubernetes resource API verb like get list watch create update delete proxy   means all 
Verb is a list of kubernetes resource API verbs like get list watch create update delete proxy   means all 

Get as many pod configs as we can from a directory Return an error if and only if something 
Get as many pod manifests as we can from a directory Return an error if and only if something 

diskResourceAnalyzer provider stats about fs resource usage 
fsResourceAnalyzer provides stats about fs resource usage 

Authorizer implements authorizerAuthorize 
Authorize implements authorizerAuthorize 

A resource policy cannot match a nonresource request 
A nonresource policy cannot match a resource request 

A nonresource policy cannot match a resource request 
A resource policy cannot match a nonresource request 

File format is one map per line  This allows easy concatentation of files 
File format is one map per line  This allows easy concatenation of files 

formatMap formats mapstringstring to a string 
FormatMap formats mapstringstring to a string 

obj could be an apiResourceQuota or a DeletionFinalStateUnknown marker item 
obj could be an v1ResourceQuota or a DeletionFinalStateUnknown marker item 

function that controls full recalculation of quota usage 
Controls full recalculation of quota usage 

Must have authority to list all resources in the system and update quota status 
Must have authority to list all quotas and update quota status 

Controls full resync of objects monitored for replenihsment 
Controls full resync of objects monitored for replenishment 

Knows how to calculate usage 
knows how to calculate usage 

Controls full recalculation of quota usage 
function that controls full recalculation of quota usage 

Must have authority to list all quotas and update quota status 
Must have authority to list all resources in the system and update quota status 

Package nfs contains the internal representation of Ceph file system 
Package cephfs contains the internal representation of Ceph file system 

TearDownAt simply deletes everything in the directory 
TearDown simply deletes everything in the directory 

TearDown simply deletes everything in the directory 
TearDownAt simply deletes everything in the directory 

gitRepoVolumeCleaner cleans git repo volumes 
gitRepoVolumeUnmounter cleans git repo volumes 

SetUpAt creates new directory and clones a git repo 
SetUp creates new directory and clones a git repo 

SetUp creates new directory and clones a git repo 
SetUpAt creates new directory and clones a git repo 

gitRepoVolumeBuilder builds git repo volumes 
gitRepoVolumeMounter builds git repo volumes 

build cgo linux Copyright 2015 The Kubernetes Authors All rights reserved 
build linux Copyright 2015 The Kubernetes Authors 

HttpError wraps a nonStatusOK error code as an error 
HTTPError wraps a nonStatusOK error code as an error 

DockerConfigJson represents dockerconfigjson file info 
DockerConfigJSON represents dockerconfigjson file info 

no ADD or UPDATE pods from the source This signals kubelet that 
no ADD or UPDATE or DELETE pods from the source This signals kubelet that 

map of source name to pod name to pod reference 
map of source name to pod uid to pod reference 

PodConfigNotificationIncremental delivers ADD UPDATE REMOVE RECONCILE to the update channel 
PodConfigNotificationIncremental delivers ADD UPDATE DELETE REMOVE RECONCILE to the update channel 

PodConfigNotificationSnapshotAndUpdates delivers an UPDATE message whenever pods are changed and a SET message if there are any additions or removals 
PodConfigNotificationSnapshotAndUpdates delivers an UPDATE and DELETE message whenever pods are 

Contains utility code for use by volume plugins 
Package util contains utility code for use by volume plugins 

it mounts it it to globalPDPath 
it mounts it to globalPDPath 

Resource takes an unqualified resource and returns back a Group qualified GroupResource 
Resource takes an unqualified resource and returns a Group qualified GroupResource 

Kind takes an unqualified kind and returns back a Group qualified GroupKind 
Kind takes an unqualified kind and returns a Group qualified GroupKind 

mirror pods are stored in the kubelet directly because they need to be in sync with the internal pods 
basicMirrorClient is a functional MirrorClient  Mirror pods are stored in the kubelet directly because they need to be in sync with the internal 

read the the manifest URL passed to kubelet 
read the manifest URL passed to kubelet 

TearDownAt unmounts the bind mount 
TearDown unmounts the bind mount 

TearDown unmounts the bind mount 
TearDownAt unmounts the bind mount 

SetUpAt attaches the disk and bind mounts to the volume path 
SetUp attaches the disk and bind mounts to the volume path 

SetUp attaches the disk and bind mounts to the volume path 
SetUpAt attaches the disk and bind mounts to the volume path 

removeSecretReferenceIfNeeded updates the given ServiceAccount to remove a reference to the given secretName if needed 
removeSecretReference updates the given ServiceAccount to remove a reference to the given secretName if needed 

Otherwise create a new token secret createSecret creates a secret of type ServiceAccountToken for the given ServiceAccount We dont want to update the caches copy of the service account 
hasReferencedToken returns true if the serviceAccount references a service account token secret generateTokenIfNeeded populates the token data for the given Secret if not already set retry Check the cached secret to see if changes are needed We dont want to update the caches copy of the secret 

createSecretIfNeeded makes sure at least one ServiceAccountToken secret exists and is included in the serviceAccounts Secrets list 
ensureReferencedToken makes sure at least one ServiceAccountToken secret exists and is included in the serviceAccounts Secrets list 

This CA will be added in the secretes of service accounts 
This CA will be added in the secrets of service accounts 

getExec handles requests to run a command inside a container 
getRun handles requests to run a command inside a container 

getRun handles requests to run a command inside a container 
getExec handles requests to run a command inside a container 

InstallDeguggingHandlers registers the HTTP request patterns that serve logs or run commandscontainers 
InstallDebuggingHandlers registers the HTTP request patterns that serve logs or run commandscontainers 

ListenAndServeKubeletReadOnlyServer initializes a server to respond to HTTP network requests on the Kubelet 
ListenAndServeKubeletServer initializes a server to respond to HTTP network requests on the Kubelet 

ListenAndServeKubeletServer initializes a server to respond to HTTP network requests on the Kubelet 
ListenAndServeKubeletReadOnlyServer initializes a server to respond to HTTP network requests on the Kubelet 

TODOthockin add a test for syncProxyRules or break it down further and test the pieces 
TODOthockin add more tests for syncProxyRules or break it down further and test the pieces 

Default port  used if no information about Kubelet port can be found in NodeNodeStatusDaemonEndpoints 
Port specifies the default port  used if no information about Kubelet port can be found in NodeNodeStatusDaemonEndpoints 

TestSelectableFieldLabelConversions verifies that given resource have field 
TestSelectableFieldLabelConversionsOfKind verifies that given resource have field 

Returns a channel from which the subscriber can receive PodLifecycleEvent 
Watch returns a channel from which the subscriber can receive PodLifecycleEvent 

kubecontainerContainerState except for the nonexistent state This state 
kubecontainerState except for the nonexistent state This state 

garbage collector is implemented to work with such situtations However to 
garbage collector is implemented to work with such situations However to 

periodic listing to discover container changes It should be be used 
periodic listing to discover container changes It should be used 

Fake cAdvisor implementation 
Fake cadvisorInterface implementation 

 matches all resources Namespace is the name of a namespace APIGroup Resource and Namespace are required to match resource requests 
 matches all API groups Resource is the name of a resource APIGroup Resource and Namespace are required to match resource requests 

 matches all API groups Resource is the name of a resource APIGroup Resource and Namespace are required to match resource requests 
 matches all resources Namespace is the name of a namespace APIGroup Resource and Namespace are required to match resource requests 

If all containers are known and succeeded just return PodCompleted 
If all init containers are known and succeeded just return PodCompleted 

GeneratePodReadyCondition returns ready condition if all containers in a pod are ready else it 
GeneratePodReadyCondition returns Ready condition of a pod The status of Ready condition is True if all containers in a pod are ready 

obj could be an apiNamespace or a DeletionFinalStateUnknown item 
obj could be an v1Namespace or a DeletionFinalStateUnknown item 

Eg If the devices cgroup for the container is stored in sysfscgroupdevicesdockernginx 
Eg if the devices cgroup for the container is stored in sysfscgroupdevicesdockernginx 

IsAllowAll checks whether the netsetsIPNet allows traffic from 00000 
IsAllowAll checks whether the utilnetIPNet allows traffic from 00000 

make a directory like varlibkubeletpluginskubernetesiopodfctargetlun0 
make a directory like varlibkubeletpluginskubernetesiofcvolumeDevicestargetlun0 

make a directory like varlibkubeletpluginskubernetesiopodfctargetlun0 
make a directory like varlibkubeletpluginskubernetesiofctarget1target2lun0 

Unmounts the device and detaches the disk from the kubelets host machine 
DetachDisk unmounts the device and detaches the disk from the kubelets host machine 

Attaches a disk specified by a volumeCinderPersistenDisk to the current kubelet 
AttachDisk attaches a disk specified by a volumeCinderPersistenDisk to the current kubelet 

We might get 1 or more updates for N endpoint updates because we over write older snapshots of endpoints from the producer goroutine 
We might get 1 or more updates for N service updates because we over write older snapshots of services from the producer goroutine 

We might get 1 or more updates for N service updates because we over write older snapshots of services from the producer goroutine 
We might get 1 or more updates for N endpoint updates because we over write older snapshots of endpoints from the producer goroutine 

TODO Do this in a single pass or use an index 
TODO Do the List and Filter in a single pass or use an index 

obj could be an apiPod or a DeletionFinalStateUnknown marker item 
obj could be an v1Pod or a DeletionFinalStateUnknown marker item 

A periodic relist will send update events for all known pods 
Periodic resync will send update events for all known pods 

podStoreSynced returns true if the pod store has been synced at least once 
podListerSynced returns true if the pod store has been synced at least once 

obj could be an apiPod or a DeletionFinalStateUnknown marker item 
obj could be an v1Pod or a DeletionFinalStateUnknown marker item 

Runs e will not return until stopCh is closed workers determines how many 
Run will not return until stopCh is closed workers determines how many 

Scheme is the default instance of runtimeScheme to which types in the abac API group are registered 
Scheme is the default instance of runtimeScheme to which types in the abac API group are apiRegistry 

Group is the API group for abac 
GroupName is the API group for abac 

Start only the pod watcher and the workqueue send a watch event and make sure it hits the sync method for the right ReplicaSet 
Start only the ReplicaSet watcher and the workqueue send a watch event and make sure it hits the sync method Put one ReplicaSet into the shared informer The pod update sent through the fakeWatcher should figure out the managing ReplicaSet and 

Cache provides two methods to retrive the PodStatus the nonblocking Get 
Cache provides two methods to retrieve the PodStatus the nonblocking Get 

3  podSpecActiveDeadlineSeconds gives the recycler pod a maximum timeout before failing  Recommended  Default is 60 seconds 
3 podSpecActiveDeadlineSeconds gives the recycler pod a maximum timeout before failing  Recommended  Default is 60 seconds 

2  podGenerateName helps distinguish recycler pods by name  Recommended Default is pvrecycler 
2 podGenerateName helps distinguish recycler pods by name  Recommended Default is pvrecycler 

1  podSpecVolumes0VolumeSource must be overridden  Recycler implementations without a valid VolumeSource will fail 
1 podSpecVolumes0VolumeSource must be overridden  Recycler implementations without a valid VolumeSource will fail 

FindAttachablePluginByName fetches an attachable volume plugin by name Unlike the other FindPlugin methods this does not return error if no plugin is found  All volumes require a builder and cleaner but not every volume will have an attacherdetacher 
every volume will have an attacherdetacher FindAttachablePluginByName fetches an attachable volume plugin by name Unlike the other FindPlugin methods this does not return error if no 

FindAttachablePluginByName fetches an attachable volume plugin by name Unlike the other FindPlugin methods this does not return error if no plugin is found  All volumes require a builder and cleaner but not every volume will have an attacherdetacher 
Unlike the other FindPlugin methods this does not return error if no plugin is found  All volumes require a mounter and unmounter but not every volume will have an attacherdetacher FindAttachablePluginByName fetches an attachable volume plugin by name 

FindAttachablePluginByName fetches an attachable volume plugin by name Unlike the other FindPlugin methods this does not return error if no plugin is found  All volumes require a builder and cleaner but not every volume will have an attacherdetacher 
FindAttachablePluginBySpec fetches a persistent volume plugin by spec Unlike the other FindPlugin methods this does not return error if no plugin is found  All volumes require a mounter and unmounter but not every volume will have an attacherdetacher 

FindAttachablePluginByName fetches an attachable volume plugin by name Unlike the other FindPlugin methods this does not return error if no plugin is found  All volumes require a builder and cleaner but not every volume will 
FindAttachablePluginByName fetches an attachable volume plugin by name Unlike the other FindPlugin methods this does not return error if no plugin is found  All volumes require a mounter and unmounter but not 

FindAttachablePluginByName fetches an attachable volume plugin by name Unlike the other FindPlugin methods this does not return error if no plugin is found  All volumes require a builder and cleaner but not every volume will 
Unlike the other FindPlugin methods this does not return error if no plugin is found  All volumes require a mounter and unmounter but not every volume will have an attacherdetacher FindAttachablePluginByName fetches an attachable volume plugin by name Unlike the other FindPlugin methods this does not return error if no 

have an attacherdetacher FindAttachablePluginByName fetches an attachable volume plugin by name Unlike the other FindPlugin methods this does not return error if no plugin is found  All volumes require a builder and cleaner but not every volume will 
Unlike the other FindPlugin methods this does not return error if no plugin is found  All volumes require a mounter and unmounter but not every volume will have an attacherdetacher FindAttachablePluginByName fetches an attachable volume plugin by name 

does not return error if no plugin is found  All volumes require a builder and cleaner but not every volume will have an attacherdetacher FindAttachablePluginByName fetches an attachable volume plugin by name Unlike the other FindPlugin methods this 
FindAttachablePluginByName fetches an attachable volume plugin by name Unlike the other FindPlugin methods this does not return error if no plugin is found  All volumes require a mounter and unmounter but not 

does not return error if no plugin is found  All volumes require a builder and cleaner but not every volume will have an attacherdetacher FindAttachablePluginByName fetches an attachable volume plugin by name Unlike the other FindPlugin methods this 
Unlike the other FindPlugin methods this does not return error if no plugin is found  All volumes require a mounter and unmounter but not every volume will have an attacherdetacher FindAttachablePluginByName fetches an attachable volume plugin by name 

FindAttachablePluginBySpec fetches a persistent volume plugin by name  Unlike the other FindPlugin methods this does not return error if no plugin is found  All volumes require a builder and cleaner but not every volume will have an attacherdetacher FindAttachablePluginByName fetches an attachable volume plugin by name Unlike the other FindPlugin methods this 
FindAttachablePluginByName fetches an attachable volume plugin by name Unlike the other FindPlugin methods this does not return error if no 

FindAttachablePluginBySpec fetches a persistent volume plugin by name  Unlike the other FindPlugin methods this does not return error if no plugin is found  All volumes require a builder and cleaner but not every volume will have an attacherdetacher 
Unlike the other FindPlugin methods this does not return error if no plugin is found  All volumes require a mounter and unmounter but not every volume will have an attacherdetacher FindDeviceMountablePluginBySpec fetches a persistent volume plugin by spec 

FindAttachablePluginBySpec fetches a persistent volume plugin by name  Unlike the other FindPlugin methods this does not return error if no plugin is found  All volumes require a builder and cleaner but not every volume will have an attacherdetacher 
FindAttachablePluginByName fetches an attachable volume plugin by name Unlike the other FindPlugin methods this does not return error if no plugin is found  All volumes require a mounter and unmounter but not 

FindAttachablePluginBySpec fetches a persistent volume plugin by name  Unlike the other FindPlugin methods this does not return error if no plugin is found  All volumes require a builder and cleaner but not every volume will have an attacherdetacher 
Unlike the other FindPlugin methods this does not return error if no plugin is found  All volumes require a mounter and unmounter but not every volume will have an attacherdetacher FindAttachablePluginByName fetches an attachable volume plugin by name 

FindAttachablePluginBySpec fetches a persistent volume plugin by name  Unlike the other FindPlugin methods this does not return error if no plugin is found  All volumes require a builder and cleaner but not every volume will 
FindAttachablePluginBySpec fetches a persistent volume plugin by spec Unlike the other FindPlugin methods this does not return error if no plugin is found  All volumes require a mounter and unmounter but not 

is found returns error FindAttachablePluginBySpec fetches a persistent volume plugin by name  Unlike the other FindPlugin methods this does not return error if no plugin is found  All volumes require a builder and cleaner but not every volume will have an attacherdetacher 
Unlike the other FindPlugin methods this does not return error if no plugin is found  All volumes require a mounter and unmounter but not every volume will have an attacherdetacher 

is found returns error FindAttachablePluginBySpec fetches a persistent volume plugin by name  Unlike the other FindPlugin methods this does not return error if no plugin is found  All volumes require a builder and cleaner but not every volume will have an attacherdetacher 
Unlike the other FindPlugin methods this does not return error if no plugin is found  All volumes require a mounter and unmounter but not every volume will have an attacherdetacher 

FindCreatablePluginBySpec fetches a persistent volume plugin by name  If no plugin is found returns error 
FindProvisionablePluginByName fetches  a persistent volume plugin by name  If no plugin is found returns error 

FindCreatablePluginBySpec fetches a persistent volume plugin by name  If no plugin is found returns error 
FindRecyclablePluginByName fetches a persistent volume plugin by name  If no plugin is found returns error 

FindCreatablePluginBySpec fetches a persistent volume plugin by name  If no plugin is found returns error 
FindPersistentPluginByName fetches a persistent volume plugin by name  If no plugin is found returns error 

is found returns error FindCreatablePluginBySpec fetches a persistent volume plugin by name  If no plugin 
FindProvisionablePluginByName fetches  a persistent volume plugin by name  If no plugin is found returns error 

is found returns error FindCreatablePluginBySpec fetches a persistent volume plugin by name  If no plugin 
FindPersistentPluginByName fetches a persistent volume plugin by name  If no plugin is found returns error 

FindDeletablePluginByName fetches a persistent volume plugin by name  If no plugin is found returns error 
FindDeletablePluginBySpec fetches a persistent volume plugin by spec  If no plugin is found returns error 

FindDeletablePluginByName fetches a persistent volume plugin by name  If no plugin is found returns error 
FindProvisionablePluginByName fetches  a persistent volume plugin by name  If no plugin is found returns error FindDeletablePluginBySpec fetches a persistent volume plugin by spec  If 

is found returns error FindDeletablePluginByName fetches a persistent volume plugin by name  If no plugin 
FindDeletablePluginBySpec fetches a persistent volume plugin by spec  If no plugin is found returns error 

is found returns error FindDeletablePluginByName fetches a persistent volume plugin by name  If no plugin 
FindProvisionablePluginByName fetches  a persistent volume plugin by name  If no plugin is found returns error FindDeletablePluginBySpec fetches a persistent volume plugin by spec  If 

FindRecyclablePluginByName fetches a persistent volume plugin by name  If no plugin is found returns error 
FindCreatablePluginBySpec fetches a persistent volume plugin by name  If no plugin is found returns error 

FindRecyclablePluginByName fetches a persistent volume plugin by name  If no plugin is found returns error 
FindProvisionablePluginByName fetches  a persistent volume plugin by name  If no plugin is found returns error 

FindRecyclablePluginByName fetches a persistent volume plugin by name  If no plugin is found returns error 
FindPersistentPluginByName fetches a persistent volume plugin by name  If no plugin is found returns error 

FindPersistentPluginByName fetches a persistent volume plugin by name  If no plugin is found returns error 
FindCreatablePluginBySpec fetches a persistent volume plugin by name  If no plugin is found returns error 

FindPersistentPluginByName fetches a persistent volume plugin by name  If no plugin is found returns error 
FindRecyclablePluginByName fetches a persistent volume plugin by name  If no plugin is found returns error 

FindPersistentPluginByName fetches a persistent volume plugin by name  If no plugin 
FindProvisionablePluginByName fetches  a persistent volume plugin by name  If 

specification  If no plugin is found return an error FindPersistentPluginByName fetches a persistent volume plugin by name  If no plugin is found returns error FindRecyclablePluginByName fetches a persistent volume plugin by name  If no plugin 
no plugin is found returns error FindDeletablePluginBySpec fetches a persistent volume plugin by spec  If no plugin is found returns error FindDeletablePluginByName fetches a persistent volume plugin by name  If no plugin is found returns error 

specification  If no plugin is found return an error FindPersistentPluginByName fetches a persistent volume plugin by name  If no plugin 
FindCreatablePluginBySpec fetches a persistent volume plugin by name  If no plugin is found returns error 

NewSpecFromVolume creates an Spec from an apiVolume NewSpecFromPersistentVolume creates an Spec from an apiPersistentVolume 
NewSpecFromVolume creates an Spec from an v1Volume NewSpecFromPersistentVolume creates an Spec from an v1PersistentVolume 

Passing config as strings is the least desirable option but can be used for truly oneoff configuration The binary should still use strong typing for this value when binding CLI values before they are passed as strings 
plugin Passing config as strings is the least desirable option but can be used for truly oneoff configuration The binary should still use strong typing for this value when binding CLI values before they are passed as 

the plugins while allowing override of those default values  Those config values are then set to an instance of VolumeConfig and passed to the plugin 
specific to the plugin will be passed to the plugins ProbeVolumePluginsconfig func  Reasonable defaults will be provided by the binary hosting the plugins while allowing override of those default values  Those config values are then set to an instance of VolumeConfig 

the plugins while allowing override of those default values  Those config values are then set to an instance of VolumeConfig and passed to the plugin 
VolumeConfig is how volume plugins receive configuration  An instance specific to the plugin will be passed to the plugins ProbeVolumePluginsconfig func  Reasonable defaults will be provided by the binary hosting the plugins while allowing override of those default 

VolumeConfig is how volume plugins receive configuration  An instance specific to the plugin will be passed to the plugins ProbeVolumePluginsconfig func  Reasonable defaults will be provided by the binary hosting the plugins while allowing override of those default values  Those config values are then set to an instance of 
the binary hosting the plugins while allowing override of those default values  Those config values are then set to an instance of VolumeConfig 

VolumeConfig is how volume plugins receive configuration  An instance specific to the plugin will be passed to the plugins ProbeVolumePluginsconfig func  Reasonable defaults will be provided by the binary hosting 
specific to the plugin will be passed to the plugins ProbeVolumePluginsconfig func  Reasonable defaults will be provided by the binary hosting the plugins while allowing override of those default values  Those config values are then set to an instance of VolumeConfig and passed to the plugin 

NewWrapperBuilder finds an appropriate plugin with which to handle 
NewWrapperMounter finds an appropriate plugin with which to handle 

a given plugin may store data for a given pod  If the specified pod does not exist the result of this call might not exist  This 
represents the named plugin for the given pod If the specified pod does not exist the result of this call might not exist 

GetPodPluginDir returns the absolute path to a directory under which a given plugin may store data for a given pod  If the specified pod 
GetPodPluginDir GetVolumeDevicePluginDir returns the absolute path to a directory under which a given plugin may store data 

GetPodPluginDir returns the absolute path to a directory under which 
GetPluginDir returns the absolute path to a directory under which 

GetPodVolumeDir returns the absolute path a directory which 
GetPodVolumeDeviceDir returns the absolute path a directory which 

GetPluginDir returns the absolute path to a directory under which 
GetPodPluginDir returns the absolute path to a directory under which 

Name returns the plugins name  Plugins should use namespaced names 
Name returns the plugins name  Plugins must use namespaced names 

unionDockerKeyring delegates to a set of keyrings 
UnionDockerKeyring delegates to a set of keyrings 

globUrldockerio targetUrlnotrightio    no match 
	globURLdockerio targetURLnotrightio    no match 

globUrldockerio targetUrlblahdockerio  match 
	globURLdockerio targetURLblahdockerio  match 

check whether the given target url matches the glob url which may have 
URLsMatch checks whether the given target url matches the glob url which may have 

split the host name into parts as well as the port 
SplitURL splits the host name into parts as well as the port 

  schemelessUrl 
  schemelessURL 

Global cache timestamp is older but the pod entry modified 
Global cache timestamp is newer but the pod entry modified 

Global cache timestamp is newer but the pod entry modified 
Global cache timestamp is older but the pod entry modified 

NewCachedMetrics creates a new cachedMetrics wrapping another MetricsProvider and caching the results 
cachedMetrics represents a MetricsProvider that wraps another provider and caches the result NewCachedMetrics creates a new cachedMetrics wrapping another 

98768083 
98768082 

98768083 
98768082 

98768082 
98768083 

98768082 
98768083 

>>comment_use_lines_files
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
If conn has no activity for serviceInfotimeout since last ReadWrite it shoule be closed because of timeout 
If conn has no activity for serviceInfotimeout since last ReadWrite it should be closed because of timeout 

When connecting to a UDP service endpoint there shoule be a Conn for proxy 
When connecting to a UDP service endpoint there should be a Conn for proxy 

Resource takes an unqualified resource and returns back a Group qualified GroupResource 
Resource takes an unqualified resource and returns a Group qualified GroupResource 

Kind takes an unqualified kind and returns back a Group qualified GroupKind 
Kind takes an unqualified kind and returns a Group qualified GroupKind 

udpProxySocket implements proxySocket  Close is implemented by netUDPConn  When Close is called 
udpProxySocket implements ProxySocket  Close is implemented by netUDPConn  When Close is called 

udpProxySocket implements proxySocket  Close is implemented by netUDPConn  When Close is called 
tcpProxySocket implements ProxySocket  Close is implemented by netListener  When Close is called 

proxyTCP proxies data bidirectionally between in and out 
ProxyTCP proxies data bidirectionally between in and out 

tcpProxySocket implements proxySocket  Close is implemented by netListener  When Close is called 
udpProxySocket implements ProxySocket  Close is implemented by netUDPConn  When Close is called 

tcpProxySocket implements proxySocket  Close is implemented by netListener  When Close is called 
tcpProxySocket implements ProxySocket  Close is implemented by netListener  When Close is called 

ListenPort returns the host port that the proxySocket is listening on 
ListenPort returns the host port that the ProxySocket is listening on 

Close stops the proxySocket from accepting incoming connections 
Close stops the ProxySocket from accepting incoming connections 

Addr gets the netAddr for a proxySocket 
Addr gets the netAddr for a ProxySocket 

Returns internal Status 
Status returns internal Status 

Returns a NodeConfig that is being used by the container manager 
GetNodeConfig returns a NodeConfig that is being used by the container manager 

Returns resources allocated to system cgroups in the machine 
SystemCgroupsLimit returns resources allocated to system cgroups in the machine 

readOnly bool is supplied by persistentclaim volume source when its builder creates other volumes 
readOnly bool is supplied by persistentclaim volume source when its mounter creates other volumes 

filter parent 1 protocol ip pref 1 u32 fh 800800 order 2048 key ht 800 bkt 0 flowid 11 
filter parent 1 protocol ip pref 1 u32 chain 0 fh 800800 order 2048 key ht 800 bkt 0 flowid 11 not_in_hw new version 

tcShaper provides an implementation of the BandwidthShaper interface on Linux using the tc tool 
tcShaper provides an implementation of the Shaper interface on Linux using the tc tool 

the originial logic of isPodRunning happens to return true when podstatus is empty so the test can always pass 
the original logic of isPodRunning happens to return true when podstatus is empty so the test can always pass 

remove a lock rbd lock remove 
Remove a lock if found rbd lock remove 

construct lock id using host name and a magic prefix 
Construct lock id using host name and a magic prefix 

make a directory like varlibkubeletpluginskubernetesiopodrbdpoolimageimage 
Make a directory like varlibkubeletpluginskubernetesiorbdvolumeDevicespoolimageimage 

make a directory like varlibkubeletpluginskubernetesiopodrbdpoolimageimage 
Make a directory like varlibkubeletpluginskubernetesiorbdmountspoolimageimage 

stat a path if not exists retry maxRetries times 
Stat a path if it doesnt exist retry maxRetries times 

found a match check if device exists 
Found a match check if device exists 

first match pool then match name 
First match pool then match name 

pool and name format 
Pool and name format 

search sysbus for rbd device that matches given pool and image 
Search sysbus for rbd device that matches given pool and image 

Build a slice of iptables args for an fromnonlocal publicport rule 
Build a slice of iptables args for a fromhost publicport rule 

Build a slice of iptables args for a fromhost publicport rule 
Build a slice of iptables args for an fromnonlocal publicport rule 

TODO Should we just reuse iptablesContainerPortalArgs 
TODO Should we just reuse iptablesHostPortalArgs 

Build a slice of iptables args for a fromcontainer publicport rule 
Build a slice of iptables args for an fromnonlocal publicport rule 

Build a slice of iptables args for a fromcontainer publicport rule 
Build a slice of iptables args for a fromhost publicport rule 

Build a slice of iptables args for a fromhost portal rule 
Build a slice of iptables args for a fromcontainer portal rule 

Build a slice of iptables args for a fromcontainer portal rule 
Build a slice of iptables args for a fromhost portal rule 

TODO check health of the socket  What if ProxyLoop exited 
TODO check health of the socket What if ProxyLoop exited 

addServiceOnPort starts listening for a new service returning the serviceInfo 
addServiceOnPortInternal starts listening for a new service returning the ServiceInfo 

Sync is called to immediately synchronize the proxier state to iptables 
Sync is called to synchronize the proxier state to iptables as soon as possible 

assert Proxier is a ProxyProvider 
assert Proxier is a proxyProvider 

protects serviceMap 
protects serviceChanges 

secretVolumeBuilder handles retrieving secrets from the API server 
secretVolumeMounter handles retrieving secrets from the API server 

ProbeVolumePlugin is the entry point for plugin detection in a package 
ProbeVolumePlugins is the entry point for plugin detection in a package 

updateReplicaCount attempts to update the StatusReplicas of the given ReplicaSet with a single GETPUT retry 
updateReplicaSetStatus attempts to update the StatusReplicas of the given ReplicaSet with a single GETPUT retry 

Apply default values and validate pods 
Apply default values and validate the pod 

Apply default values and validate the pod 
Apply default values and validate pods 

Package gcp_credentials contains  implementations of DockerConfigProvider 
Package gcp contains implementations of DockerConfigProvider 

Returns a readonly copy of the system capabilities 
Get returns a readonly copy of the system capabilities 

Setup the capability set  It wraps Initialize for improving usibility 
Setup the capability set  It wraps Initialize for improving usability 

of capabilities like host networking sharing the host IPC namespace and sharing the host PID namespace List of pod sources for which using host network is allowed 
Pod sources from which to allow privileged capabilities like host networking sharing the host IPC namespace and sharing the host PID namespace 

PrivilegedSources defines the pod sources allowed to make privileged requests for certain types of capabilities like host networking sharing the host IPC namespace and sharing the host PID namespace 
Pod sources from which to allow privileged capabilities like host networking sharing the host IPC namespace and sharing the host PID namespace 

Pod sources from which to allow privileged capabilities like host networking sharing the host IPC namespace and sharing the host PID namespace 
of capabilities like host networking sharing the host IPC namespace and sharing the host PID namespace List of pod sources for which using host network is allowed 

Pod sources from which to allow privileged capabilities like host networking sharing the host IPC namespace and sharing the host PID namespace 
PrivilegedSources defines the pod sources allowed to make privileged requests for certain types of capabilities like host networking sharing the host IPC namespace and sharing the host PID namespace 

Assuemes that the container has Custom Metrics enabled if it has etccustommetrics directory 
Assumes that the container has Custom Metrics enabled if it has etccustommetrics directory 

Returns a path to a cAdvisorspecific custom metrics configuration 
GetCAdvisorCustomMetricsDefinitionPath returns a path to a cAdvisorspecific custom metrics configuration 

resolvePort attempts to turn a IntOrString port reference into a concrete port number 
resolvePort attempts to turn an IntOrString port reference into a concrete port number 

readOnly bool is supplied by persistentclaim volume source when its builder creates other volumes 
readOnly bool is supplied by persistentclaim volume source when its mounter creates other volumes 

readOnly bool is supplied by persistentclaim volume source when its builder creates other volumes 
readOnly bool is supplied by persistentclaim volume source when its mounter creates other volumes 

pairs and returns a a populated stringstring httpHeader map 
pairs and returns a populated stringstring httpHeader map 

Prober helps to check the livenessreadiness of a container 
Prober helps to check the livenessreadinessstartup of a container 

the deletion is complete Any error returned indicates the volume has failed to be reclaimed A nil return indicates success This method should block until completion 
to this method should block until the deletion is complete Any error returned indicates the volume has failed to be reclaimed A nil return 

Deleter removes the resource from the underlying storage provider  Calls to this method should block until 
Deleter removes the resource from the underlying storage provider Calls to this method should block until the deletion is complete Any error 

Any error returned indicates the volume has failed to be reclaimed  A nil return indicates success 
returned indicates the volume has failed to be reclaimed A nil return indicates success 

specified directory path which may or may not exist yet The mount point and its content should be owned by fsGroup so that it can be accessed by the pod This may be called more than once so implementations must be 
selfdetermined directory path The mount point and its content should be owned by fsUser or fsGroup so that it can be accessed by the pod This may be called more than once so 

selfdetermined directory path The mount point and its content should be owned by fsGroup so that it can be accessed by the pod This may be called more than once so implementations must be idempotent 
The mount point and its content should be owned by fsUser fsGroup so that it can be accessed by the pod This may be called more than once so implementations must be 

Attributes represents the attributes of this builder 
Attributes represents the attributes of this mounter 

space on the underlying storage and is shared with host processes and other Volumes 
on the underlying storage and is shared with host processes and other volumes 

For Volumes that share a filesystem with the host eg emptydir hostpath this is the available 
For volumes that share a filesystem with the host eg emptydir hostpath 

of the underlying storage and will not equal Used  Available as the fs is shared Available represents the storage space available bytes for the Volume For Volumes that share a filesystem with the host eg emptydir hostpath this is the available 
underlying storage For Volumes that share a filesystem with the host eg emptydir hostpath this is the size of the underlying storage and will not equal Used  Available as the fs is shared Available represents the storage space available bytes for the 

For Volumes that share a filesystem with the host eg emptydir hostpath this is the size of the underlying storage and will not equal Used  Available as the fs is shared Available represents the storage space available bytes for the Volume 
and will not equal Used  Available as the fs is shared Available represents the storage space available bytes for the Volume For Volumes that share a filesystem with the host eg 

For Volumes that share a filesystem with the host eg emptydir hostpath this is the size of the underlying storage and will not equal Used  Available as the fs is shared 
eg emptydir hostpath this is the size of the underlying storage and will not equal Used  Available as the fs is shared Available represents the storage space available bytes for the Volume For Volumes that share a filesystem with the host eg 

For Volumes that share a filesystem with the host eg emptydir hostpath this is the size 
For volumes that share a filesystem with the host eg emptydir hostpath 

GetMetrics returns the Metrics for the Volume  Maybe expensive for some implementations 
GetMetrics returns the Metrics for the Volume Maybe expensive for some implementations 

MetricsProvider embeds methods for exposing metrics eg usedavailable space MetricsProvider exposes metrics eg usedavailable space related to a Volume 
MetricsProvider embeds methods for exposing metrics eg used available space MetricsProvider exposes metrics eg usedavailable space related to a 

rbd volume implements diskManager calls diskSetup when creating a volume and calls diskTearDown inside volume cleaner 
rbd volume implements diskManager calls diskSetup when creating a volume and calls diskTearDown inside volume unmounter 

diskManager interface and diskSetupTearDown functions abtract commonly used procedures to setup a block volume 
diskManager interface and diskSetupTearDown functions abstract commonly used procedures to setup a block volume 

A type to help sort container statuses based on container names 
SortedContainerStatuses is a type to help sort container statuses based on container names 

GetString returns the time in the string format using the RFC3339Nano 
GetString returns the time in the string format using the RFC3339NanoFixed 

ConvertToTimestamp takes a string parses it using the RFC3339Nano layout 
ConvertToTimestamp takes a string parses it using the RFC3339NanoLenient layout 

the nice effect of flushing the chain  Then we can remove the chain 
for it which has the nice effect of flushing the chain Then we can remove the chain 

Now write loadbalancing  DNAT rules 
Now write loadbalancing rules 

Create and link the kube postrouting chain 
Create and link the kube chains 

assumes proxiermu is held 
Assumes proxiermu is held 

then encoding to base32 and truncating with the prefix KUBESVC  We do this because Iptables Chain Names must be  28 chars long and the longer they are the harder they are to read 
then encoding to base32 and truncating to 16 chars We do this because IPTables Chain Names must be  28 chars long and the longer they are the harder they are to read 

Sync is called to immediately synchronize the proxier state to iptables 
Sync is called to synchronize the proxier state to iptables as soon as possible 

Proxier implements ProxyProvider 
Proxier implements proxyProvider 

CanUseIptablesProxier returns true if we should use the iptables Proxier 
CanUseIPTablesProxier returns true if we should use the iptables Proxier 

newSourceApiserverFromLW holds creates a config source that watches and pulls from the apiserver 
NewSourceApiserver creates a config source that watches and pulls from the apiserver 

NewSourceApiserver creates a config source that watches and pulls from the apiserver 
newSourceApiserverFromLW holds creates a config source that watches and pulls from the apiserver 

Set default MaxSurge as 1 by default 
Set default MaxUnavailable as 1 by default 

Set default MaxSurge as 1 by default 
Set default MaxSurge as 0 by default 

Set default MaxSurge as 1 by default 
Set default MaxUnavailable as 1 by default 

Set default MaxUnavailable as 1 by default 
Set default MaxSurge as 1 by default 

Set DeploymentSpecReplicas to 1 if it is not set 
Set extensionsv1beta1DeploymentSpecReplicas to 1 if it is not set 

Pod returns a string reprenetating a pod in a human readable format 
Pod returns a string representing a pod in a consistent human readable format 

If the PodSyncResult contains error result it should be error 
If the PodSyncResult doesnt contain error result it should not be error 

If the PodSyncResult doesnt contain error result it should not be error 
If the PodSyncResult contains error result it should be error 

downwardAPIVolumeCleander handles cleaning up downwardAPI volumes 
downwardAPIVolumeCleaner handles cleaning up downwardAPI volumes 

collectData collects requested downwardAPI in data map 
CollectData collects requested downwardAPI in data map 

downwardAPIVolumeBuilder fetches info from downward API from the pod 
downwardAPIVolumeMounter fetches info from downward API from the pod 

TODO remove this redundancy as soon NewCleaner func will have apiPOD and not only typesUID 
TODO remove this redundancy as soon NewUnmounter func will have v1POD and not only typesUID 

It accepts set add and remove operations of endpoints via channels and invokes registered handlers on change 
It accepts set add and remove operations of node via channels and invokes registered handlers on change 

EndpointsConfig tracks a set of endpoints configurations 
EndpointSliceConfig tracks a set of endpoints configurations 

Create an event recorder to record objects event except implicitly required containers like infra container 
FilterEventRecorder creates an event recorder to record objects event except implicitly required containers like infra container 

EnvVarsToMap constructs a map of environment name to value from a slice 
envVarsToMap constructs a map of environment name to value from a slice 

Gets all validated sources from the specified sources 
GetValidatedSources gets all validated sources from the specified sources 

Updates from Kubernetes API Server 
ApiserverSource identifies updates from Kubernetes API Server 

Updates from a file Updates from querying a web page 
Filesource idenitified updates from a file HTTPSource identifies updates from querying a web page 

The first Command call is checking the rule  Success of that exec means done 
The second Command call is checking the rule  Success of that exec means done 

The first Command call is checking the rule  Success of that exec means done 
The second Command call is checking the rule  Success of that exec means done 

Success on the first call Status 1 on the second call 
Status 1 on the first call Success on the second call 

Status 1 on the first call Success on the second call 
Success on the first call Status 1 on the second call 

The second Command call is checking the rule  Success of that exec means done 
The first Command call is checking the rule  Success of that exec means done 

The second Command call is checking the rule  Success of that exec means done 
The first Command call is checking the rule  Success of that exec means done 

the error string looking for known values which is imperfect but works in 
the error string looking for known values which is imperfect beware using 

AddReloadFunc is part of Interface 
ChainExists is part of Interface 

AddReloadFunc is part of Interface 
RestoreAll is part of Interface 

AddReloadFunc is part of Interface 
SaveInto is part of Interface 

AddReloadFunc is part of Interface 
DeleteRule is part of Interface 

AddReloadFunc is part of Interface 
DeleteChain is part of Interface 

AddReloadFunc is part of Interface 
FlushChain is part of Interface 

RestoreAll is part of Interface 
SaveInto is part of Interface 

RestoreAll is part of Interface 
FlushChain is part of Interface 

SaveAll is part of Interface 
ChainExists is part of Interface 

SaveAll is part of Interface 
SaveInto is part of Interface 

SaveAll is part of Interface 
DeleteRule is part of Interface 

SaveAll is part of Interface 
DeleteChain is part of Interface 

SaveAll is part of Interface 
FlushChain is part of Interface 

DeleteRule is part of Interface 
ChainExists is part of Interface 

DeleteRule is part of Interface 
SaveInto is part of Interface 

DeleteRule is part of Interface 
DeleteChain is part of Interface 

DeleteRule is part of Interface 
FlushChain is part of Interface 

DeleteChain is part of Interface 
ChainExists is part of Interface 

DeleteChain is part of Interface 
SaveInto is part of Interface 

DeleteChain is part of Interface 
DeleteRule is part of Interface 

DeleteChain is part of Interface 
FlushChain is part of Interface 

FlushChain is part of Interface 
ChainExists is part of Interface 

FlushChain is part of Interface 
RestoreAll is part of Interface 

FlushChain is part of Interface 
SaveInto is part of Interface 

FlushChain is part of Interface 
DeleteRule is part of Interface 

FlushChain is part of Interface 
DeleteChain is part of Interface 

EnsureChain is part of Interface 
SaveInto is part of Interface 

Option flag for Flush 
FlushFlag an option flag for Flush 

IsIpv6 returns true if this is managing ipv6 tables 
IsIPv6 returns true if this is managing ipv6 tables 

DeleteChain deletes the specified chain  If the chain did not exist return error 
FlushChain clears the specified chain  If the chain did not exist return error 

FlushChain clears the specified chain  If the chain did not exist return error 
DeleteChain deletes the specified chain  If the chain did not exist return error 

An injectable interface for running iptables commands  Implementations must be goroutinesafe 
Interface is an injectable interface for running iptables commands  Implementations must be goroutinesafe 

the deleted keyvalue Note that this value might be stale If the pod 
the deleted keyvalue Note that this value might be stale If the Pod 

the deleted keyvalue Note that this value might be stale If the pod 
the deleted keyvalue Note that this value might be stale If the ReplicaSet 

is updated and wake them up If anything of the Pods have changed we need to awaken both the old and new deployments old and cur must be apiPod types 
is updated and wake them up If the anything of the ReplicaSets have changed we need to awaken both the old and new deployments old and cur must be appsReplicaSet 

the deleted keyvalue Note that this value might be stale If the ReplicaSet 
the deleted keyvalue Note that this value might be stale If the Pod 

the ReplicaSet is deleted obj could be an extensionsReplicaSet or 
the ReplicaSet is deleted obj could be an appsReplicaSet or 

awaken both the old and new deployments old and cur must be extensionsReplicaSet 
awaken both the old and new deployments old and cur must be appsReplicaSet 

trying to clean up one of the controllers for now we just return one of the two 
trying to clean up one of the controllers for now we just return the older one 

podStoreSynced returns true if the pod store has been synced at least once 
podListerSynced returns true if the pod store has been synced at least once 

rsStoreSynced returns true if the ReplicaSet store has been synced at least once 
rsListerSynced returns true if the ReplicaSet store has been synced at least once 

current status of the scale More info httpreleasesk8siorelease12docsdevelapiconventionsmdspecandstatus Readonly 
current status of the scale More info httpsgitk8siocommunitycontributorsdevelsigarchitectureapiconventionsmdspecandstatus Readonly 

defines the behavior of the scale More info httpreleasesk8siorelease12docsdevelapiconventionsmdspecandstatus 
defines the behavior of the scale More info httpsgitk8siocommunitycontributorsdevelsigarchitectureapiconventionsmdspecandstatus 

Standard object metadata More info httpreleasesk8siorelease12docsdevelapiconventionsmdmetadata 
Standard object metadata More info httpsgitk8siocommunitycontributorsdevelsigarchitectureapiconventionsmdmetadata 

ReplicaSetsByCreationTimestamp sorts a list of ReplicationSets by creation timestamp using their names as a tie breaker 
ReplicaSetsByCreationTimestamp sorts a list of ReplicaSet by creation timestamp using their names as a tie breaker 

ReplicaSetsByCreationTimestamp sorts a list of ReplicationSets by creation timestamp using their names as a tie breaker 
ControllersByCreationTimestamp sorts a list of ReplicationControllers by creation timestamp using their names as a tie breaker 

ControllersByCreationTimestamp sorts a list of ReplicationControllers by creation timestamp using their names as a tie breaker 
ReplicaSetsByCreationTimestamp sorts a list of ReplicaSet by creation timestamp using their names as a tie breaker 

6 Empty creation time pods  newer pods  older pods 
8 Empty creation time pods  newer pods  older pods 

6 Empty creation time pods  newer pods  older pods 
6 older pods  newer pods  empty timestamp pods 

5 Pods with containers with higher restart counts  lower restart counts 
7 Pods with containers with higher restart counts  lower restart counts 

4 Been ready for empty time  less time  more time 
4 Been ready for more time  less time  empty time 

3 Not ready  ready 
3 ready  not ready 

1 Unassigned  assigned 
1 assigned  unassigned 

DeletionTimestamp on an object as a delete To do so consistenly one needs 
DeletionTimestamp on an object as a delete To do so consistently one needs 

UIDSetKeyFunc to parse out the key from a UIDSet 
ExpKeyFunc to parse out the key from a ControlleeExpectation 

ExpKeyFunc to parse out the key from a ControlleeExpectation 
UIDSetKeyFunc to parse out the key from a UIDSet 

ValidSecurityContextWithContainerDefaults creates a valid security context provider based on 
ValidInternalSecurityContextWithContainerDefaults creates a valid security context provider based on 

A DockerConfigProvider that reads its configuration from a URL read from 
DockerConfigURLKeyProvider is a DockerConfigProvider that reads its configuration from a URL read from 

Resource takes an unqualified resource and returns back a Group qualified GroupResource 
Resource takes an unqualified resource and returns a Group qualified GroupResource 

Kind takes an unqualified kind and returns back a Group qualified GroupKind 
Kind takes an unqualified kind and returns a Group qualified GroupKind 

configMapVolumeCleaner handles cleaning up configMap volumes 
configMapVolumeUnmounter handles cleaning up configMap volumes 

configMapVolumeBuilder handles retrieving secrets from the API server 
configMapVolumeMounter handles retrieving secrets from the API server 

ProbeVolumePlugin is the entry point for plugin detection in a package 
ProbeVolumePlugins is the entry point for plugin detection in a package 

extract portal and iqn from device path 
Extract the portal and iqn from device path 

extract portal and iqn from device path 
Extract the portal and iqn from device path 

this portaliqn are no longer referenced log out 
This portaliqniface is no longer referenced log out 

this portaliqn are no longer referenced log out 
This portaliqniface is no longer referenced log out 

if device is no longer used see if need to logout the target 
If device is no longer used see if need to logout the target 

if device is no longer used see if need to logout the target 
If we arrive here device is no longer used see if need to logout the target 

if device is no longer used see if need to logout the target 
If device is no longer used see if need to logout the target 

make a directory like varlibkubeletpluginskubernetesioiscsiportalsome_iqnlunlun_id 
make a directory like varlibkubeletpluginskubernetesioiscsiiface_nameportalsome_iqnlunlun_id 

This function is run to sync the desired stated of pod 
This function is run to sync the desired state of pod 

Named defaulttokenfplln since that is the first generated name after randSeed1 
Named defaulttokenxn8fg since that is the first generated name after randSeed1 

opaqueSecret returns a persisted nonServiceAccountToken secret named regularsecret1 createdTokenSecret returns the ServiceAccountToken secret posted when creating a new token secret 
namedTokenSecret returns the ServiceAccountToken secret posted when creating a new token secret with the given name serviceAccountTokenSecret returns an existing ServiceAccountToken secret named tokensecret1 

Delete endpoints1 Delete endpoints2 
Delete service1 Delete service2 

Delete service1 Delete service2 
Delete endpoints1 Delete endpoints2 

Build the pod full name from pod name and namespace 
BuildPodFullName builds the pod full name from pod name and namespace 

ToAPIPod converts Pod to apiPod Note that if a field in apiPod has no 
ToAPIPod converts Pod to v1Pod Note that if a field in v1Pod has no 

When there are multiple containers with the same name the first match will be returned 
When there are multiple containers statuses with the same name the first match will be returned 

pod list either by the full name or the pod ID It will return an empty pod if not found 
FindPodByFullName finds and returns a pod in the pod list by the full name It will return an empty pod if not found 

FindPod combines FindPodByID and FindPodByFullName it finds and returns a pod in the pod list either by the full name or the pod ID It will return an empty pod 
FindPodByFullName finds and returns a pod in the pod list by the full name It will return an empty pod if not found 

It will return an empty pod if not found FindPod combines FindPodByID and FindPodByFullName it finds and returns a pod in the pod list either by the full name or the pod ID It will return an empty pod 
FindPodByID finds and returns a pod in the pod list by UID It will return an empty pod if not found FindPodByFullName finds and returns a pod in the pod list by the full name 

FindPodByFullName finds and returns a pod in the pod list by the full name It will return an empty pod if not found 
pod list either by the full name or the pod ID It will return an empty pod if not found 

FindPodByFullName finds and returns a pod in the pod list by the full name It will return an empty pod if not found 
FindPod combines FindPodByID and FindPodByFullName it finds and returns a pod in the pod list either by the full name or the pod ID It will return an empty pod 

FindPodByFullName finds and returns a pod in the pod list by the full name It will return an empty pod if not found 
FindPodByID finds and returns a pod in the pod list by UID It will return an empty pod if not found 

FindPodByID finds and returns a pod in the pod list by UID It will return an empty pod if not found FindPodByFullName finds and returns a pod in the pod list by the full name 
It will return an empty pod if not found FindPod combines FindPodByID and FindPodByFullName it finds and returns a pod in the pod list either by the full name or the pod ID It will return an empty pod 

FindPodByID finds and returns a pod in the pod list by UID It will return an empty pod if not found 
FindPodByFullName finds and returns a pod in the pod list by the full name It will return an empty pod if not found 

Get container status of all the running containers in a pod 
GetRunningContainerStatuses returns container status of all the running containers in a pod 

When there are multiple containers statuses with the same name the first match will be returned 
When there are multiple containers with the same name the first match will be returned 

Convenience method for creating a ContainerID from an ID string 
ParseContainerID is a convenience method for creating a ContainerID from an ID string 

RunningPod is the pod defined defined in pkgkubeletcontainerruntimePod 
RunningPod is the pod defined in pkgkubeletcontainerruntimePod 

Removes the specified image 
RemoveImage removes the specified image 

Syncs the running pod into the desired pod 
SyncPod syncs the running pod into the desired pod 

GetPods returns a list containers group by pods The boolean parameter 
GetPods returns a list of containers grouped by pods The boolean parameter 

Maintains NodeSpecUnschedulable value from previous run of tryUpdateNodeStatus 
maintains NodeSpecUnschedulable value from previous run of tryUpdateNodeStatus 

We should not use the pod from livenessManager because it is never updated after initialization 
We should not use the pod from manager because it is never updated after initialization 

admission process and may be rejcted This can be resolved 
admission process and may be rejected This can be resolved 

The resyncTicker wakes up kubelet to checks if there are any pod workers 
The syncTicker wakes up kubelet to checks if there are any pod workers 

cannot pod is new pod while pods include all admitted pods plus the 
cannot pod is new pod while pods are all admitted pods 

initializeRuntimeDependentModules will initialize internal modules that require the container runtime to be up 
initializeModules will initialize internal modules that do not require the container runtime to be up 

initializeModules will initialize internal modules that do not require the container runtime to be up 
initializeRuntimeDependentModules will initialize internal modules that require the container runtime to be up 

Starts garbage collection threads 
StartGarbageCollection starts garbage collection threads 

Set to true to have the node register itself as schedulable 
Set to true to have the node register itself with the apiserver 

Set to true to have the node register itself with the apiserver 
Set to true to have the node register itself as schedulable 

setup containerGC 
setup volumeManager 

New instantiates a new Kubelet object along with all the required internal modules 
NewMainKubelet instantiates a new Kubelet object along with all the required internal modules 

backOffPeriod is the period to back off when pod syncing resulting in an 
backOffPeriod is the period to back off when pod syncing results in an 

bar           databar 
bar  databar 

validatePayload returns an error if any path in the payload  returns a copy of the payload with the paths cleaned 
validatePayload returns an error if any path in the payload returns a copy of the payload with the paths cleaned 

9  The new data directory symlink is renamed to the data directory rename is atomic 
8  The new data directory symlink is renamed to the data directory rename is atomic 

8  A symlink to the new timestamped directory data_tmp is created that will 
7  A symlink to the new timestamped directory data_tmp is created that will 

7  The current timestamped directory is detected by reading the data directory 
2  The current timestamped directory is detected by reading the data directory 

6  Symlinks and directory for new uservisible files are created if needed 
9  Symlinks and directory for new uservisible files are created if needed 

5  The payload is written to the new timestamped directory 
6 The payload is written to the new timestamped directory 

1  The payload is validated if the payload is invalid the function returns 
1 The payload is validated if the payload is invalid the function returns 

GenerateContainerRef returns an apiObjectReference which references the given container 
GenerateContainerRef returns an v1ObjectReference which references the given container 

NewBuilder We could then find volumeID there without probing MountRefs 
NewMounter We could then find volumeID there without probing MountRefs 

TODO refactor VolumePluginNewCleaner to get full volumeSpec just like 
TODO refactor VolumePluginNewUnmounter to get full volumeSpec just like 

getFullContainerName gets the container name given the root process id of the container 
GetFullContainerName gets the container name given the root process id of the container 

package qos contains helper functions for quality of service 
Package qos contains helper functions for quality of service 

readOnly bool is supplied by persistentclaim volume source when its builder creates other volumes 
readOnly bool is supplied by persistentclaim volume source when its mounter creates other volumes 

metricsDu represents a MetricsProvider that calculates the used and available Volume space by executing the du command and gathering filesystem info for the Volume path 
metricsDu represents a MetricsProvider that calculates the used and available Volume space by calling fsDiskUsage and gathering filesystem info for the Volume path 

readOnly bool is supplied by persistentclaim volume source when its builder creates other volumes 
readOnly bool is supplied by persistentclaim volume source when its mounter creates other volumes 

auth field decodes to username  password 
auth field overrides username  password 

auth field overrides username  password 
auth field decodes to username  password 

auth field overrides username  password 
auth field decodes to username  password 

auth field decodes to username  password 
auth field overrides username  password 

Resource takes an unqualified resource and returns back a Group qualified GroupResource 
Resource takes an unqualified resource and returns a Group qualified GroupResource 

Kind takes an unqualified kind and returns back a Group qualified GroupKind 
Kind takes an unqualified kind and returns a Group qualified GroupKind 

Allowed is required  True if the action would be allowed false otherwise 
Allowed is required True if the action would be allowed false otherwise 

Verb is a kubernetes resource API verb like get list watch create update delete proxy   means all 
Verb is a list of kubernetes resource API verbs like get list watch create update delete proxy   means all 

reasonInfo is the cached item in ReasonCache 
ReasonItem is the cached item in ReasonCache 

Get as many pod configs as we can from a directory Return an error if and only if something 
Get as many pod manifests as we can from a directory Return an error if and only if something 

First probe should fail 
First probe should succeed 

First probe should succeed 
First probe should fail 

A resource policy cannot match a nonresource request 
A nonresource policy cannot match a resource request 

A nonresource policy cannot match a resource request 
A resource policy cannot match a nonresource request 

File format is one map per line  This allows easy concatentation of files 
File format is one map per line  This allows easy concatenation of files 

Policy authorizes Kubernetes API actions using an Attributebased access control scheme 
Package abac authorizes Kubernetes API actions using an Attributebased access control scheme 

formatMap formats mapstringstring to a string 
FormatMap formats mapstringstring to a string 

CloneAndRemoveLabel clones the given map and returns a new map with the given key removed 
Clones the given map and returns a new map with the given key and value added 

Clones the given map and returns a new map with the given key and value added 
CloneAndRemoveLabel clones the given map and returns a new map with the given key removed 

check if the quota controller can evaluate this kind if not ignore it altogether 
check if the quota controller can evaluate this groupResource if not ignore it altogether 

obj could be an apiResourceQuota or a DeletionFinalStateUnknown marker item 
obj could be an v1ResourceQuota or a DeletionFinalStateUnknown marker item 

Controls full resync of objects monitored for replenihsment 
Controls full resync of objects monitored for replenishment 

Knows how to calculate usage 
knows how to calculate usage 

parseImageName parses a docker image string into two parts repo and tag 
ParseImageName parses a docker image string into three parts repo tag and digest 

gitRepoVolumeCleaner cleans git repo volumes 
gitRepoVolumeUnmounter cleans git repo volumes 

SetUpAt creates new directory and clones a git repo 
SetUp creates new directory and clones a git repo 

SetUp creates new directory and clones a git repo 
SetUpAt creates new directory and clones a git repo 

gitRepoVolumeBuilder builds git repo volumes 
gitRepoVolumeMounter builds git repo volumes 

OnEndpointsUpdate can be called without NewService being called externally 
OnEndpointsAdd can be called without NewService being called externally 

build cgo linux 
build linux 

HttpError wraps a nonStatusOK error code as an error 
HTTPError wraps a nonStatusOK error code as an error 

DockerConfigJson represents dockerconfigjson file info 
DockerConfigJSON represents dockerconfigjson file info 

Manages garbage collection of dead containers 
GC manages garbage collection of dead containers 

Specified a policy for garbage collecting containers 
GCPolicy specifies a policy for garbage collecting containers 

Now needUpdate and needReconcile should never be both true 
Now needUpdate needGracefulDelete and needReconcile should never be both true 

map of source name to pod name to pod reference 
map of source name to pod uid to pod reference 

PodConfigNotificationIncremental delivers ADD UPDATE REMOVE RECONCILE to the update channel 
PodConfigNotificationIncremental delivers ADD UPDATE DELETE REMOVE RECONCILE to the update channel 

Contains utility code for use by volume plugins 
Package util contains utility code for use by volume plugins 

it mounts it it to globalPDPath 
it mounts it to globalPDPath 

Resource takes an unqualified resource and returns back a Group qualified GroupResource 
Resource takes an unqualified resource and returns a Group qualified GroupResource 

Kind takes an unqualified kind and returns back a Group qualified GroupKind 
Kind takes an unqualified kind and returns a Group qualified GroupKind 

Deletes a mirror pod 
DeleteMirrorPod deletes a mirror pod 

mirror pods are stored in the kubelet directly because they need to be in sync with the internal pods 
basicMirrorClient is a functional MirrorClient  Mirror pods are stored in the kubelet directly because they need to be in sync with the internal pods 

read the the manifest URL passed to kubelet 
read the manifest URL passed to kubelet 

GatePath creates global mount path 
GetPath creates global mount path 

SetUpAt attaches the disk and bind mounts to the volume path 
SetUp attaches the disk and bind mounts to the volume path 

SetUp attaches the disk and bind mounts to the volume path 
SetUpAt attaches the disk and bind mounts to the volume path 

removeSecretReferenceIfNeeded updates the given ServiceAccount to remove a reference to the given secretName if needed 
removeSecretReference updates the given ServiceAccount to remove a reference to the given secretName if needed 

createSecretIfNeeded makes sure at least one ServiceAccountToken secret exists and is included in the serviceAccounts Secrets list 
ensureReferencedToken makes sure at least one ServiceAccountToken secret exists and is included in the serviceAccounts Secrets list 

Channel returns a channel where a configuration source 
ChannelWithContext returns a channel where a configuration source 

getExec handles requests to run a command inside a container 
getRun handles requests to run a command inside a container 

getRun handles requests to run a command inside a container 
getExec handles requests to run a command inside a container 

encodePods creates an apiPodList object from pods and returns the encoded 
encodePods creates an v1PodList object from pods and returns the encoded 

Setup pporf handlers 
Setup pprof handlers 

InstallDeguggingHandlers registers the HTTP request patterns that serve logs or run commandscontainers 
InstallDebuggingHandlers registers the HTTP request patterns that serve logs or run commandscontainers 

ListenAndServeKubeletReadOnlyServer initializes a server to respond to HTTP network requests on the Kubelet 
ListenAndServeKubeletServer initializes a server to respond to HTTP network requests on the Kubelet 

ListenAndServeKubeletServer initializes a server to respond to HTTP network requests on the Kubelet 
ListenAndServeKubeletReadOnlyServer initializes a server to respond to HTTP network requests on the Kubelet 

TODOthockin add a test for syncProxyRules or break it down further and test the pieces 
TODOthockin add more tests for syncProxyRules or break it down further and test the pieces 

Default port  used if no information about Kubelet port can be found in NodeNodeStatusDaemonEndpoints 
Port specifies the default port  used if no information about Kubelet port can be found in NodeNodeStatusDaemonEndpoints 

TestSelectableFieldLabelConversions verifies that given resource have field 
TestSelectableFieldLabelConversionsOfKind verifies that given resource have field 

which lazily draws from the set of registered credential providers 
which draws from the set of registered credential providers 

with the internal podscontainers and generats events accordingly 
with the internal podscontainers and generates events accordingly 

Returns a channel from which the subscriber can receive PodLifecycleEvent 
Watch returns a channel from which the subscriber can receive PodLifecycleEvent 

kubecontainerContainerState except for the nonexistent state This state 
kubecontainerState except for the nonexistent state This state 

garbage collector is implemented to work with such situtations However to 
garbage collector is implemented to work with such situations However to 

periodic listing to discover container changes It should be be used 
periodic listing to discover container changes It should be used 

package capbabilities manages system level capabilities 
Package capabilities manages system level capabilities 

Simulate a matchin with 1 core and 375GB of memory 
Simulate a machine with 1 core and 375GB of memory 

build linux darwin freebsd openbsd netbsd dragonfly 
gobuild linux  darwin  freebsd  openbsd  netbsd  dragonfly 

ecrProvider is a DockerConfigProvider that gets and refreshes 12hour tokens 
ecrProvider is a DockerConfigProvider that gets and refreshes tokens 

If all containers are known and succeeded just return PodCompleted 
If all init containers are known and succeeded just return PodCompleted 

obj could be an apiNamespace or a DeletionFinalStateUnknown item 
obj could be an v1Namespace or a DeletionFinalStateUnknown item 

readOnly bool is supplied by persistentclaim volume source when its builder creates other volumes 
readOnly bool is supplied by persistentclaim volume source when its mounter creates other volumes 

Eg If the devices cgroup for the container is stored in sysfscgroupdevicesdockernginx 
Eg if the devices cgroup for the container is stored in sysfscgroupdevicesdockernginx 

IsAllowAll checks whether the netsetsIPNet allows traffic from 00000 
IsAllowAll checks whether the utilnetIPNet allows traffic from 00000 

make a directory like varlibkubeletpluginskubernetesiopodfctargetlun0 
make a directory like varlibkubeletpluginskubernetesiofcvolumeDevicestargetlun0 

make a directory like varlibkubeletpluginskubernetesiopodfctargetlun0 
make a directory like varlibkubeletpluginskubernetesiofctarget1target2lun0 

Unmounts the device and detaches the disk from the kubelets host machine 
DetachDisk unmounts the device and detaches the disk from the kubelets host machine 

Attaches a disk specified by a volumeCinderPersistenDisk to the current kubelet 
AttachDisk attaches a disk specified by a volumeCinderPersistenDisk to the current kubelet 

obj could be an extensionsReplicaSet or a DeletionFinalStateUnknown marker item 
obj could be an v1Pod or a DeletionFinalStateUnknown marker item 

obj could be an apiPod or a DeletionFinalStateUnknown marker item 
obj could be an v1Pod or a DeletionFinalStateUnknown marker item 

and new replica set old and cur must be apiPod types 
and new replica set old and cur must be v1Pod types 

podStoreSynced returns true if the pod store has been synced at least once 
podListerSynced returns true if the pod store has been synced at least once 

podStoreSynced returns true if the pod store has been synced at least once 
rsListerSynced returns true if the pod store has been synced at least once 

obj could be an apiService or a DeletionFinalStateUnknown marker item 
obj could be an v1Pod or a DeletionFinalStateUnknown marker item 

obj could be an apiPod or a DeletionFinalStateUnknown marker item 
obj could be an v1Pod or a DeletionFinalStateUnknown marker item 

old and cur must be apiPod types 
old and cur must be v1Pod types 

enqueue them obj must have apiPod type 
enqueue them obj must have v1Pod type 

Runs e will not return until stopCh is closed workers determines how many 
Run will not return until stopCh is closed workers determines how many 

Scheme is the default instance of runtimeScheme to which types in the abac API group are registered 
Scheme is the default instance of runtimeScheme to which types in the abac API group are apiRegistry 

Group is the API group for abac 
GroupName is the API group for abac 

In v0 unspecified user and group matches all subjects 
In v0 unspecified user and group matches all authenticated subjects 

Cache provides two methods to retrive the PodStatus the nonblocking Get 
Cache provides two methods to retrieve the PodStatus the nonblocking Get 

3  podSpecActiveDeadlineSeconds gives the recycler pod a maximum timeout before failing  Recommended  Default is 60 seconds 
3 podSpecActiveDeadlineSeconds gives the recycler pod a maximum timeout before failing  Recommended  Default is 60 seconds 

2  podGenerateName helps distinguish recycler pods by name  Recommended Default is pvrecycler 
2 podGenerateName helps distinguish recycler pods by name  Recommended 

1  podSpecVolumes0VolumeSource must be overridden  Recycler implementations without a valid VolumeSource will fail 
1 podSpecVolumes0VolumeSource must be overridden  Recycler implementations without a valid VolumeSource will fail 

FindAttachablePluginByName fetches an attachable volume plugin by name Unlike the other FindPlugin methods this does not return error if no plugin is found  All volumes require a builder and cleaner but not every volume will have an attacherdetacher 
FindAttachablePluginBySpec fetches a persistent volume plugin by spec Unlike the other FindPlugin methods this does not return error if no plugin is found  All volumes require a mounter and unmounter but not every volume will have an attacherdetacher 

FindAttachablePluginByName fetches an attachable volume plugin by name Unlike the other FindPlugin methods this does not return error if no plugin is found  All volumes require a builder and cleaner but not every volume will 
Unlike the other FindPlugin methods this does not return error if no plugin is found  All volumes require a mounter and unmounter but not every volume will have an attacherdetacher FindDeviceMountablePluginBySpec fetches a persistent volume plugin by spec FindDeviceMountablePluginByName fetches a devicemountable volume plugin by name 

FindAttachablePluginByName fetches an attachable volume plugin by name Unlike the other FindPlugin methods this does not return error if no plugin is found  All volumes require a builder and cleaner but not every volume will 
FindAttachablePluginByName fetches an attachable volume plugin by name Unlike the other FindPlugin methods this does not return error if no plugin is found  All volumes require a mounter and unmounter but not 

FindAttachablePluginByName fetches an attachable volume plugin by name Unlike the other FindPlugin methods this does not return error if no plugin is found  All volumes require a builder and cleaner but not every volume will 
plugin is found  All volumes require a mounter and unmounter but not every volume will have an attacherdetacher FindAttachablePluginByName fetches an attachable volume plugin by name Unlike the other FindPlugin methods this does not return error if no 

FindAttachablePluginByName fetches an attachable volume plugin by name Unlike the other FindPlugin methods this does not return error if no plugin is found  All volumes require a builder and cleaner but not every volume will 
Unlike the other FindPlugin methods this does not return error if no plugin is found  All volumes require a mounter and unmounter but not every volume will have an attacherdetacher FindAttachablePluginByName fetches an attachable volume plugin by name 

does not return error if no plugin is found  All volumes require a builder and cleaner but not every volume will have an attacherdetacher FindAttachablePluginByName fetches an attachable volume plugin by name Unlike the other FindPlugin methods this 
FindAttachablePluginByName fetches an attachable volume plugin by name Unlike the other FindPlugin methods this does not return error if no plugin is found  All volumes require a mounter and unmounter but not 

does not return error if no plugin is found  All volumes require a builder and cleaner but not every volume will have an attacherdetacher FindAttachablePluginByName fetches an attachable volume plugin by name Unlike the other FindPlugin methods this 
plugin is found  All volumes require a mounter and unmounter but not every volume will have an attacherdetacher FindAttachablePluginByName fetches an attachable volume plugin by name Unlike the other FindPlugin methods this does not return error if no 

does not return error if no plugin is found  All volumes require a builder and cleaner but not every volume will have an attacherdetacher FindAttachablePluginByName fetches an attachable volume plugin by name Unlike the other FindPlugin methods this 
Unlike the other FindPlugin methods this does not return error if no plugin is found  All volumes require a mounter and unmounter but not every volume will have an attacherdetacher FindAttachablePluginByName fetches an attachable volume plugin by name 

FindAttachablePluginBySpec fetches a persistent volume plugin by name  Unlike the other FindPlugin methods this does not return error if no plugin is found  All volumes require a builder and cleaner but not every volume will have an attacherdetacher FindAttachablePluginByName fetches an attachable volume plugin by name Unlike the other FindPlugin methods this 
every volume will have an attacherdetacher FindAttachablePluginByName fetches an attachable volume plugin by name Unlike the other FindPlugin methods this does not return error if no 

FindAttachablePluginBySpec fetches a persistent volume plugin by name  Unlike the other FindPlugin methods this does not return error if no plugin is found  All volumes require a builder and cleaner but not every volume will have an attacherdetacher 
FindAttachablePluginByName fetches an attachable volume plugin by name Unlike the other FindPlugin methods this does not return error if no plugin is found  All volumes require a mounter and unmounter but not 

FindAttachablePluginBySpec fetches a persistent volume plugin by name  Unlike the other FindPlugin methods this does not return error if no plugin is found  All volumes require a builder and cleaner but not every volume will have an attacherdetacher 
FindAttachablePluginBySpec fetches a persistent volume plugin by spec Unlike the other FindPlugin methods this does not return error if no plugin is found  All volumes require a mounter and unmounter but not 

FindAttachablePluginBySpec fetches a persistent volume plugin by name  Unlike the other FindPlugin methods this does not return error if no plugin is found  All volumes require a builder and cleaner but not every volume will 
Unlike the other FindPlugin methods this does not return error if no plugin is found  All volumes require a mounter and unmounter but not every volume will have an attacherdetacher FindDeviceMountablePluginBySpec fetches a persistent volume plugin by spec 

FindAttachablePluginBySpec fetches a persistent volume plugin by name  Unlike the other FindPlugin methods this does not return error if no plugin is found  All volumes require a builder and cleaner but not every volume will 
FindAttachablePluginByName fetches an attachable volume plugin by name Unlike the other FindPlugin methods this does not return error if no plugin is found  All volumes require a mounter and unmounter but not every volume will have an attacherdetacher 

FindAttachablePluginBySpec fetches a persistent volume plugin by name  Unlike the other FindPlugin methods this does not return error if no plugin is found  All volumes require a builder and cleaner but not every volume will 
every volume will have an attacherdetacher FindAttachablePluginByName fetches an attachable volume plugin by name Unlike the other FindPlugin methods this does not return error if no plugin is found  All volumes require a mounter and unmounter but not 

FindAttachablePluginBySpec fetches a persistent volume plugin by name  Unlike the other FindPlugin methods this does not return error if no plugin is found  All volumes require a builder and cleaner but not every volume will 
plugin is found  All volumes require a mounter and unmounter but not every volume will have an attacherdetacher FindAttachablePluginByName fetches an attachable volume plugin by name Unlike the other FindPlugin methods this does not return error if no 

FindAttachablePluginBySpec fetches a persistent volume plugin by name  Unlike the other FindPlugin methods this does not return error if no plugin is found  All volumes require a builder and cleaner but not every volume will 
Unlike the other FindPlugin methods this does not return error if no plugin is found  All volumes require a mounter and unmounter but not every volume will have an attacherdetacher FindAttachablePluginByName fetches an attachable volume plugin by name 

FindAttachablePluginBySpec fetches a persistent volume plugin by name  Unlike the other FindPlugin methods this does not return error if no plugin is found  All volumes require a builder and cleaner but not every volume will 
FindAttachablePluginBySpec fetches a persistent volume plugin by spec Unlike the other FindPlugin methods this does not return error if no plugin is found  All volumes require a mounter and unmounter but not every volume will have an attacherdetacher 

FindCreatablePluginBySpec fetches a persistent volume plugin by name  If no plugin is found returns error 
no plugin is found returns error FindCreatablePluginBySpec fetches a persistent volume plugin by name  If 

FindCreatablePluginBySpec fetches a persistent volume plugin by name  If no plugin is found returns error 
FindDeletablePluginByName fetches a persistent volume plugin by name  If no plugin is found returns error 

FindCreatablePluginBySpec fetches a persistent volume plugin by name  If no plugin is found returns error 
no plugin is found returns error FindDeletablePluginByName fetches a persistent volume plugin by name  If 

FindCreatablePluginBySpec fetches a persistent volume plugin by name  If no plugin is found returns error 
FindDeletablePluginBySpec fetches a persistent volume plugin by spec  If no plugin is found returns error 

FindCreatablePluginBySpec fetches a persistent volume plugin by name  If no plugin is found returns error 
FindProvisionablePluginByName fetches  a persistent volume plugin by name  If no plugin is found returns error 

FindCreatablePluginBySpec fetches a persistent volume plugin by name  If no plugin is found returns error 
no plugin is found returns error FindProvisionablePluginByName fetches  a persistent volume plugin by name  If 

FindCreatablePluginBySpec fetches a persistent volume plugin by name  If no plugin is found returns error 
FindRecyclablePluginByName fetches a persistent volume plugin by name  If no plugin is found returns error 

FindCreatablePluginBySpec fetches a persistent volume plugin by name  If no plugin is found returns error 
no plugin is found returns error FindRecyclablePluginByName fetches a persistent volume plugin by name  If 

FindCreatablePluginBySpec fetches a persistent volume plugin by name  If no plugin is found returns error 
FindPersistentPluginByName fetches a persistent volume plugin by name  If no plugin is found returns error 

FindCreatablePluginBySpec fetches a persistent volume plugin by name  If no plugin 
FindDeletablePluginByName fetches a persistent volume plugin by name  If no plugin is found returns error FindCreatablePluginBySpec fetches a persistent volume plugin by name  If 

is found returns error FindCreatablePluginBySpec fetches a persistent volume plugin by name  If no plugin 
no plugin is found returns error FindCreatablePluginBySpec fetches a persistent volume plugin by name  If 

is found returns error FindCreatablePluginBySpec fetches a persistent volume plugin by name  If no plugin 
FindDeletablePluginByName fetches a persistent volume plugin by name  If no plugin is found returns error 

is found returns error FindCreatablePluginBySpec fetches a persistent volume plugin by name  If no plugin 
no plugin is found returns error FindDeletablePluginByName fetches a persistent volume plugin by name  If 

is found returns error FindCreatablePluginBySpec fetches a persistent volume plugin by name  If no plugin 
FindProvisionablePluginByName fetches  a persistent volume plugin by name  If no plugin is found returns error 

is found returns error FindCreatablePluginBySpec fetches a persistent volume plugin by name  If no plugin 
no plugin is found returns error FindProvisionablePluginByName fetches  a persistent volume plugin by name  If 

is found returns error FindCreatablePluginBySpec fetches a persistent volume plugin by name  If no plugin 
FindRecyclablePluginByName fetches a persistent volume plugin by name  If no plugin is found returns error 

is found returns error FindCreatablePluginBySpec fetches a persistent volume plugin by name  If no plugin 
no plugin is found returns error FindRecyclablePluginByName fetches a persistent volume plugin by name  If 

is found returns error FindCreatablePluginBySpec fetches a persistent volume plugin by name  If no plugin 
FindPersistentPluginByName fetches a persistent volume plugin by name  If no plugin is found returns error 

FindDeletablePluginByName fetches a persistent volume plugin by name  If no plugin is found returns error 
FindCreatablePluginBySpec fetches a persistent volume plugin by name  If no plugin is found returns error 

FindDeletablePluginByName fetches a persistent volume plugin by name  If no plugin is found returns error 
no plugin is found returns error FindCreatablePluginBySpec fetches a persistent volume plugin by name  If 

FindDeletablePluginByName fetches a persistent volume plugin by name  If no plugin is found returns error 
no plugin is found returns error FindDeletablePluginByName fetches a persistent volume plugin by name  If 

FindDeletablePluginByName fetches a persistent volume plugin by name  If no plugin is found returns error 
FindDeletablePluginBySpec fetches a persistent volume plugin by spec  If no plugin is found returns error 

FindDeletablePluginByName fetches a persistent volume plugin by name  If no plugin is found returns error 
FindProvisionablePluginByName fetches  a persistent volume plugin by name  If no plugin is found returns error 

FindDeletablePluginByName fetches a persistent volume plugin by name  If no plugin is found returns error 
no plugin is found returns error FindProvisionablePluginByName fetches  a persistent volume plugin by name  If 

FindDeletablePluginByName fetches a persistent volume plugin by name  If no plugin is found returns error 
FindRecyclablePluginByName fetches a persistent volume plugin by name  If no plugin is found returns error 

FindDeletablePluginByName fetches a persistent volume plugin by name  If no plugin is found returns error 
no plugin is found returns error FindRecyclablePluginByName fetches a persistent volume plugin by name  If 

FindDeletablePluginByName fetches a persistent volume plugin by name  If no plugin is found returns error 
FindPersistentPluginByName fetches a persistent volume plugin by name  If no plugin is found returns error 

is found returns error FindDeletablePluginByName fetches a persistent volume plugin by name  If no plugin 
FindCreatablePluginBySpec fetches a persistent volume plugin by name  If no plugin is found returns error 

is found returns error FindDeletablePluginByName fetches a persistent volume plugin by name  If no plugin 
no plugin is found returns error FindCreatablePluginBySpec fetches a persistent volume plugin by name  If 

is found returns error FindDeletablePluginByName fetches a persistent volume plugin by name  If no plugin 
FindDeletablePluginByName fetches a persistent volume plugin by name  If no plugin is found returns error 

is found returns error FindDeletablePluginByName fetches a persistent volume plugin by name  If no plugin 
no plugin is found returns error FindDeletablePluginByName fetches a persistent volume plugin by name  If 

is found returns error FindDeletablePluginByName fetches a persistent volume plugin by name  If no plugin 
FindProvisionablePluginByName fetches  a persistent volume plugin by name  If no plugin is found returns error 

is found returns error FindDeletablePluginByName fetches a persistent volume plugin by name  If no plugin 
no plugin is found returns error FindProvisionablePluginByName fetches  a persistent volume plugin by name  If 

is found returns error FindDeletablePluginByName fetches a persistent volume plugin by name  If no plugin 
FindRecyclablePluginByName fetches a persistent volume plugin by name  If no plugin is found returns error 

is found returns error FindDeletablePluginByName fetches a persistent volume plugin by name  If no plugin 
no plugin is found returns error FindRecyclablePluginByName fetches a persistent volume plugin by name  If 

is found returns error FindDeletablePluginByName fetches a persistent volume plugin by name  If no plugin 
FindPersistentPluginByName fetches a persistent volume plugin by name  If no plugin is found returns error 

FindRecyclablePluginByName fetches a persistent volume plugin by name  If no plugin is found returns error 
FindCreatablePluginBySpec fetches a persistent volume plugin by name  If no plugin is found returns error 

FindRecyclablePluginByName fetches a persistent volume plugin by name  If no plugin is found returns error 
no plugin is found returns error FindCreatablePluginBySpec fetches a persistent volume plugin by name  If 

FindRecyclablePluginByName fetches a persistent volume plugin by name  If no plugin is found returns error 
FindDeletablePluginByName fetches a persistent volume plugin by name  If no plugin is found returns error 

FindRecyclablePluginByName fetches a persistent volume plugin by name  If no plugin is found returns error 
no plugin is found returns error FindDeletablePluginByName fetches a persistent volume plugin by name  If 

FindRecyclablePluginByName fetches a persistent volume plugin by name  If no plugin is found returns error 
FindDeletablePluginBySpec fetches a persistent volume plugin by spec  If no plugin is found returns error 

FindRecyclablePluginByName fetches a persistent volume plugin by name  If no plugin is found returns error 
FindProvisionablePluginByName fetches  a persistent volume plugin by name  If no plugin is found returns error 

FindRecyclablePluginByName fetches a persistent volume plugin by name  If no plugin is found returns error 
no plugin is found returns error FindProvisionablePluginByName fetches  a persistent volume plugin by name  If 

FindRecyclablePluginByName fetches a persistent volume plugin by name  If no plugin is found returns error 
no plugin is found returns error FindRecyclablePluginByName fetches a persistent volume plugin by name  If 

FindRecyclablePluginByName fetches a persistent volume plugin by name  If no plugin is found returns error 
FindPersistentPluginByName fetches a persistent volume plugin by name  If no plugin is found returns error 

is found returns error FindRecyclablePluginByName fetches a persistent volume plugin by name  If no plugin 
FindCreatablePluginBySpec fetches a persistent volume plugin by name  If no plugin is found returns error 

is found returns error FindRecyclablePluginByName fetches a persistent volume plugin by name  If no plugin 
no plugin is found returns error FindCreatablePluginBySpec fetches a persistent volume plugin by name  If 

is found returns error FindRecyclablePluginByName fetches a persistent volume plugin by name  If no plugin 
FindDeletablePluginByName fetches a persistent volume plugin by name  If no plugin is found returns error 

is found returns error FindRecyclablePluginByName fetches a persistent volume plugin by name  If no plugin 
no plugin is found returns error FindDeletablePluginByName fetches a persistent volume plugin by name  If 

is found returns error FindRecyclablePluginByName fetches a persistent volume plugin by name  If no plugin 
FindProvisionablePluginByName fetches  a persistent volume plugin by name  If no plugin is found returns error 

is found returns error FindRecyclablePluginByName fetches a persistent volume plugin by name  If no plugin 
no plugin is found returns error FindProvisionablePluginByName fetches  a persistent volume plugin by name  If 

is found returns error FindRecyclablePluginByName fetches a persistent volume plugin by name  If no plugin 
FindRecyclablePluginByName fetches a persistent volume plugin by name  If no plugin is found returns error 

is found returns error FindRecyclablePluginByName fetches a persistent volume plugin by name  If no plugin 
no plugin is found returns error FindRecyclablePluginByName fetches a persistent volume plugin by name  If 

is found returns error FindRecyclablePluginByName fetches a persistent volume plugin by name  If no plugin 
FindPersistentPluginByName fetches a persistent volume plugin by name  If no plugin is found returns error 

FindPersistentPluginByName fetches a persistent volume plugin by name  If no plugin is found returns error 
FindCreatablePluginBySpec fetches a persistent volume plugin by name  If no plugin is found returns error 

FindPersistentPluginByName fetches a persistent volume plugin by name  If no plugin is found returns error 
no plugin is found returns error FindCreatablePluginBySpec fetches a persistent volume plugin by name  If 

FindPersistentPluginByName fetches a persistent volume plugin by name  If no plugin is found returns error 
FindDeletablePluginByName fetches a persistent volume plugin by name  If no plugin is found returns error 

FindPersistentPluginByName fetches a persistent volume plugin by name  If no plugin is found returns error 
no plugin is found returns error FindDeletablePluginByName fetches a persistent volume plugin by name  If 

FindPersistentPluginByName fetches a persistent volume plugin by name  If no plugin is found returns error 
FindDeletablePluginBySpec fetches a persistent volume plugin by spec  If no plugin is found returns error 

FindPersistentPluginByName fetches a persistent volume plugin by name  If no plugin is found returns error 
FindProvisionablePluginByName fetches  a persistent volume plugin by name  If no plugin is found returns error 

FindPersistentPluginByName fetches a persistent volume plugin by name  If no plugin is found returns error 
no plugin is found returns error FindProvisionablePluginByName fetches  a persistent volume plugin by name  If 

FindPersistentPluginByName fetches a persistent volume plugin by name  If no plugin is found returns error 
FindRecyclablePluginByName fetches a persistent volume plugin by name  If no plugin is found returns error 

FindPersistentPluginByName fetches a persistent volume plugin by name  If no plugin is found returns error 
no plugin is found returns error FindRecyclablePluginByName fetches a persistent volume plugin by name  If 

specification  If no plugin is found return an error FindPersistentPluginByName fetches a persistent volume plugin by name  If no plugin 
FindCreatablePluginBySpec fetches a persistent volume plugin by name  If no plugin is found returns error 

specification  If no plugin is found return an error FindPersistentPluginByName fetches a persistent volume plugin by name  If no plugin 
FindDeletablePluginByName fetches a persistent volume plugin by name  If no plugin is found returns error 

specification  If no plugin is found return an error FindPersistentPluginByName fetches a persistent volume plugin by name  If no plugin 
FindProvisionablePluginByName fetches  a persistent volume plugin by name  If no plugin is found returns error 

specification  If no plugin is found return an error FindPersistentPluginByName fetches a persistent volume plugin by name  If no plugin 
FindRecyclablePluginByName fetches a persistent volume plugin by name  If no plugin is found returns error 

NewSpecFromPersistentVolume creates an Spec from an apiPersistentVolume 
NewSpecFromPersistentVolume creates an Spec from an v1PersistentVolume 

NewSpecFromVolume creates an Spec from an apiVolume 
NewSpecFromVolume creates an Spec from an v1Volume 

Gi of capacity in the persistent volume Example 5Gi volume x 30s increment  150s  30s minimum  180s ActiveDeadlineSeconds for recycler pod 
pods ActiveDeadlineSeconds for each Gi of capacity in the persistent volume Example 5Gi volume x 30s increment  150s  30s minimum  180s 

Added to the minimum timeout is the increment per Gi of capacity RecyclerTimeoutIncrement is the number of seconds added to the recycler pods ActiveDeadlineSeconds for each 
recycler pods ActiveDeadlineSeconds attribute Added to the minimum timeout is the increment per Gi of capacity RecyclerTimeoutIncrement is the number of seconds added to the recycler 

The binary should still use strong typing for this value when binding CLI values before they are passed as strings 
used for truly oneoff configuration The binary should still use strong typing for this value when binding CLI values before they are passed as 

given directly to the plugin so config names specific to plugins are unneeded and wrongly expose plugins 
config names specific to plugins are unneeded and wrongly expose plugins in 

preference is to leverage strong typing in this struct  All config items must have a descriptive but nonspecific name ie RecyclerMinimumTimeout is OK but RecyclerMinimumTimeoutForNFS is OK  An instance of config will be given directly to the plugin so config names specific to plugins are unneeded and wrongly expose plugins 
not necessarily all plugins  The preference is to leverage strong typing in this struct  All config items must have a descriptive but nonspecific name ie RecyclerMinimumTimeout is OK but RecyclerMinimumTimeoutForNFS is OK  An instance of config will be given directly to the plugin so 

the plugins while allowing override of those default values  Those config values are then set to an instance of VolumeConfig and passed to the plugin Values in VolumeConfig are intended to be relevant to several plugins but not necessarily all plugins  The 
values  Those config values are then set to an instance of VolumeConfig and passed to the plugin Values in VolumeConfig are intended to be relevant to several plugins but not necessarily all plugins  The preference is to leverage strong typing 

the provided spec  See comments on NewWrapperBuilder for more 
the provided spec  See comments on NewWrapperMounter for more 

NewWrapperCleaner finds an appropriate plugin with which to handle 
NewWrapperUnmounter finds an appropriate plugin with which to handle 

NewWrapperBuilder finds an appropriate plugin with which to handle 
NewWrapperUnmounter finds an appropriate plugin with which to handle 

NewWrapperBuilder finds an appropriate plugin with which to handle 
NewWrapperMounter finds an appropriate plugin with which to handle 

GetPodPluginDir returns the absolute path to a directory under which 
GetPodVolumeDeviceDir returns the absolute path a directory which 

GetPodPluginDir returns the absolute path to a directory under which 
GetPodVolumeDir returns the absolute path a directory which 

GetPodPluginDir returns the absolute path to a directory under which 
GetPodPluginDir GetVolumeDevicePluginDir returns the absolute path to a directory 

GetPodPluginDir returns the absolute path to a directory under which 
GetPluginDir returns the absolute path to a directory under which 

represents the named volume under the named plugin for the given pod  If the specified pod does not exist the result of this call 
represents the named plugin for the given pod If the specified pod does not exist the result of this call 

GetPodVolumeDir returns the absolute path a directory which 
GetPodVolumeDeviceDir returns the absolute path a directory which 

GetPodVolumeDir returns the absolute path a directory which 
GetPodPluginDir returns the absolute path to a directory under which 

GetPodVolumeDir returns the absolute path a directory which 
GetPluginDir returns the absolute path to a directory under which 

GetPluginDir returns the absolute path to a directory under which a given plugin may store data  This directory might not actually 
GetVolumeDevicePluginDir returns the absolute path to a directory under which a given plugin may store data 

GetPluginDir returns the absolute path to a directory under which 
GetPodVolumeDeviceDir returns the absolute path a directory which 

GetPluginDir returns the absolute path to a directory under which 
GetPodPluginDir returns the absolute path to a directory under which 

GetPluginDir returns the absolute path to a directory under which 
GetPodVolumeDir returns the absolute path a directory which 

VolumeHost is an interface that plugins can use to access the kubelet 
KubeletVolumeHost is a Kubelet specific interface that plugins can use to access the kubelet 

to be deleted from the cluster after their release from a PersistentVolumeClaim NewDeleter creates a new volumeDeleter which knows how to delete this resource in accordance with the underlying storage provider after the volumes release from a claim 
by persistent volumes that want to be deleted from the cluster after their release from a PersistentVolumeClaim NewDeleter creates a new volumeDeleter which knows how to delete this resource in accordance with the underlying storage provider after the 

after the volumes release from a PersistentVolumeClaim DeletableVolumePlugin is an extended interface of VolumePlugin and is used by persistent volumes that want to be deleted from the cluster after their release from a PersistentVolumeClaim NewDeleter creates a new volumeDeleter which knows how to delete this resource 
by persistent volumes that want to be deleted from the cluster after their release from a PersistentVolumeClaim NewDeleter creates a new volumeDeleter which knows how to delete this resource in accordance with the underlying storage provider after the volumes release from a claim ProvisionableVolumePlugin is an extended interface of VolumePlugin and is 

RecyclableVolumePlugin is an extended interface of VolumePlugin and is used 
DeletableVolumePlugin is an extended interface of VolumePlugin and is used 

RecyclableVolumePlugin is an extended interface of VolumePlugin and is used 
PersistentVolumePlugin is an extended interface of VolumePlugin and is used 

PersistentVolumePlugin is an extended interface of VolumePlugin and is used 
DeviceMountableVolumePlugin is an extended interface of VolumePlugin and is used 

PersistentVolumePlugin is an extended interface of VolumePlugin and is used 
DeletableVolumePlugin is an extended interface of VolumePlugin and is used 

PersistentVolumePlugin is an extended interface of VolumePlugin and is used 
RecyclableVolumePlugin is an extended interface of VolumePlugin and is used 

 name The volume name as per the apiVolume spec 
 name The volume name as per the v1Volume spec 

 name The volume name as per the apiVolume spec 
 name The volume name as per the v1Volume spec 

 spec The apiVolume spec 
 spec The v1Volume spec 

 spec The apiVolume spec 
 spec The v1Volume spec 

NewBuilder creates a new volumeBuilder from an API specification 
NewBlockVolumeMapper creates a new volumeBlockVolumeMapper from an API specification 

Name returns the plugins name  Plugins should use namespaced names 
Name returns the plugins name  Plugins must use namespaced names 

unionDockerKeyring delegates to a set of keyrings 
UnionDockerKeyring delegates to a set of keyrings 

globUrldockerio targetUrlnotrightio    no match 
	globURLdockerio targetURLnotrightio    no match 

check whether the given target url matches the glob url which may have 
URLsMatch checks whether the given target url matches the glob url which may have 

split the host name into parts as well as the port 
SplitURL splits the host name into parts as well as the port 

  schemelessUrl 
  schemelessURL 

than the given timestamp 
than the timestamp 

Both the global cache timestamp and the modified time are older 
Both the global cache timestamp and the modified time are newer 

time is newer than the given timestamp This means that the 
time is older than the given timestamp This means that the 

Global cache timestamp is older but the pod entry modified 
Global cache timestamp is newer but the pod entry modified 

time is older than the given timestamp This means that the 
time is newer than the given timestamp This means that the 

Global cache timestamp is newer but the pod entry modified 
Global cache timestamp is older but the pod entry modified 

than the timestamp 
than the given timestamp 

Both the global cache timestamp and the modified time are newer 
Both the global cache timestamp and the modified time are older 

Copied from syncOnce but we dont want to cache the results if there is an error 
error Copied from syncOnce but we dont want to cache the results if there is an 

Copied from syncOnce but we dont want to cache the results if there is an error 
error Copied from syncOnce but we dont want to cache the results if there is an 

Runs GetMetrics Once and caches the result  Will not cache result if there is an error 
caches the result Will not cache result if there is an error 

